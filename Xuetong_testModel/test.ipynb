{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b847f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tabulate import tabulate\n",
    "from hyperopt import hp, fmin, tpe, Trials, space_eval\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cvx\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "import quadprog\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d24b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "arabiensis_female    3000\n",
      "culex_female         3000\n",
      "funestus_female      3000\n",
      "gambiae_female       3000\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "gambiae_female       600\n",
      "culex_female         522\n",
      "funestus_female      512\n",
      "arabiensis_female    428\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_incubator = pd.read_csv('train_incubator.csv')\n",
    "test_sf2 = pd.read_csv('test_sf2.csv')\n",
    "\n",
    "# Check number of examples per class\n",
    "print (train_incubator['class'].value_counts())\n",
    "print (test_sf2['class'].value_counts())\n",
    "\n",
    "nclasses = len(train_incubator['class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95be81b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets\n",
    "special_features = ['temperature', 'duration', 'humidity', 'hour', 'luminosity', 'altitude', 'air_pressure']\n",
    "wbf_features = ['L_harmcherry_wbf_mean','L_harmcherry_wbf_stddev']\n",
    "freq_features = [f'L_harmcherry_h{i}_freq' for i in range(1,9)]\n",
    "basefreq_features = [f'L_harmcherry_h{i}_basefreq' for i in range(1,9)]\n",
    "relbasefreq_features = [f'L_harmcherry_h{i}_relbasefreq' for i in range(1,9)]\n",
    "power_features = [f'L_harmcherry_h{i}_power' for i in range(1,9)]\n",
    "relpower_features = [f'L_harmcherry_h{i}_relpower' for i in range(1,9)]\n",
    "invented_features = [f'L_harmcherry_h{i}_invented' for i in range(1,9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4df6d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  LGBM\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9591\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "\tAcc: 0.4942\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.41      0.30      0.34       428\n",
      "     culex_female       0.48      0.74      0.58       522\n",
      "  funestus_female       0.53      0.91      0.67       512\n",
      "   gambiae_female       0.67      0.07      0.12       600\n",
      "\n",
      "         accuracy                           0.49      2062\n",
      "        macro avg       0.52      0.50      0.43      2062\n",
      "     weighted avg       0.53      0.49      0.42      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 127 │            177 │               108 │               16 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 109 │            385 │                28 │                0 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                   5 │             36 │               468 │                3 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  70 │            210 │               281 │               39 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Train and test a LGBM model\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "models = [('LGBM', lgb.LGBMClassifier())]\n",
    "\n",
    "for name, model in models:\n",
    "    print(\"Model: \", name)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    p_labels = model.predict(X_test)\n",
    "    a_labels = y_test\n",
    "    acc = accuracy_score(a_labels, p_labels)\n",
    "    \n",
    "    print(\"\\tAcc: %.4f\" % acc)\n",
    "    print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "        \n",
    "    cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "    print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680ea50",
   "metadata": {},
   "source": [
    "### Estimate the probability distribution for each class using different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "004a7cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabiensis_female' 'culex_female' 'funestus_female' 'gambiae_female']\n",
      "12000\n",
      "[0.25 0.25 0.25 0.25]\n"
     ]
    }
   ],
   "source": [
    "def class_dist(Y, nclasses):\n",
    "    return np.array([np.count_nonzero(Y == i) for i in range(nclasses)]) / Y.shape[0]\n",
    "\n",
    "Y = train_incubator['class'].values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y)\n",
    "\n",
    "classes = class_dist(Y_encoded, nclasses)\n",
    "\n",
    "print(np.unique(Y))\n",
    "print(Y.shape[0])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79fc7a",
   "metadata": {},
   "source": [
    "Use getScores to get the predicted score of train and test\n",
    "\n",
    "Perform group cross-validation on the data through the group_kfold.split() method, where group_kfold is a defined group cross-validation object. In each iteration of cross-validation, use the fit() method to fit the model, and use the predict_proba() method to obtain the probability score of each sample belonging to each category, and then fill these scores into the corresponding positions of the train_scores array.\n",
    "\n",
    "Finally, the model is refitted on the entire training set and the predict_proba() method is used to obtain the predicted probability score for the test set and stored in the test_scores array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30e3bb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9572\n",
      "[LightGBM] [Info] Number of data points in the train set: 9395, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.379823\n",
      "[LightGBM] [Info] Start training from score -1.321889\n",
      "[LightGBM] [Info] Start training from score -1.412937\n",
      "[LightGBM] [Info] Start training from score -1.434148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9570\n",
      "[LightGBM] [Info] Number of data points in the train set: 9638, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.430199\n",
      "[LightGBM] [Info] Start training from score -1.406205\n",
      "[LightGBM] [Info] Start training from score -1.335126\n",
      "[LightGBM] [Info] Start training from score -1.376178\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9577\n",
      "[LightGBM] [Info] Number of data points in the train set: 9961, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.429059\n",
      "[LightGBM] [Info] Start training from score -1.371645\n",
      "[LightGBM] [Info] Start training from score -1.410375\n",
      "[LightGBM] [Info] Start training from score -1.336649\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9592\n",
      "[LightGBM] [Info] Number of data points in the train set: 10062, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.428724\n",
      "[LightGBM] [Info] Start training from score -1.370714\n",
      "[LightGBM] [Info] Start training from score -1.445454\n",
      "[LightGBM] [Info] Start training from score -1.306298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9551\n",
      "[LightGBM] [Info] Number of data points in the train set: 8944, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.261579\n",
      "[LightGBM] [Info] Start training from score -1.472655\n",
      "[LightGBM] [Info] Start training from score -1.325985\n",
      "[LightGBM] [Info] Start training from score -1.505364\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9591\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n"
     ]
    }
   ],
   "source": [
    "best_params = {'learning_rate': 0.13, \n",
    "                'max_depth': 7, \n",
    "                'n_estimators': 200,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'num_leaves': 5,\n",
    "                'reg_alpha': 0.01,\n",
    "                'reg_lambda': 0.01,\n",
    "                'subsample': 0.88}\n",
    "\n",
    "def getScores(X_train, X_test, Y_train, nclasses):\n",
    "\n",
    "    # model = lgb.LGBMClassifier(**best_params)\n",
    "    model = lgb.LGBMClassifier()\n",
    "   \n",
    "    train_scores = np.zeros((len(X_train), nclasses))\n",
    "    test_scores = np.zeros((len(X_test), nclasses))\n",
    "\n",
    "    groups = train_incubator['sensor'].values\n",
    "    group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "    for train_index, test_index in group_kfold.split(X_train, Y_train, groups):\n",
    "        model.fit(X_train[train_index], Y_train[train_index])\n",
    "        train_scores[test_index] = model.predict_proba(X_train)[test_index]\n",
    "    \n",
    "    model.fit(X_train, Y_train)\n",
    "    test_scores = model.predict_proba(X_test)\n",
    "           \n",
    "    return train_scores, test_scores\n",
    "\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).values\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).values\n",
    "y_train = train_incubator['class'].values\n",
    "groups = train_incubator['sensor'].values\n",
    "\n",
    "train_scores, test_scores = getScores(X_train, X_test, y_train, nclasses)\n",
    "# print('train_scores')\n",
    "# print(train_scores)\n",
    "# print('test_scores')\n",
    "# print(test_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908b541e",
   "metadata": {},
   "source": [
    "EMQ function\n",
    "\n",
    "EM algorithm to estimate the class distribution of the test set. The EMQ function assumes that the class distribution of the test set is unknown, but can be estimated by the class conditional probabilities on the test set. It iteratively adjusts the class distribution so that, under given model parameters, the class conditional probability of the test set best matches the actual observed test set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c8138ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "\n",
    "\n",
    "def EMQ(test_scores, train_labels, nclasses):\n",
    "    max_it = 1000        # Max num of iterations\n",
    "    eps = 1e-6           # Small constant for stopping criterium\n",
    "\n",
    "    p_tr = class_dist(train_labels, nclasses)\n",
    "    p_s = np.copy(p_tr)\n",
    "    p_cond_tr = np.array(test_scores)\n",
    "    p_cond_s = np.zeros(p_cond_tr.shape)\n",
    "    prob_arrays = []\n",
    "\n",
    "    for _ in range(max_it):\n",
    "        # Add Laplacian smoothing\n",
    "        # r = (p_s + alpha) / (p_tr + (alpha * nclasses))\n",
    "        r = p_s / p_tr\n",
    "        \n",
    "        p_cond_s = p_cond_tr * r\n",
    "        s = np.sum(p_cond_s, axis = 1)\n",
    "        for c in range(nclasses):\n",
    "            p_cond_s[:,c] = p_cond_s[:,c] / s\n",
    "\n",
    "        prob_arrays.append(p_cond_s)\n",
    "        p_s_old = np.copy(p_s)\n",
    "        p_s = np.sum(p_cond_s, axis = 0) / p_cond_s.shape[0]\n",
    "        if (np.sum(np.abs(p_s - p_s_old)) < eps):\n",
    "            break\n",
    "\n",
    "    return(p_s/np.sum(p_s))\n",
    "    # return p_cond_s\n",
    "\n",
    "Y = train_incubator['class'].values\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(Y)\n",
    "EMQ_result = EMQ(test_scores, train_labels, nclasses)\n",
    "# print(EMQ_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6beaa",
   "metadata": {},
   "source": [
    "### Test model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0e17288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  LGBM\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9591\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\tAcc: 0.5131\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.40      0.21      0.27       428\n",
      "     culex_female       0.45      0.79      0.57       522\n",
      "  funestus_female       0.62      0.87      0.72       512\n",
      "   gambiae_female       0.56      0.19      0.29       600\n",
      "\n",
      "         accuracy                           0.51      2062\n",
      "        macro avg       0.51      0.51      0.46      2062\n",
      "     weighted avg       0.51      0.51      0.46      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  90 │            206 │                63 │               69 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  87 │            410 │                21 │                4 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                   2 │             49 │               443 │               18 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  48 │            251 │               186 │              115 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Train and test a LGBM model\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "best_params = {'learning_rate': 0.14, \n",
    "                'max_depth': 9, \n",
    "                'n_estimators': 212,\n",
    "                'colsample_bytree': 0.82,\n",
    "                'num_leaves': 49,\n",
    "                'reg_alpha': 0.077,\n",
    "                'reg_lambda': 0.7,\n",
    "                'subsample': 0.93}\n",
    "\n",
    "models = [('LGBM', lgb.LGBMClassifier(**best_params))]\n",
    "\n",
    "for name, model in models:\n",
    "    print(\"Model: \", name)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    p_labels = model.predict(X_test)\n",
    "    a_labels = y_test\n",
    "    acc = accuracy_score(a_labels, p_labels)\n",
    "    \n",
    "    print(\"\\tAcc: %.4f\" % acc)\n",
    "    print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "        \n",
    "    cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "    print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2649b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  LGBM\n",
      "EMQ [9.18755340e-02 4.67859621e-01 4.40264845e-01 1.69099221e-15]\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9591\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 0, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 0, 2, 0, 1, 2, 0, 0, 2, 1, 2, 0, 2, 2, 1, 0, 1, 1, 0, 1, 2, 0, 2, 2, 2, 2, 0, 1, 2, 2, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 0, 2, 2, 1, 2, 1, 1, 1, 0, 1, 2, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 2, 1, 0, 1, 1, 0, 2, 2, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 0, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 2, 0, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 0, 1, 0, 1, 1, 2, 2, 1, 1, 2, 1, 2, 0, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 2, 0, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 2, 2, 0, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 0, 2, 1, 2, 2, 1, 2, 0, 2, 2, 2, 2, 1, 2, 2, 0, 2, 1, 1, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 0, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 0, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 0, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 0, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 2, 2, 2, 2, 1, 1, 0, 1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 0, 2, 0, 0, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 0, 0, 2, 0, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 0, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 1, 0, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 0, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\tAcc: 0.4724\n"
     ]
    }
   ],
   "source": [
    "# Train and test a LGBM model\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "best_params = {'learning_rate': 0.14, \n",
    "                'max_depth': 9, \n",
    "                'n_estimators': 212,\n",
    "                'colsample_bytree': 0.82,\n",
    "                'num_leaves': 49,\n",
    "                'reg_alpha': 0.077,\n",
    "                'reg_lambda': 0.7,\n",
    "                'subsample': 0.93}\n",
    "\n",
    "model = lgb.LGBMClassifier(**best_params)\n",
    "# model = lgb.LGBMClassifier()\n",
    "\n",
    "# for name, model in models:\n",
    "print(\"Model: \", name)\n",
    "\n",
    "Y = train_incubator['class'].values\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(Y)\n",
    "EMQ_result = EMQ(test_scores, train_labels, nclasses)\n",
    "print('EMQ', EMQ_result)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "p_labels = model.predict_proba(X_test)\n",
    "p_pred = []\n",
    "for item in p_labels:\n",
    "    p_pred.append(np.argmax(item * EMQ_result))\n",
    "p_pred = label_encoder.inverse_transform(p_pred)\n",
    "\n",
    "a_labels = y_test\n",
    "acc = accuracy_score(a_labels, p_pred)\n",
    "\n",
    "print(\"\\tAcc: %.4f\" % acc)\n",
    "# print (classification_report(a_labels, p_pred, labels=np.unique(y_test)))\n",
    "      \n",
    "# cf = confusion_matrix(a_labels, p_pred, labels=np.unique(y_train))\n",
    "# print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf250209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4772065955383123\n"
     ]
    }
   ],
   "source": [
    "alpha = 1\n",
    "\n",
    "\n",
    "def EMQ(test_scores, train_labels, nclasses):\n",
    "    max_it = 1000        # Max num of iterations\n",
    "    eps = 1e-6           # Small constant for stopping criterium\n",
    "\n",
    "    p_tr = class_dist(train_labels, nclasses)\n",
    "    p_s = np.copy(p_tr)\n",
    "    p_cond_tr = np.array(test_scores)\n",
    "    p_cond_s = np.zeros(p_cond_tr.shape)\n",
    "    prob_arrays = []\n",
    "\n",
    "    for _ in range(max_it):\n",
    "        # Add Laplacian smoothing\n",
    "        # r = (p_s + alpha) / (p_tr + (alpha * nclasses))\n",
    "        r = p_s / p_tr\n",
    "        \n",
    "        p_cond_s = p_cond_tr * r\n",
    "        s = np.sum(p_cond_s, axis = 1)\n",
    "        for c in range(nclasses):\n",
    "            p_cond_s[:,c] = p_cond_s[:,c] / s\n",
    "\n",
    "        prob_arrays.append(p_cond_s)\n",
    "        p_s_old = np.copy(p_s)\n",
    "        p_s = np.sum(p_cond_s, axis = 0) / p_cond_s.shape[0]\n",
    "        if (np.sum(np.abs(p_s - p_s_old)) < eps):\n",
    "            break\n",
    "\n",
    "    # return(p_s/np.sum(p_s))\n",
    "    return p_cond_s\n",
    "\n",
    "Y = train_incubator['class'].values\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(Y)\n",
    "EMQ_result = EMQ(test_scores, train_labels, nclasses)\n",
    "# print(EMQ_result)\n",
    "p_pred = []\n",
    "for item in EMQ_result:\n",
    "    p_pred.append(np.argmax(item))\n",
    "p_pred = label_encoder.inverse_transform(p_pred)\n",
    "\n",
    "acc = accuracy_score(y_test, p_pred)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp9991",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
