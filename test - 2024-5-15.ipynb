{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b847f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tabulate import tabulate\n",
    "from hyperopt import hp, fmin, tpe, Trials, space_eval\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cvx\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "import quadprog\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d24b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "arabiensis_female    3000\n",
      "culex_female         3000\n",
      "funestus_female      3000\n",
      "gambiae_female       3000\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "gambiae_female       600\n",
      "culex_female         522\n",
      "funestus_female      512\n",
      "arabiensis_female    428\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_incubator = pd.read_csv('train_incubator.csv')\n",
    "test_sf2 = pd.read_csv('test_sf2.csv')\n",
    "\n",
    "# Check number of examples per class\n",
    "print (train_incubator['class'].value_counts())\n",
    "print (test_sf2['class'].value_counts())\n",
    "\n",
    "nclasses = len(train_incubator['class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95be81b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets\n",
    "special_features = ['temperature', 'duration', 'humidity']\n",
    "wbf_features = ['L_harmcherry_wbf_mean','L_harmcherry_wbf_stddev']\n",
    "freq_features = [f'L_harmcherry_h{i}_freq' for i in range(1,9)]\n",
    "basefreq_features = [f'L_harmcherry_h{i}_basefreq' for i in range(1,9)]\n",
    "relbasefreq_features = [f'L_harmcherry_h{i}_relbasefreq' for i in range(1,9)]\n",
    "power_features = [f'L_harmcherry_h{i}_power' for i in range(1,9)]\n",
    "relpower_features = [f'L_harmcherry_h{i}_relpower' for i in range(1,9)]\n",
    "invented_features = [f'L_harmcherry_h{i}_invented' for i in range(1,9)]\n",
    "\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df6d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test a LGBM model\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "models = [('LGBM', lgb.LGBMClassifier())]\n",
    "\n",
    "for name, model in models:\n",
    "    print(\"Model: \", name)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    p_labels = model.predict(X_test)\n",
    "    a_labels = y_test\n",
    "    acc = accuracy_score(a_labels, p_labels)\n",
    "    print('number: ', len(a_labels))\n",
    "    \n",
    "    print(\"\\tAcc: %.4f\" % acc)\n",
    "    print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "        \n",
    "    cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "    print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0bc3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "# class_weights = {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0}\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(class_weights='balanced')\n",
    "\n",
    "# 训练模型\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# 获取特征重要性\n",
    "feature_importance = lgb_model.feature_importances_\n",
    "\n",
    "new_features = []\n",
    "\n",
    "# 打印特征重要性\n",
    "for i, importance in enumerate(feature_importance):\n",
    "    if ((importance - 1) > 0):\n",
    "        print(f\"Feature {i+1}: Importance = {importance}, {feature_set[i]}\")\n",
    "        new_features.append(feature_set[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22cd6c",
   "metadata": {},
   "source": [
    "Test's confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e39f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test a LGBM model\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "    \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "p_labels = model.predict(X_test)\n",
    "a_labels = y_test\n",
    "acc = accuracy_score(a_labels, p_labels)\n",
    "print('number: ', len(a_labels))\n",
    "\n",
    "print(\"\\tAcc: %.4f\" % acc)\n",
    "print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "    \n",
    "cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5fc6e",
   "metadata": {},
   "source": [
    "Lab's confustion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8f2da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y = train_incubator['class'].values \n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "groups = train_incubator['sensor'].values\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "for train_index, test_index in group_kfold.split(X, y, groups):\n",
    "  X_train, y_train, X_test, y_test = X[train_index], y[train_index], X[test_index], y[test_index]\n",
    "  model.fit(X[train_index], y[train_index])\n",
    "\n",
    "  p_labels = model.predict(X_test)\n",
    "  a_labels = y_test\n",
    "  acc = accuracy_score(a_labels, p_labels)\n",
    "  print('number: ', len(a_labels))\n",
    "\n",
    "  print(\"\\tAcc: %.4f\" % acc)\n",
    "  print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "      \n",
    "  cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "  print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcbabbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScores(X_train, X_test, Y_train, nclasses):\n",
    "\n",
    "    # model = lgb.LGBMClassifier(**best_params)\n",
    "    model = lgb.LGBMClassifier()\n",
    "   \n",
    "    train_scores = np.zeros((len(X_train), nclasses))\n",
    "    test_scores = np.zeros((len(X_test), nclasses))\n",
    "\n",
    "    groups = train_incubator['sensor'].values\n",
    "    group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "    for train_index, test_index in group_kfold.split(X_train, Y_train, groups):\n",
    "        model.fit(X_train[train_index], Y_train[train_index])\n",
    "        train_scores[test_index] = model.predict_proba(X_train)[test_index]\n",
    "    \n",
    "    model.fit(X_train, Y_train)\n",
    "    test_scores = model.predict_proba(X_test)\n",
    "           \n",
    "    return train_scores, test_scores\n",
    "\n",
    "def EMQ(test_scores, nclasses):\n",
    "    max_it = 1000        # Max num of iterations\n",
    "    eps = 1e-1           # Small constant for stopping criterium\n",
    "\n",
    "    p_tr = [0.25, 0.25, 0.25, 0.25]\n",
    "    p_s = np.copy(p_tr)\n",
    "    p_cond_tr = np.array(test_scores)\n",
    "    p_cond_s = np.zeros(p_cond_tr.shape)\n",
    "    prob_arrays = []\n",
    "\n",
    "    for _ in range(max_it):\n",
    "        # Add Laplacian smoothing\n",
    "        # r = (p_s + alpha) / (p_tr + (alpha * nclasses))\n",
    "        r = p_s / p_tr\n",
    "        \n",
    "        p_cond_s = p_cond_tr * r\n",
    "        s = np.sum(p_cond_s, axis = 1)\n",
    "        for c in range(nclasses):\n",
    "            p_cond_s[:,c] = p_cond_s[:,c] / s\n",
    "\n",
    "        prob_arrays.append(p_cond_s)\n",
    "        p_s_old = np.copy(p_s)\n",
    "        p_s = np.sum(p_cond_s, axis = 0) / p_cond_s.shape[0]\n",
    "        if (np.sum(np.abs(p_s - p_s_old)) < eps):\n",
    "            break\n",
    "\n",
    "    return(p_s/np.sum(p_s))\n",
    "    # return p_cond_s\n",
    "\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).to_numpy()\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "train_scores, test_scores = getScores(X_train, X_test, y_train, 4)\n",
    "res = EMQ(test_scores, 4)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738711a",
   "metadata": {},
   "source": [
    "Probabilistic Classify and Count (PCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fca72584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "Estimated counts using PCC:\n",
      "arabiensis_female: 528.99\n",
      "culex_female: 522.31\n",
      "funestus_female: 224.69\n",
      "gambiae_female: 786.01\n"
     ]
    }
   ],
   "source": [
    "# 将数据集拆分为训练集和测试集\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).to_numpy()\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "# 训练一个概率分类器（这里使用随机森林分类器）\n",
    "model = lgb.LGBMClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 预测测试集的后验概率\n",
    "proba_predictions = model.predict_proba(X_test)\n",
    "class_labels = model.classes_\n",
    "\n",
    "# 使用PCC方法估计每个类别的数量\n",
    "# 通过计算每个类别后验概率的平均值并乘以总样本数，得到每个类别的估计数量\n",
    "def pcc(proba_predictions, class_labels):\n",
    "    estimated_counts = np.mean(proba_predictions, axis=0) * len(proba_predictions)\n",
    "    return dict(zip(class_labels, estimated_counts))\n",
    "\n",
    "estimated_counts = pcc(proba_predictions, class_labels)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Estimated counts using PCC:\")\n",
    "for label, count in estimated_counts.items():\n",
    "    print(f\"{label}: {count:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d7215",
   "metadata": {},
   "source": [
    "Expectation Maximisation for Quantification(EMQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8931433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScores(X_train, X_test, Y_train, nclasses):\n",
    "\n",
    "    # model = lgb.LGBMClassifier(**best_params)\n",
    "    model = lgb.LGBMClassifier()\n",
    "   \n",
    "    train_scores = np.zeros((len(X_train), nclasses))\n",
    "    test_scores = np.zeros((len(X_test), nclasses))\n",
    "\n",
    "    groups = train_incubator['sensor'].values\n",
    "    group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "    for train_index, test_index in group_kfold.split(X_train, Y_train, groups):\n",
    "        model.fit(X_train[train_index], Y_train[train_index])\n",
    "        train_scores[test_index] = model.predict_proba(X_train)[test_index]\n",
    "    \n",
    "    model.fit(X_train, Y_train)\n",
    "    test_scores = model.predict_proba(X_test)\n",
    "           \n",
    "    return train_scores, test_scores\n",
    "\n",
    "def pcc(test_scores, class_labels):\n",
    "    estimated_counts = np.mean(test_scores, axis=0) * len(test_scores)\n",
    "    return dict(zip(class_labels, estimated_counts))\n",
    "\n",
    "def EMQ(test_scores, nclasses):\n",
    "    max_it = 1000        # Max num of iterations\n",
    "    eps = 1e-1           # Small constant for stopping criterium\n",
    "\n",
    "    p_tr = [0.25, 0.25, 0.25, 0.25]\n",
    "    p_s = np.copy(p_tr)\n",
    "    p_cond_tr = np.array(test_scores)\n",
    "    p_cond_s = np.zeros(p_cond_tr.shape)\n",
    "    prob_arrays = []\n",
    "\n",
    "    for _ in range(max_it):\n",
    "        # Add Laplacian smoothing\n",
    "        # r = (p_s + alpha) / (p_tr + (alpha * nclasses))\n",
    "        r = p_s / p_tr\n",
    "        \n",
    "        p_cond_s = p_cond_tr * r\n",
    "        s = np.sum(p_cond_s, axis = 1)\n",
    "        for c in range(nclasses):\n",
    "            p_cond_s[:,c] = p_cond_s[:,c] / s\n",
    "\n",
    "        prob_arrays.append(p_cond_s)\n",
    "        p_s_old = np.copy(p_s)\n",
    "        p_s = np.sum(p_cond_s, axis = 0) / p_cond_s.shape[0]\n",
    "        if (np.sum(np.abs(p_s - p_s_old)) < eps):\n",
    "            break\n",
    "\n",
    "    return (p_s/np.sum(p_s))\n",
    "    # return p_cond_s \n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).to_numpy()\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "nclasses = 4\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "# y_proba = model.predict_proba(X_test)\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "train_scores, test_scores = getScores(X_train, X_test, y_train, nclasses)\n",
    "\n",
    "pcc_estimated_counts = pcc(test_scores, class_labels)\n",
    "\n",
    "emq_estimated_counts = EMQ(test_scores, nclasses)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Estimated counts using PCC:\")\n",
    "for label, count in pcc_estimated_counts.items():\n",
    "    print(f\"{label}: {count:.2f}\")\n",
    "\n",
    "print(\"Estimated counts using EMQ:\")\n",
    "print(emq_estimated_counts * len(test_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48385a",
   "metadata": {},
   "source": [
    "Probabilistic Adjusted Classify and Count(PACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a963a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PACC(proba_predictions, y_true):\n",
    "    # 计算真实类别的数量\n",
    "    class_counts = {'A': 428, 'B': 522, 'C': 512, 'D': 600}\n",
    "    \n",
    "    # 计算各个类别的条件期望\n",
    "    expected_conditionals = []\n",
    "    for class_label in np.unique(y_true):\n",
    "        class_indices = np.where(y_true == class_label)[0]\n",
    "        class_proba_predictions = proba_predictions[class_indices]\n",
    "        class_expected_conditional = np.mean(class_proba_predictions, axis=0)\n",
    "        expected_conditionals.append(class_expected_conditional)\n",
    "    \n",
    "    # 计算每个类别的估计数量\n",
    "    estimated_counts = []\n",
    "    for class_label, class_count in enumerate(class_counts):\n",
    "        expected_conditional = expected_conditionals[class_label]\n",
    "        print(expected_conditional)\n",
    "        estimated_count = np.sum(expected_conditional) * class_count\n",
    "        estimated_counts.append(estimated_count)\n",
    "    \n",
    "    # 将类别标签和估计数量对应起来，形成字典\n",
    "    class_labels = np.unique(y_true)\n",
    "    estimated_counts_dict = dict(zip(class_labels, estimated_counts))\n",
    "    \n",
    "    return estimated_counts_dict\n",
    "\n",
    "# 使用示例\n",
    "# proba_predictions 是测试集的后验概率预测结果，shape为(n_samples, n_classes)\n",
    "# y_true 是测试集的真实类别标签\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).to_numpy()\n",
    "y_test = test_sf2['class']\n",
    "\n",
    "nclasses = 4\n",
    "\n",
    "# train_scores, test_scores = getScores(X_train, X_test, y_train, nclasses)\n",
    "\n",
    "estimated_counts = PACC(test_scores, y_test)\n",
    "print(\"Estimated Counts:\", estimated_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680ea50",
   "metadata": {},
   "source": [
    "### Estimate the probability distribution for each class using different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a7cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_dist(Y, nclasses):\n",
    "    return np.array([np.count_nonzero(Y == i) for i in range(nclasses)]) / Y.shape[0]\n",
    "\n",
    "Y = train_incubator['class'].values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y)\n",
    "\n",
    "classes = class_dist(Y_encoded, nclasses)\n",
    "\n",
    "print(np.unique(Y))\n",
    "print(Y.shape[0])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3afd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "import lightgbm\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "def getScores(X_train, X_test, Y_train, nclasses):\n",
    "\n",
    "    models = [linear_model.LogisticRegression(solver='liblinear', multi_class='ovr'),\n",
    "              discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "              ensemble.RandomForestClassifier(),\n",
    "              svm.SVC(probability=True),\n",
    "              lightgbm.LGBMClassifier(),\n",
    "              naive_bayes.GaussianNB(),\n",
    "              ensemble.GradientBoostingClassifier()]\n",
    "   \n",
    "    train_scores = np.zeros((len(models), len(X_train), nclasses))\n",
    "    test_scores = np.zeros((len(models), len(X_test), nclasses))\n",
    "    for i, model in enumerate(models):\n",
    "        Y_cts = np.unique(Y_train, return_counts=True)\n",
    "        nfolds = min(10, min(Y_cts[1]))\n",
    "       \n",
    "        if nfolds > 1:\n",
    "            kfold = model_selection.StratifiedKFold(n_splits=nfolds, random_state=1, shuffle=True)\n",
    "            for train, test in kfold.split(X_train, Y_train):\n",
    "                model.fit(X_train[train], Y_train[train])\n",
    "                train_scores[i][test] = model.predict_proba(X_train)[test]\n",
    "       \n",
    "        model.fit(X_train, Y_train)\n",
    "        test_scores[i] = model.predict_proba(X_test)\n",
    "       \n",
    "        if nfolds < 2:\n",
    "            train_scores[i] = model.predict_proba(X_train)\n",
    "           \n",
    "    return train_scores, test_scores, len(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184eced7",
   "metadata": {},
   "source": [
    "集成学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbbd0188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  2\n",
      "number:  2062\n",
      "\tAcc: 0.5485\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.32      0.44      0.37       428\n",
      "     culex_female       0.61      0.55      0.58       522\n",
      "  funestus_female       0.76      0.76      0.76       512\n",
      "   gambiae_female       0.54      0.45      0.49       600\n",
      "\n",
      "         accuracy                           0.55      2062\n",
      "        macro avg       0.56      0.55      0.55      2062\n",
      "     weighted avg       0.57      0.55      0.55      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 188 │             94 │                33 │              113 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 204 │            287 │                 9 │               22 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  18 │             15 │               387 │               92 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 180 │             73 │                78 │              269 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "import lightgbm\n",
    "from sklearn import naive_bayes\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Train and test a LGBM model\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "models = [\n",
    "        # ('Linear', linear_model.LogisticRegression(solver='liblinear', multi_class='ovr')),\n",
    "        ('2', discriminant_analysis.LinearDiscriminantAnalysis()),\n",
    "        # ('3', ensemble.RandomForestClassifier()),\n",
    "        # ('4', svm.SVC(probability=True)),\n",
    "        # ('5', lightgbm.LGBMClassifier()),\n",
    "        # ('6', naive_bayes.GaussianNB()),\n",
    "        # ('7', ensemble.GradientBoostingClassifier())\n",
    "        ]\n",
    "\n",
    "for name, model in models:\n",
    "    print(\"Model: \", name)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    p_labels = model.predict(X_test)\n",
    "    a_labels = y_test\n",
    "    acc = accuracy_score(a_labels, p_labels)\n",
    "    print('number: ', len(a_labels))\n",
    "    \n",
    "    print(\"\\tAcc: %.4f\" % acc)\n",
    "    print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "        \n",
    "    cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "    print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acd79dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number:  2605\n",
      "\tAcc: 0.4656\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.32      0.27      0.29       636\n",
      "     culex_female       0.42      0.72      0.53       495\n",
      "  funestus_female       0.67      0.73      0.70       713\n",
      "   gambiae_female       0.37      0.22      0.28       761\n",
      "\n",
      "         accuracy                           0.47      2605\n",
      "        macro avg       0.45      0.48      0.45      2605\n",
      "     weighted avg       0.45      0.47      0.44      2605\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 172 │            209 │               104 │              151 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  81 │            354 │                 3 │               57 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  38 │             78 │               519 │               78 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 248 │            201 │               144 │              168 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "number:  2362\n",
      "\tAcc: 0.5055\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.29      0.06      0.10       694\n",
      "     culex_female       0.54      0.70      0.61       638\n",
      "  funestus_female       0.74      0.86      0.79       464\n",
      "   gambiae_female       0.36      0.54      0.43       566\n",
      "\n",
      "         accuracy                           0.51      2362\n",
      "        macro avg       0.48      0.54      0.48      2362\n",
      "     weighted avg       0.46      0.51      0.45      2362\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  41 │            227 │                61 │              365 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  41 │            448 │                 6 │              143 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  16 │              9 │               400 │               39 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  44 │            140 │                77 │              305 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "number:  2039\n",
      "\tAcc: 0.5213\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.53      0.24      0.33       614\n",
      "     culex_female       0.54      0.69      0.61       473\n",
      "  funestus_female       0.66      0.81      0.73       569\n",
      "   gambiae_female       0.28      0.34      0.31       383\n",
      "\n",
      "         accuracy                           0.52      2039\n",
      "        macro avg       0.50      0.52      0.49      2039\n",
      "     weighted avg       0.52      0.52      0.50      2039\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 145 │            175 │                89 │              205 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  55 │            327 │                 8 │               83 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  15 │             45 │               461 │               48 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  60 │             56 │               137 │              130 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "number:  1938\n",
      "\tAcc: 0.4907\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.36      0.13      0.19       589\n",
      "     culex_female       0.58      0.61      0.59       445\n",
      "  funestus_female       0.75      0.79      0.77       629\n",
      "   gambiae_female       0.17      0.37      0.24       275\n",
      "\n",
      "         accuracy                           0.49      1938\n",
      "        macro avg       0.47      0.48      0.45      1938\n",
      "     weighted avg       0.51      0.49      0.48      1938\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  78 │            130 │                77 │              304 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  63 │            271 │                 1 │              110 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  29 │             32 │               500 │               68 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  46 │             34 │                93 │              102 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "number:  1746\n",
      "\tAcc: 0.4931\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.29      0.49      0.36       326\n",
      "     culex_female       0.77      0.58      0.66       699\n",
      "  funestus_female       0.48      0.79      0.60       258\n",
      "   gambiae_female       0.38      0.21      0.27       463\n",
      "\n",
      "         accuracy                           0.49      1746\n",
      "        macro avg       0.48      0.52      0.47      1746\n",
      "     weighted avg       0.53      0.49      0.49      1746\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 159 │             40 │                69 │               58 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 194 │            402 │                32 │               71 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  22 │              5 │               204 │               27 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 174 │             76 │               117 │               96 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "number:  1310\n",
      "\tAcc: 0.4718\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.20      0.63      0.31       141\n",
      "     culex_female       0.49      0.55      0.52       250\n",
      "  funestus_female       0.66      0.73      0.69       367\n",
      "   gambiae_female       0.68      0.22      0.33       552\n",
      "\n",
      "         accuracy                           0.47      1310\n",
      "        macro avg       0.51      0.53      0.46      1310\n",
      "     weighted avg       0.59      0.47      0.47      1310\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  89 │             19 │                12 │               21 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  74 │            138 │                14 │               24 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  51 │             34 │               269 │               13 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 225 │             92 │               113 │              122 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y = train_incubator['class'].values \n",
    "\n",
    "model = discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "\n",
    "groups = train_incubator['sensor'].values\n",
    "group_kfold = GroupKFold(n_splits=6)\n",
    "\n",
    "for train_index, test_index in group_kfold.split(X, y, groups):\n",
    "  X_train, y_train, X_test, y_test = X[train_index], y[train_index], X[test_index], y[test_index]\n",
    "  model.fit(X[train_index], y[train_index])\n",
    "\n",
    "  p_labels = model.predict(X_test)\n",
    "  a_labels = y_test\n",
    "  acc = accuracy_score(a_labels, p_labels)\n",
    "  print('number: ', len(a_labels))\n",
    "\n",
    "  print(\"\\tAcc: %.4f\" % acc)\n",
    "  print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "      \n",
    "  cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "  print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a8c2a",
   "metadata": {},
   "source": [
    "LDA model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f44ab1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated counts using PCC:\n",
      "arabiensis_female: 585.21\n",
      "culex_female: 465.87\n",
      "funestus_female: 465.65\n",
      "gambiae_female: 545.28\n",
      "Estimated counts using EMQ:\n",
      "[585.20590705 465.86686026 465.64985039 545.27738229]\n"
     ]
    }
   ],
   "source": [
    "def class_dist(Y, nclasses):\n",
    "    return np.array([np.count_nonzero(Y == i) for i in range(nclasses)]) / Y.shape[0]\n",
    "\n",
    "def getScores(X_train, X_test, Y_train, nclasses):\n",
    "\n",
    "    # model = ensemble.RandomForestClassifier()\n",
    "    model = discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "    # model = lgb.LGBMClassifier()\n",
    "   \n",
    "    train_scores = np.zeros((len(X_train), nclasses))\n",
    "    test_scores = np.zeros((len(X_test), nclasses))\n",
    "\n",
    "    groups = train_incubator['sensor'].values\n",
    "    group_kfold = GroupKFold(n_splits=6)\n",
    "\n",
    "    for train_index, test_index in group_kfold.split(X_train, Y_train, groups):\n",
    "        model.fit(X_train[train_index], Y_train[train_index])\n",
    "        train_scores[test_index] = model.predict_proba(X_train)[test_index]\n",
    "    \n",
    "    model.fit(X_train, Y_train)\n",
    "    test_scores = model.predict_proba(X_test)\n",
    "           \n",
    "    return train_scores, test_scores\n",
    "\n",
    "def pcc(test_scores, class_labels):\n",
    "    estimated_counts = np.mean(test_scores, axis=0) * len(test_scores)\n",
    "    return dict(zip(class_labels, estimated_counts))\n",
    "\n",
    "def EMQ(test_scores, train_labels, nclasses):\n",
    "    max_it = 1000        # Max num of iterations\n",
    "    eps = 1e-1           # Small constant for stopping criterium\n",
    "\n",
    "    p_tr = class_dist(train_labels, nclasses)\n",
    "    p_s = np.copy(p_tr)\n",
    "    p_cond_tr = np.array(test_scores)\n",
    "    p_cond_s = np.zeros(p_cond_tr.shape)\n",
    "    prob_arrays = []\n",
    "\n",
    "    for _ in range(max_it):\n",
    "        # Add Laplacian smoothing\n",
    "        # r = (p_s + alpha) / (p_tr + (alpha * nclasses))\n",
    "        r = p_s / p_tr\n",
    "        \n",
    "        p_cond_s = p_cond_tr * r\n",
    "        s = np.sum(p_cond_s, axis = 1)\n",
    "        for c in range(nclasses):\n",
    "            p_cond_s[:,c] = p_cond_s[:,c] / s\n",
    "\n",
    "        prob_arrays.append(p_cond_s)\n",
    "        p_s_old = np.copy(p_s)\n",
    "        p_s = np.sum(p_cond_s, axis = 0) / p_cond_s.shape[0]\n",
    "        if (np.sum(np.abs(p_s - p_s_old)) < eps):\n",
    "            break\n",
    "\n",
    "    return (p_s/np.sum(p_s))\n",
    "    # return p_cond_s \n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).to_numpy()\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "nclasses = 4\n",
    "\n",
    "train_scores, test_scores = getScores(X_train, X_test, y_train, nclasses)\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "pcc_estimated_counts = pcc(test_scores, class_labels)\n",
    "\n",
    "emq_estimated_counts = EMQ(test_scores, Y_encoded, nclasses)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Estimated counts using PCC:\")\n",
    "for label, count in pcc_estimated_counts.items():\n",
    "    print(f\"{label}: {count:.2f}\")\n",
    "\n",
    "print(\"Estimated counts using EMQ:\")\n",
    "print(emq_estimated_counts * len(test_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b02f8",
   "metadata": {},
   "source": [
    "Generalized Assignment Configuration(GAC) 通过优化问题估计类别的概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "210738dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.57350549e-11 8.99331869e-02 6.71615086e-02 8.42905304e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\comp9991\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:336: FutureWarning: \n",
      "    Your problem is being solved with the ECOS solver by default. Starting in \n",
      "    CVXPY 1.5.0, Clarabel will be used as the default solver instead. To continue \n",
      "    using ECOS, specify the ECOS solver explicitly using the ``solver=cp.ECOS`` \n",
      "    argument to the ``problem.solve`` method.\n",
      "    \n",
      "  warnings.warn(ECOS_DEPRECATION_MSG, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def GAC(train_scores, test_scores, train_labels, nclasses):\n",
    "    yt_hat = np.argmax(train_scores, axis=1)\n",
    "    y_hat = np.argmax(test_scores, axis=1)\n",
    "    CM = metrics.confusion_matrix(train_labels, yt_hat, normalize=\"true\").T\n",
    "    p_y_hat = np.zeros(nclasses)\n",
    "    values, counts = np.unique(y_hat, return_counts=True)\n",
    "    p_y_hat[values] = counts\n",
    "    p_y_hat = p_y_hat / p_y_hat.sum()\n",
    "\n",
    "    p_hat = cvx.Variable(CM.shape[1])\n",
    "    constraints = [p_hat >= 0, cvx.sum(p_hat) == 1.0]\n",
    "    problem = cvx.Problem(cvx.Minimize(cvx.norm(CM @ p_hat - p_y_hat)), constraints)\n",
    "    problem.solve()\n",
    "    return p_hat.value\n",
    "\n",
    "gac_estimated_counts = GAC(train_scores, test_scores, Y_encoded, nclasses)\n",
    "print(gac_estimated_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "23b9aaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.20566836e-09 1.11678516e-01 1.19318044e-01 7.69003435e-01]\n"
     ]
    }
   ],
   "source": [
    "def GPAC(train_scores, test_scores, train_labels, nclasses):\n",
    "    CM = np.zeros((nclasses, nclasses))\n",
    "    for i in range(nclasses):\n",
    "        idx = np.where(train_labels == i)[0]\n",
    "        CM[i] = np.sum(train_scores[idx], axis=0)\n",
    "        CM[i] /= np.sum(CM[i])\n",
    "    CM = CM.T\n",
    "    p_y_hat = np.sum(test_scores, axis=0)\n",
    "    p_y_hat = p_y_hat / np.sum(p_y_hat)\n",
    "\n",
    "    p_hat = cvx.Variable(CM.shape[1])\n",
    "    constraints = [p_hat >= 0, cvx.sum(p_hat) == 1.0]\n",
    "    problem = cvx.Problem(cvx.Minimize(cvx.norm(CM @ p_hat - p_y_hat)), constraints)\n",
    "    problem.solve()\n",
    "    return p_hat.value\n",
    "\n",
    "gpac_estimated_counts = GPAC(train_scores, test_scores, Y_encoded, nclasses)\n",
    "print(gpac_estimated_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b674536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.49914854e-09 1.31445411e-01 5.94857615e-02 8.09068826e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\comp9991\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:336: FutureWarning: \n",
      "    Your problem is being solved with the ECOS solver by default. Starting in \n",
      "    CVXPY 1.5.0, Clarabel will be used as the default solver instead. To continue \n",
      "    using ECOS, specify the ECOS solver explicitly using the ``solver=cp.ECOS`` \n",
      "    argument to the ``problem.solve`` method.\n",
      "    \n",
      "  warnings.warn(ECOS_DEPRECATION_MSG, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def FM(train_scores, test_scores, train_labels, nclasses):\n",
    "    CM = np.zeros((nclasses, nclasses))\n",
    "    y_cts = np.array([np.count_nonzero(train_labels == i) for i in range(nclasses)])\n",
    "    p_yt = y_cts / train_labels.shape[0]\n",
    "    for i in range(nclasses):\n",
    "        idx = np.where(train_labels == i)[0]\n",
    "        CM[:, i] += np.sum(train_scores[idx] > p_yt, axis=0)\n",
    "    CM = CM / y_cts\n",
    "    p_y_hat = np.sum(test_scores > p_yt, axis=0) / test_scores.shape[0]\n",
    "\n",
    "    p_hat = cvx.Variable(CM.shape[1])\n",
    "    constraints = [p_hat >= 0, cvx.sum(p_hat) == 1.0]\n",
    "    problem = cvx.Problem(cvx.Minimize(cvx.norm(CM @ p_hat - p_y_hat)), constraints)\n",
    "    problem.solve()\n",
    "    return p_hat.value\n",
    "\n",
    "fm_estimated_counts = FM(train_scores, test_scores, Y_encoded, nclasses)\n",
    "print(fm_estimated_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "39a868cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08252292 0.16024792 0.14413187 0.61309729]\n"
     ]
    }
   ],
   "source": [
    "def dpofa(m):\n",
    "    r = np.array(m, copy=True)\n",
    "    n = len(r)\n",
    "    for k in range(n):\n",
    "        s = 0.0\n",
    "        if k >= 1:\n",
    "            for i in range(k):\n",
    "                t = r[i, k]\n",
    "                if i > 0:\n",
    "                    t = t - np.sum(r[0:i, i] * r[0:i, k])\n",
    "                t = t / r[i, i]\n",
    "                r[i, k] = t\n",
    "                s = s + t * t\n",
    "        s = r[k, k] - s\n",
    "        if s <= 0.0:\n",
    "            return k + 1, r\n",
    "        r[k, k] = np.sqrt(s)\n",
    "    return 0, r\n",
    "\n",
    "def is_pd(m):\n",
    "    return dpofa(m)[0] == 0\n",
    "\n",
    "def solve_ed(G, a, C, b):\n",
    "    sol = quadprog.solve_qp(G=G, a=a, C=C, b=b)\n",
    "    prevalences = sol[0]\n",
    "    # the last class was removed from the problem, its prevalence is 1 - the sum of prevalences for the other classes\n",
    "    return np.append(prevalences, 1 - prevalences.sum())\n",
    "\n",
    "def compute_ed_param_train(distance_func, train_distrib, classes, n_cls_i):\n",
    "    n_classes = len(classes)\n",
    "    #  computing sum de distances for each pair of classes\n",
    "    K = np.zeros((n_classes, n_classes))\n",
    "    for i in range(n_classes):\n",
    "        K[i, i] = distance_func(train_distrib[classes[i]], train_distrib[classes[i]]).sum()\n",
    "        for j in range(i + 1, n_classes):\n",
    "            K[i, j] = distance_func(train_distrib[classes[i]], train_distrib[classes[j]]).sum()\n",
    "            K[j, i] = K[i, j]\n",
    "\n",
    "    #  average distance\n",
    "    K = K / np.dot(n_cls_i, n_cls_i.T)\n",
    "\n",
    "    B = np.zeros((n_classes - 1, n_classes - 1))\n",
    "    for i in range(n_classes - 1):\n",
    "        B[i, i] = - K[i, i] - K[-1, -1] + 2 * K[i, -1]\n",
    "        for j in range(n_classes - 1):\n",
    "            if j == i:\n",
    "                continue\n",
    "            B[i, j] = - K[i, j] - K[-1, -1] + K[i, -1] + K[j, -1]\n",
    "\n",
    "    #  computing the terms for the optimization problem\n",
    "    G = 2 * B\n",
    "    if not is_pd(G):\n",
    "        G = nearest_pd(G)\n",
    "\n",
    "    C = -np.vstack([np.ones((1, n_classes - 1)), -np.eye(n_classes - 1)]).T\n",
    "    b = -np.array([1] + [0] * (n_classes - 1), dtype=float)\n",
    "\n",
    "    return K, G, C, b\n",
    "\n",
    "def compute_ed_param_test(distance_func, train_distrib, test_distrib, K, classes, n_cls_i):\n",
    "    n_classes = len(classes)\n",
    "    Kt = np.zeros(n_classes)\n",
    "    for i in range(n_classes):\n",
    "        Kt[i] = distance_func(train_distrib[classes[i]], test_distrib).sum()\n",
    "\n",
    "    Kt = Kt / (n_cls_i.squeeze() * float(len(test_distrib)))\n",
    "\n",
    "    a = 2 * (- Kt[:-1] + K[:-1, -1] + Kt[-1] - K[-1, -1])\n",
    "    return a\n",
    "\n",
    "def EDy(tr_scores, labels, te_scores, nclasses):\n",
    "    distance = manhattan_distances\n",
    "    classes_ = np.unique(labels)\n",
    "    train_distrib_ = dict.fromkeys(classes_)\n",
    "    train_n_cls_i_ = np.zeros((nclasses, 1))\n",
    "\n",
    "    if len(labels) == len(tr_scores):\n",
    "        y_ext_ = labels\n",
    "    else:\n",
    "        y_ext_ = np.tile(labels, len(tr_scores) // len(labels))\n",
    "\n",
    "    for n_cls, cls in enumerate(classes_):\n",
    "        train_distrib_[cls] = tr_scores[y_ext_ == cls, :]\n",
    "        train_n_cls_i_[n_cls, 0] = len(train_distrib_[cls])\n",
    "\n",
    "    K_, G_, C_, b_ = compute_ed_param_train(distance, train_distrib_, classes_, train_n_cls_i_)\n",
    "\n",
    "    a_ = compute_ed_param_test(distance, train_distrib_, te_scores, K_, classes_, train_n_cls_i_)\n",
    "\n",
    "    prevalences = solve_ed(G=G_, a=a_, C=C_, b=b_)\n",
    "\n",
    "    return prevalences / np.sum(prevalences)\n",
    "\n",
    "edy_estimated_counts = EDy(train_scores, Y_encoded, test_scores, nclasses)\n",
    "print(edy_estimated_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6577b54",
   "metadata": {},
   "source": [
    "尝试混合模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f435e531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated counts using PCC:\n",
      "arabiensis_female: 530.88\n",
      "culex_female: 418.03\n",
      "funestus_female: 553.56\n",
      "gambiae_female: 559.52\n",
      "Estimated counts using EMQ:\n",
      "[530.87941821 418.03245866 553.56441286 559.52371028]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import cvxpy as cvx\n",
    "\n",
    "def class_dist(Y, nclasses):\n",
    "    return np.array([np.count_nonzero(Y == i) for i in range(nclasses)]) / len(Y)\n",
    "\n",
    "def getScores(X_train, X_test, Y_train, nclasses):\n",
    "    models = [\n",
    "        # linear_model.LogisticRegression(solver='liblinear', multi_class='ovr'),\n",
    "        LinearDiscriminantAnalysis(),\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "        SVC(probability=True),\n",
    "        # lightgbm.LGBMClassifier(),\n",
    "        # ensemble.GradientBoostingClassifier()\n",
    "    ]\n",
    "\n",
    "    train_scores = np.zeros((len(models), len(X_train), nclasses))\n",
    "    test_scores = np.zeros((len(models), len(X_test), nclasses))\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        Y_cts = np.unique(Y_train, return_counts=True)\n",
    "        nfolds = min(10, min(Y_cts[1]))\n",
    "        \n",
    "        if nfolds > 1:\n",
    "            kfold = model_selection.StratifiedKFold(n_splits=nfolds, random_state=1, shuffle=True)\n",
    "            for train, test in kfold.split(X_train, Y_train):\n",
    "                model.fit(X_train[train], Y_train[train])\n",
    "                train_scores[i][test] = model.predict_proba(X_train[test])\n",
    "        \n",
    "        model.fit(X_train, Y_train)\n",
    "        test_scores[i] = model.predict_proba(X_test)\n",
    "        \n",
    "        if nfolds < 2:\n",
    "            train_scores[i] = model.predict_proba(X_train)\n",
    "            \n",
    "    return train_scores, test_scores\n",
    "\n",
    "def pcc(test_scores, class_labels):\n",
    "    estimated_counts = np.mean(test_scores, axis=0) * len(test_scores)\n",
    "    return dict(zip(class_labels, estimated_counts))\n",
    "\n",
    "def EMQ(test_scores, nclasses):\n",
    "    max_it = 1000        # Max num of iterations\n",
    "    eps = 1e-1           # Small constant for stopping criterium\n",
    "\n",
    "    p_tr = [0.25, 0.25, 0.25, 0.25]\n",
    "    p_s = np.copy(p_tr)\n",
    "    p_cond_tr = np.array(test_scores)\n",
    "    p_cond_s = np.zeros(p_cond_tr.shape)\n",
    "    prob_arrays = []\n",
    "\n",
    "    for _ in range(max_it):\n",
    "        # Add Laplacian smoothing\n",
    "        # r = (p_s + alpha) / (p_tr + (alpha * nclasses))\n",
    "        r = p_s / p_tr\n",
    "        \n",
    "        p_cond_s = p_cond_tr * r\n",
    "        s = np.sum(p_cond_s, axis = 1)\n",
    "        for c in range(nclasses):\n",
    "            p_cond_s[:,c] = p_cond_s[:,c] / s\n",
    "\n",
    "        prob_arrays.append(p_cond_s)\n",
    "        p_s_old = np.copy(p_s)\n",
    "        p_s = np.sum(p_cond_s, axis = 0) / p_cond_s.shape[0]\n",
    "        if (np.sum(np.abs(p_s - p_s_old)) < eps):\n",
    "            break\n",
    "\n",
    "    return (p_s/np.sum(p_s))\n",
    "\n",
    "def GAC(train_scores, test_scores, train_labels, nclasses):\n",
    "    yt_hat = np.argmax(train_scores, axis=2).flatten()\n",
    "    y_hat = np.argmax(test_scores, axis=2).flatten()\n",
    "    CM = metrics.confusion_matrix(train_labels, yt_hat, normalize=\"true\").T\n",
    "    p_y_hat = np.zeros(nclasses)\n",
    "    values, counts = np.unique(y_hat, return_counts=True)\n",
    "    p_y_hat[values] = counts\n",
    "    p_y_hat = p_y_hat / p_y_hat.sum()\n",
    "\n",
    "    p_hat = cvx.Variable(CM.shape[1])\n",
    "    constraints = [p_hat >= 0, cvx.sum(p_hat) == 1.0]\n",
    "    problem = cvx.Problem(cvx.Minimize(cvx.norm(CM @ p_hat - p_y_hat)), constraints)\n",
    "    problem.solve()\n",
    "    return p_hat.value\n",
    "\n",
    "# 定义特征集\n",
    "feature_set = special_features + wbf_features + freq_features + basefreq_features + relbasefreq_features + power_features\n",
    "\n",
    "# 准备训练和测试数据\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).to_numpy()\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "nclasses = 4\n",
    "\n",
    "# 获取训练和测试得分\n",
    "train_scores, test_scores = getScores(X_train, X_test, y_train, nclasses)\n",
    "\n",
    "# 编码训练标签\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# 使用 GAC 估计类别分布\n",
    "train_scores_combined = train_scores.mean(axis=0)\n",
    "test_scores_combined = test_scores.mean(axis=0)\n",
    "\n",
    "pcc_estimated_counts = pcc(test_scores_combined, class_labels)\n",
    "\n",
    "emq_estimated_proportion = EMQ(test_scores_combined, Y_encoded, nclasses)\n",
    "emq_estimated_counts = emq_estimated_proportion * len(test_scores_combined)\n",
    "\n",
    "# gac_estimated_counts = GAC(train_scores_combined, test_scores_combined, Y_encoded, nclasses)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Estimated counts using PCC:\")\n",
    "for label, count in pcc_estimated_counts.items():\n",
    "    print(f\"{label}: {count:.2f}\")\n",
    "\n",
    "print(\"Estimated counts using EMQ:\")\n",
    "print(emq_estimated_counts)\n",
    "\n",
    "# print(\"Estimated counts using GAC:\")\n",
    "# print(gac_estimated_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af9556",
   "metadata": {},
   "source": [
    "集成模型， Voting ***目前最佳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75f6a8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAcc: 0.5393\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.30      0.32      0.31       428\n",
      "     culex_female       0.61      0.52      0.56       522\n",
      "  funestus_female       0.66      0.85      0.74       512\n",
      "   gambiae_female       0.54      0.45      0.49       600\n",
      "\n",
      "         accuracy                           0.54      2062\n",
      "        macro avg       0.53      0.53      0.52      2062\n",
      "     weighted avg       0.54      0.54      0.53      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 136 │             99 │                60 │              133 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 197 │            269 │                12 │               44 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  12 │             14 │               436 │               50 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 111 │             61 │               157 │              271 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\comp9991\\lib\\site-packages\\threadpoolctl.py:1186: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tabulate import tabulate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 定义特征集\n",
    "feature_set = special_features + wbf_features + freq_features + basefreq_features + relbasefreq_features + power_features\n",
    "\n",
    "# 准备训练和测试数据\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).to_numpy()\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "nclasses = 4\n",
    "\n",
    "clf1 = LinearDiscriminantAnalysis()\n",
    "clf2 = RandomForestClassifier(n_estimators=100)\n",
    "clf3 = SVC(probability=True)\n",
    "\n",
    "model = VotingClassifier(estimators=[\n",
    "    ('lda', clf1), ('rf', clf2), ('svc', clf3)], voting='soft')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "p_labels = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, p_labels)\n",
    "\n",
    "print(f\"\\tAcc: {acc:.4f}\")\n",
    "print(classification_report(y_test, p_labels, labels=np.unique(y_test)))\n",
    "\n",
    "cf = confusion_matrix(y_test, p_labels, labels=np.unique(y_train))\n",
    "print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc2332",
   "metadata": {},
   "source": [
    "Lab's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ba89061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number:  2605\n",
      "\tAcc: 0.5006\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.35      0.27      0.31       636\n",
      "     culex_female       0.44      0.80      0.57       495\n",
      "  funestus_female       0.68      0.75      0.71       713\n",
      "   gambiae_female       0.47      0.26      0.33       761\n",
      "\n",
      "         accuracy                           0.50      2605\n",
      "        macro avg       0.49      0.52      0.48      2605\n",
      "     weighted avg       0.49      0.50      0.48      2605\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 174 │            221 │               109 │              132 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  62 │            395 │                 3 │               35 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  34 │             87 │               538 │               54 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 222 │            200 │               142 │              197 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "number:  2362\n",
      "\tAcc: 0.5233\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.12      0.02      0.03       694\n",
      "     culex_female       0.64      0.77      0.70       638\n",
      "  funestus_female       0.68      0.87      0.76       464\n",
      "   gambiae_female       0.36      0.58      0.44       566\n",
      "\n",
      "         accuracy                           0.52      2362\n",
      "        macro avg       0.45      0.56      0.49      2362\n",
      "     weighted avg       0.43      0.52      0.46      2362\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  12 │            167 │                86 │              429 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  20 │            492 │                 8 │              118 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  15 │             13 │               405 │               31 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  51 │             92 │                96 │              327 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "number:  2039\n",
      "\tAcc: 0.5606\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.55      0.26      0.35       614\n",
      "     culex_female       0.55      0.73      0.63       473\n",
      "  funestus_female       0.70      0.82      0.76       569\n",
      "   gambiae_female       0.37      0.45      0.40       383\n",
      "\n",
      "         accuracy                           0.56      2039\n",
      "        macro avg       0.55      0.56      0.54      2039\n",
      "     weighted avg       0.56      0.56      0.54      2039\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 159 │            170 │                81 │              204 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  61 │            346 │                 6 │               60 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  24 │             50 │               467 │               28 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  43 │             58 │               111 │              171 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "number:  1938\n",
      "\tAcc: 0.5315\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.55      0.13      0.20       589\n",
      "     culex_female       0.57      0.71      0.63       445\n",
      "  funestus_female       0.74      0.82      0.78       629\n",
      "   gambiae_female       0.22      0.45      0.30       275\n",
      "\n",
      "         accuracy                           0.53      1938\n",
      "        macro avg       0.52      0.53      0.48      1938\n",
      "     weighted avg       0.57      0.53      0.50      1938\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  74 │            166 │                77 │              272 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  20 │            318 │                 1 │              106 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  23 │             40 │               514 │               52 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  18 │             34 │                99 │              124 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "number:  3056\n",
      "\tAcc: 0.4781\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.24      0.66      0.35       467\n",
      "     culex_female       0.68      0.58      0.63       949\n",
      "  funestus_female       0.61      0.77      0.68       625\n",
      "   gambiae_female       0.73      0.12      0.21      1015\n",
      "\n",
      "         accuracy                           0.48      3056\n",
      "        macro avg       0.57      0.53      0.47      3056\n",
      "     weighted avg       0.62      0.48      0.46      3056\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 309 │             61 │                77 │               20 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 356 │            550 │                33 │               10 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  94 │             37 │               480 │               14 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 536 │            157 │               200 │              122 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y = train_incubator['class'].values \n",
    "\n",
    "clf1 = LinearDiscriminantAnalysis()\n",
    "clf2 = RandomForestClassifier(n_estimators=100)\n",
    "clf3 = SVC(probability=True)\n",
    "clf4 = linear_model.LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "\n",
    "model = VotingClassifier(estimators=[\n",
    "    ('lda', clf1), ('rf', clf2), ('svc', clf3), ('lr', clf4)], voting='soft')\n",
    "\n",
    "groups = train_incubator['sensor'].values\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "for train_index, test_index in group_kfold.split(X, y, groups):\n",
    "  X_train, y_train, X_test, y_test = X[train_index], y[train_index], X[test_index], y[test_index]\n",
    "  model.fit(X[train_index], y[train_index])\n",
    "\n",
    "  p_labels = model.predict(X_test)\n",
    "  a_labels = y_test\n",
    "  acc = accuracy_score(a_labels, p_labels)\n",
    "  print('number: ', len(a_labels))\n",
    "\n",
    "  print(\"\\tAcc: %.4f\" % acc)\n",
    "  print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "      \n",
    "  cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "  print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "528fdb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3433\n",
      "[LightGBM] [Info] Number of data points in the train set: 7155, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score -1.261748\n",
      "[LightGBM] [Info] Start training from score -1.472506\n",
      "[LightGBM] [Info] Start training from score -1.325958\n",
      "[LightGBM] [Info] Start training from score -1.505336\n",
      "Fold validation accuracy: 0.6015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3437\n",
      "[LightGBM] [Info] Number of data points in the train set: 7155, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score -1.261748\n",
      "[LightGBM] [Info] Start training from score -1.472506\n",
      "[LightGBM] [Info] Start training from score -1.325958\n",
      "[LightGBM] [Info] Start training from score -1.505336\n",
      "Fold validation accuracy: 0.6288\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3433\n",
      "[LightGBM] [Info] Number of data points in the train set: 7155, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score -1.261748\n",
      "[LightGBM] [Info] Start training from score -1.472506\n",
      "[LightGBM] [Info] Start training from score -1.325958\n",
      "[LightGBM] [Info] Start training from score -1.505336\n",
      "Fold validation accuracy: 0.6020\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3435\n",
      "[LightGBM] [Info] Number of data points in the train set: 7155, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score -1.261255\n",
      "[LightGBM] [Info] Start training from score -1.473115\n",
      "[LightGBM] [Info] Start training from score -1.325958\n",
      "[LightGBM] [Info] Start training from score -1.505336\n",
      "Fold validation accuracy: 0.6165\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3433\n",
      "[LightGBM] [Info] Number of data points in the train set: 7156, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score -1.261394\n",
      "[LightGBM] [Info] Start training from score -1.472645\n",
      "[LightGBM] [Info] Start training from score -1.326097\n",
      "[LightGBM] [Info] Start training from score -1.505476\n",
      "Fold validation accuracy: 0.6107\n",
      "Mean validation accuracy: 0.6119\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3435\n",
      "[LightGBM] [Info] Number of data points in the train set: 8944, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score -1.261579\n",
      "[LightGBM] [Info] Start training from score -1.472655\n",
      "[LightGBM] [Info] Start training from score -1.325985\n",
      "[LightGBM] [Info] Start training from score -1.505364\n",
      "\tAcc: 0.4918\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.22      0.64      0.33       467\n",
      "     culex_female       0.70      0.48      0.57       949\n",
      "  funestus_female       0.74      0.63      0.68       625\n",
      "   gambiae_female       0.67      0.35      0.46      1015\n",
      "\n",
      "         accuracy                           0.49      3056\n",
      "        macro avg       0.58      0.53      0.51      3056\n",
      "     weighted avg       0.62      0.49      0.52      3056\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 299 │             67 │                40 │               61 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 424 │            453 │                22 │               50 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 151 │             11 │               395 │               68 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 462 │            119 │                78 │              356 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tabulate import tabulate\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 定义基础模型\n",
    "clf1 = LinearDiscriminantAnalysis()\n",
    "clf2 = RandomForestClassifier(n_estimators=100)\n",
    "clf3 = SVC(probability=True)\n",
    "clf4 = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "\n",
    "# 使用 Stacking 方法\n",
    "estimators = [\n",
    "    ('lda', clf1),\n",
    "    ('rf', clf2),\n",
    "    ('svc', clf3),\n",
    "    ('lr', clf4)\n",
    "]\n",
    "\n",
    "model = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=lgb.LGBMClassifier()\n",
    ")\n",
    "\n",
    "# 交叉验证\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "val_scores = []\n",
    "for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n",
    "    model.fit(X_train_scaled[train_idx], y_train[train_idx])\n",
    "    val_predictions = model.predict(X_train_scaled[val_idx])\n",
    "    val_acc = accuracy_score(y_train[val_idx], val_predictions)\n",
    "    val_scores.append(val_acc)\n",
    "    print(f'Fold validation accuracy: {val_acc:.4f}')\n",
    "\n",
    "# 平均验证准确率\n",
    "print(f'Mean validation accuracy: {np.mean(val_scores):.4f}')\n",
    "\n",
    "# 定义特征集\n",
    "feature_set = special_features + wbf_features + freq_features + basefreq_features + relbasefreq_features + power_features\n",
    "\n",
    "# 准备训练和测试数据\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).to_numpy()\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "# 训练最终模型并进行预测\n",
    "model.fit(X_train, y_train)\n",
    "p_labels = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, p_labels)\n",
    "\n",
    "print(f\"\\tAcc: {acc:.4f}\")\n",
    "print(classification_report(y_test, p_labels, labels=np.unique(y_test)))\n",
    "\n",
    "cf = confusion_matrix(y_test, p_labels, labels=np.unique(y_train))\n",
    "print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850314ab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp9991",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
