{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tabulate import tabulate\n",
    "from hyperopt import hp, fmin, tpe, Trials, space_eval\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cvx\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "import quadprog\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import model_selection\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate CC/ACC accuary score\n",
    "def Accuary(estimate, actual):\n",
    "  return 1 - (abs(estimate - actual) / actual)\n",
    "\n",
    "# calculate train_score, test_score\n",
    "def getScores(X_train, X_test, Y_train, nclasses):\n",
    "\n",
    "    clf1 = LinearDiscriminantAnalysis()\n",
    "    clf2 = RandomForestClassifier(n_estimators=100)\n",
    "    clf3 = SVC(probability=True)\n",
    "\n",
    "    # 使用 Platt Scaling 校准概率分数\n",
    "    clf1_calibrated = CalibratedClassifierCV(clf1, method='sigmoid', cv=5)\n",
    "    clf2_calibrated = CalibratedClassifierCV(clf2, method='sigmoid', cv=5)\n",
    "    clf3_calibrated = CalibratedClassifierCV(clf3, method='sigmoid', cv=5)\n",
    "\n",
    "    model = VotingClassifier(estimators=[\n",
    "        ('lda', clf1_calibrated), ('rf', clf2_calibrated), ('svc', clf3_calibrated)], voting='soft')\n",
    "\n",
    "    train_scores = np.zeros((len(X_train), nclasses))\n",
    "    test_scores = np.zeros((len(X_test), nclasses))\n",
    "\n",
    "    Y_cts = np.unique(Y_train, return_counts=True)\n",
    "    nfolds = min(10, min(Y_cts[1]))\n",
    "    \n",
    "    if nfolds > 1:\n",
    "        kfold = model_selection.StratifiedKFold(n_splits=nfolds, random_state=1, shuffle=True)\n",
    "        for train_idx, test_idx in kfold.split(X_train, Y_train):\n",
    "            model.fit(X_train[train_idx], Y_train[train_idx])\n",
    "            train_scores[test_idx] = model.predict_proba(X_train[test_idx])\n",
    "\n",
    "    # 训练最终模型并预测\n",
    "    model.fit(X_train, Y_train)\n",
    "    test_scores = model.predict_proba(X_test)\n",
    "            \n",
    "    return train_scores, test_scores\n",
    "\n",
    "# EMQ function\n",
    "def EMQ(test_scores, nclasses):\n",
    "    max_it = 1000        # Max num of iterations\n",
    "    eps = 1e-1           # Small constant for stopping criterium\n",
    "\n",
    "    p_tr = [0.25, 0.25, 0.25, 0.25]\n",
    "    p_s = np.copy(p_tr)\n",
    "    p_cond_tr = np.array(test_scores)\n",
    "    p_cond_s = np.zeros(p_cond_tr.shape)\n",
    "    prob_arrays = []\n",
    "\n",
    "    for _ in range(max_it):\n",
    "        # Add Laplacian smoothing\n",
    "        # r = (p_s + alpha) / (p_tr + (alpha * nclasses))\n",
    "        r = p_s / p_tr\n",
    "        \n",
    "        p_cond_s = p_cond_tr * r\n",
    "        s = np.sum(p_cond_s, axis = 1)\n",
    "        for c in range(nclasses):\n",
    "            p_cond_s[:,c] = p_cond_s[:,c] / s\n",
    "\n",
    "        prob_arrays.append(p_cond_s)\n",
    "        p_s_old = np.copy(p_s)\n",
    "        p_s = np.sum(p_cond_s, axis = 0) / p_cond_s.shape[0]\n",
    "        if (np.sum(np.abs(p_s - p_s_old)) < eps):\n",
    "            break\n",
    "\n",
    "    return (p_s/np.sum(p_s))\n",
    "\n",
    "def GAC(train_scores, test_scores, train_labels, nclasses):\n",
    "   \n",
    "    yt_hat = np.argmax(train_scores, axis = 1)\n",
    "    y_hat = np.argmax(test_scores, axis = 1)\n",
    "    CM = metrics.confusion_matrix(train_labels, yt_hat, normalize=\"true\").T\n",
    "    p_y_hat = np.zeros(nclasses)\n",
    "    values, counts = np.unique(y_hat, return_counts=True)\n",
    "    p_y_hat[values] = counts \n",
    "    p_y_hat = p_y_hat/p_y_hat.sum()\n",
    "    \n",
    "    p_hat = cvx.Variable(CM.shape[1])\n",
    "    constraints = [p_hat >= 0, cvx.sum(p_hat) == 1.0]\n",
    "    problem = cvx.Problem(cvx.Minimize(cvx.norm(CM @ p_hat - p_y_hat)), constraints)\n",
    "    problem.solve()\n",
    "    return p_hat.value\n",
    "\n",
    "def GPAC(train_scores, test_scores, train_labels, nclasses):\n",
    "\n",
    "    CM = np.zeros((nclasses, nclasses))\n",
    "    for i in range(nclasses):\n",
    "        idx = np.where(train_labels == i)[0]\n",
    "        CM[i] = np.sum(train_scores[idx], axis=0)\n",
    "        CM[i] /= np.sum(CM[i])\n",
    "    CM = CM.T\n",
    "    p_y_hat = np.sum(test_scores, axis = 0)\n",
    "    p_y_hat = p_y_hat / np.sum(p_y_hat)\n",
    "    \n",
    "    p_hat = cvx.Variable(CM.shape[1])\n",
    "    constraints = [p_hat >= 0, cvx.sum(p_hat) == 1.0]\n",
    "    problem = cvx.Problem(cvx.Minimize(cvx.norm(CM @ p_hat - p_y_hat)), constraints)\n",
    "    problem.solve()\n",
    "    return p_hat.value\n",
    "\n",
    "def FM(train_scores, test_scores, train_labels, nclasses):\n",
    "\n",
    "    CM = np.zeros((nclasses, nclasses))\n",
    "    y_cts = np.array([np.count_nonzero(train_labels == i) for i in range(nclasses)])\n",
    "    p_yt = y_cts / train_labels.shape[0]\n",
    "    for i in range(nclasses):\n",
    "        idx = np.where(train_labels == i)[0]\n",
    "        CM[:, i] += np.sum(train_scores[idx] > p_yt, axis=0) \n",
    "    CM = CM / y_cts\n",
    "    p_y_hat = np.sum(test_scores > p_yt, axis = 0) / test_scores.shape[0]\n",
    "    \n",
    "    p_hat = cvx.Variable(CM.shape[1])\n",
    "    constraints = [p_hat >= 0, cvx.sum(p_hat) == 1.0]\n",
    "    problem = cvx.Problem(cvx.Minimize(cvx.norm(CM @ p_hat - p_y_hat)), constraints)\n",
    "    problem.solve()\n",
    "    return p_hat.value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "arabiensis_female    3000\n",
      "culex_female         3000\n",
      "funestus_female      3000\n",
      "gambiae_female       3000\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "gambiae_female       600\n",
      "culex_female         522\n",
      "funestus_female      512\n",
      "arabiensis_female    428\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_incubator = pd.read_csv('train_incubator.csv')\n",
    "test_sf2 = pd.read_csv('test_sf2.csv')\n",
    "\n",
    "# Check number of examples per class\n",
    "print (train_incubator['class'].value_counts())\n",
    "print (test_sf2['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets\n",
    "special_features = ['temperature', 'duration', 'humidity']\n",
    "wbf_features = ['L_harmcherry_wbf_mean','L_harmcherry_wbf_stddev']\n",
    "freq_features = [f'L_harmcherry_h{i}_freq' for i in range(1,9)]\n",
    "basefreq_features = [f'L_harmcherry_h{i}_basefreq' for i in range(1,9)]\n",
    "relbasefreq_features = [f'L_harmcherry_h{i}_relbasefreq' for i in range(1,9)]\n",
    "power_features = [f'L_harmcherry_h{i}_power' for i in range(1,9)]\n",
    "relpower_features = [f'L_harmcherry_h{i}_relpower' for i in range(1,9)]\n",
    "invented_features = [f'L_harmcherry_h{i}_invented' for i in range(1,9)]\n",
    "\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y_train = train_incubator['class'].values \n",
    "y_train = pd.Series(y_train)\n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).to_numpy()\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "nclasses = len(train_incubator['class'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "几种不同方法的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " class\n",
      "arabiensis_female    3000\n",
      "culex_female         3000\n",
      "funestus_female      3000\n",
      "gambiae_female       3000\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "\n",
      " sensor\n",
      "330017    2605\n",
      "330020    2362\n",
      "330018    2039\n",
      "330019    1938\n",
      "330021    1746\n",
      "330016    1310\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "\t\tModel:  LGBM\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8982\n",
      "[LightGBM] [Info] Number of data points in the train set: 10690, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.318837\n",
      "[LightGBM] [Info] Start training from score -1.357708\n",
      "[LightGBM] [Info] Start training from score -1.401185\n",
      "[LightGBM] [Info] Start training from score -1.474037\n",
      "\t\t\tAcc (Sensor 330016): 0.5939\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8981\n",
      "[LightGBM] [Info] Number of data points in the train set: 9395, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.379823\n",
      "[LightGBM] [Info] Start training from score -1.321889\n",
      "[LightGBM] [Info] Start training from score -1.412937\n",
      "[LightGBM] [Info] Start training from score -1.434148\n",
      "\t\t\tAcc (Sensor 330017): 0.5893\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8985\n",
      "[LightGBM] [Info] Number of data points in the train set: 9961, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.429059\n",
      "[LightGBM] [Info] Start training from score -1.371645\n",
      "[LightGBM] [Info] Start training from score -1.410375\n",
      "[LightGBM] [Info] Start training from score -1.336649\n",
      "\t\t\tAcc (Sensor 330018): 0.6057\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001385 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8989\n",
      "[LightGBM] [Info] Number of data points in the train set: 10062, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.428724\n",
      "[LightGBM] [Info] Start training from score -1.370714\n",
      "[LightGBM] [Info] Start training from score -1.445454\n",
      "[LightGBM] [Info] Start training from score -1.306298\n",
      "\t\t\tAcc (Sensor 330019): 0.6259\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001589 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8972\n",
      "[LightGBM] [Info] Number of data points in the train set: 9638, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.430199\n",
      "[LightGBM] [Info] Start training from score -1.406205\n",
      "[LightGBM] [Info] Start training from score -1.335126\n",
      "[LightGBM] [Info] Start training from score -1.376178\n",
      "\t\t\tAcc (Sensor 330020): 0.5453\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8984\n",
      "[LightGBM] [Info] Number of data points in the train set: 10254, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.344092\n",
      "[LightGBM] [Info] Start training from score -1.494324\n",
      "[LightGBM] [Info] Start training from score -1.318980\n",
      "[LightGBM] [Info] Start training from score -1.396686\n",
      "\t\t\tAcc (Sensor 330021): 0.4926\n",
      "\t\t\tMean acc: 0.5754, std: 0.0443\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 946 │            569 │               322 │             1163 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 555 │           1972 │                83 │              390 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 264 │            146 │              2354 │              236 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 679 │            376 │               308 │             1637 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "\t\tModel:  MLP\n",
      "\t\t\tAcc (Sensor 330016): 0.5763\n",
      "\t\t\tAcc (Sensor 330017): 0.5639\n",
      "\t\t\tAcc (Sensor 330018): 0.6238\n",
      "\t\t\tAcc (Sensor 330019): 0.5960\n",
      "\t\t\tAcc (Sensor 330020): 0.5635\n",
      "\t\t\tAcc (Sensor 330021): 0.5584\n",
      "\t\t\tMean acc: 0.5803, std: 0.0230\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 871 │            657 │               368 │             1104 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 442 │           2130 │               100 │              328 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 152 │            175 │              2344 │              329 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 622 │            404 │               362 │             1612 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "\t\tModel:  LR\n",
      "\t\t\tAcc (Sensor 330016): 0.4710\n",
      "\t\t\tAcc (Sensor 330017): 0.4783\n",
      "\t\t\tAcc (Sensor 330018): 0.5287\n",
      "\t\t\tAcc (Sensor 330019): 0.4933\n",
      "\t\t\tAcc (Sensor 330020): 0.5089\n",
      "\t\t\tAcc (Sensor 330021): 0.4897\n",
      "\t\t\tMean acc: 0.4950, std: 0.0192\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 611 │            834 │               444 │             1111 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 437 │           1987 │                97 │              479 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 145 │            195 │              2408 │              252 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 759 │            646 │               647 │              948 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "\t\tModel:  SVM\n",
      "\t\t\tAcc (Sensor 330016): 0.5069\n",
      "\t\t\tAcc (Sensor 330017): 0.5309\n",
      "\t\t\tAcc (Sensor 330018): 0.6096\n",
      "\t\t\tAcc (Sensor 330019): 0.5748\n",
      "\t\t\tAcc (Sensor 330020): 0.5660\n",
      "\t\t\tAcc (Sensor 330021): 0.5132\n",
      "\t\t\tMean acc: 0.5502, std: 0.0365\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 967 │            733 │               331 │              969 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 528 │           2060 │                75 │              337 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 174 │            188 │              2314 │              324 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 813 │            528 │               363 │             1296 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "\t\tModel:  DT\n",
      "\t\t\tAcc (Sensor 330016): 0.5351\n",
      "\t\t\tAcc (Sensor 330017): 0.4944\n",
      "\t\t\tAcc (Sensor 330018): 0.4953\n",
      "\t\t\tAcc (Sensor 330019): 0.5454\n",
      "\t\t\tAcc (Sensor 330020): 0.4818\n",
      "\t\t\tAcc (Sensor 330021): 0.4003\n",
      "\t\t\tMean acc: 0.4921, std: 0.0470\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 770 │            664 │               402 │             1164 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 606 │           1576 │               208 │              610 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 346 │            210 │              2072 │              372 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 679 │            460 │               386 │             1475 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "\t\tModel:  RF\n",
      "\t\t\tAcc (Sensor 330016): 0.5718\n",
      "\t\t\tAcc (Sensor 330017): 0.5647\n",
      "\t\t\tAcc (Sensor 330018): 0.6027\n",
      "\t\t\tAcc (Sensor 330019): 0.5759\n",
      "\t\t\tAcc (Sensor 330020): 0.5453\n",
      "\t\t\tAcc (Sensor 330021): 0.5218\n",
      "\t\t\tMean acc: 0.5637, std: 0.0253\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 881 │            630 │               264 │             1225 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 505 │           2069 │                50 │              376 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 278 │            223 │              2238 │              261 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 708 │            417 │               299 │             1576 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "\t\tModel:  KNN\n",
      "\t\t\tAcc (Sensor 330016): 0.3557\n",
      "\t\t\tAcc (Sensor 330017): 0.4088\n",
      "\t\t\tAcc (Sensor 330018): 0.4414\n",
      "\t\t\tAcc (Sensor 330019): 0.4659\n",
      "\t\t\tAcc (Sensor 330020): 0.4306\n",
      "\t\t\tAcc (Sensor 330021): 0.4341\n",
      "\t\t\tMean acc: 0.4228, std: 0.0344\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                1032 │           1118 │               379 │              471 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 733 │           1871 │               148 │              248 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 489 │            525 │              1690 │              296 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                1080 │            834 │               570 │              516 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "\t\tModel:  GNB\n",
      "\t\t\tAcc (Sensor 330016): 0.4275\n",
      "\t\t\tAcc (Sensor 330017): 0.4188\n",
      "\t\t\tAcc (Sensor 330018): 0.4831\n",
      "\t\t\tAcc (Sensor 330019): 0.4634\n",
      "\t\t\tAcc (Sensor 330020): 0.4318\n",
      "\t\t\tAcc (Sensor 330021): 0.4708\n",
      "\t\t\tMean acc: 0.4492, std: 0.0242\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 407 │           1476 │               579 │              538 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 259 │           2297 │               280 │              164 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 144 │            569 │              2118 │              169 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 556 │            962 │               928 │              554 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "\t\tModel:  LDA\n",
      "\t\t\tAcc (Sensor 330016): 0.4718\n",
      "\t\t\tAcc (Sensor 330017): 0.4656\n",
      "\t\t\tAcc (Sensor 330018): 0.5213\n",
      "\t\t\tAcc (Sensor 330019): 0.4907\n",
      "\t\t\tAcc (Sensor 330020): 0.5055\n",
      "\t\t\tAcc (Sensor 330021): 0.4931\n",
      "\t\t\tMean acc: 0.4913, std: 0.0189\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 684 │            800 │               412 │             1104 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 508 │           1940 │                64 │              488 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 171 │            203 │              2353 │              273 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 797 │            599 │               681 │              923 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "sensors = train_incubator.sensor.values\n",
    "sensor_ids = np.unique(sensors)\n",
    "\n",
    "print ('\\n', train_incubator['class'].value_counts(), '\\n')\n",
    "print ('\\n', train_incubator['sensor'].value_counts(), '\\n')\n",
    "\n",
    "# These are the ML models\n",
    "models = [('LGBM', lightgbm.LGBMClassifier()),\n",
    "          ('MLP', Pipeline([('scaler', MinMaxScaler()), ('model', MLPClassifier(max_iter=5000))])),\n",
    "          ('LR', Pipeline([('scaler', MinMaxScaler()), ('model', LogisticRegression(max_iter=5000))])),\n",
    "          ('SVM', Pipeline([('scaler', MinMaxScaler()), ('model', SVC())])),\n",
    "          ('DT', DecisionTreeClassifier()),\n",
    "          ('RF', RandomForestClassifier(n_estimators=100)),\n",
    "          ('KNN', KNeighborsClassifier()),\n",
    "          ('GNB', GaussianNB()),\n",
    "          ('LDA', LinearDiscriminantAnalysis()),\n",
    "         ]\n",
    "\n",
    "for name, model in models:\n",
    "    print(\"\\t\\tModel: \", name)\n",
    "    accs = []\n",
    "    y_pred_all = []\n",
    "    y_true_all = []\n",
    "    importances = []\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    for i, (train, test) in enumerate(logo.split(X_train, y_train, groups=sensors)):\n",
    "        model.fit(X_train[train], y_train[train])\n",
    "        p_labels = model.predict(X_train[test])\n",
    "        a_labels = y_train[test]\n",
    "        acc = accuracy_score(a_labels, p_labels)\n",
    "        accs.append(acc)\n",
    "\n",
    "        y_pred_all.extend(p_labels)\n",
    "        y_true_all.extend(a_labels)            \n",
    "        print(\"\\t\\t\\tAcc (Sensor %s): %.4f\" % (sensor_ids[i], acc))\n",
    "    print(\"\\t\\t\\tMean acc: %.4f, std: %.4f\" % (np.mean(accs), np.std(accs)))\n",
    "    cf = confusion_matrix(y_true_all, y_pred_all, labels=np.unique(y_train))\n",
    "    print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble model, voting method：\n",
    "1. LDA  2. RFC  3. SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8980\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8979\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001049 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8987\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001099 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8981\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "\tAcc: 0.5199\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.27      0.25      0.26       428\n",
      "     culex_female       0.54      0.58      0.56       522\n",
      "  funestus_female       0.87      0.63      0.73       512\n",
      "   gambiae_female       0.46      0.57      0.51       600\n",
      "\n",
      "         accuracy                           0.52      2062\n",
      "        macro avg       0.54      0.51      0.51      2062\n",
      "     weighted avg       0.54      0.52      0.52      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 105 │            124 │                12 │              187 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 138 │            303 │                 8 │               73 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  27 │             20 │               323 │              142 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 121 │            109 │                29 │              341 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "clf1 = lightgbm.LGBMClassifier()\n",
    "clf2 = DecisionTreeClassifier()\n",
    "clf3 = SVC(probability=True)\n",
    "\n",
    " # 使用 Platt Scaling 校准概率分数\n",
    "clf1_calibrated = CalibratedClassifierCV(clf1, method='sigmoid', cv=5)\n",
    "clf2_calibrated = CalibratedClassifierCV(clf2, method='sigmoid', cv=5)\n",
    "clf3_calibrated = CalibratedClassifierCV(clf3, method='sigmoid', cv=5)\n",
    "\n",
    "# model = VotingClassifier(estimators=[\n",
    "#     ('lda', clf1), ('rf', clf2), ('svc', clf3)], voting='soft')\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "model = VotingClassifier(estimators=[\n",
    "    ('lgbm', clf1_calibrated), ('rf', clf2_calibrated), ('svc', clf3_calibrated)], voting='soft')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "p_labels = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, p_labels)\n",
    "\n",
    "print(f\"\\tAcc: {acc:.4f}\")\n",
    "print(classification_report(y_test, p_labels, labels=np.unique(y_test)))\n",
    "\n",
    "cf = confusion_matrix(y_test, p_labels, labels=np.unique(y_train))\n",
    "print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify and Count(CC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabiensis CC: 0.9135514018691588\n",
      "culex CC: 0.9348659003831418\n",
      "funestus CC: 0.7265625\n",
      "gambiae CC: 0.7616666666666667\n"
     ]
    }
   ],
   "source": [
    "arabiensis_CC_estimate = cf[0][0]+cf[1][0]+cf[2][0]+cf[3][0]\n",
    "arabiensis_actual = cf[0][0]+cf[0][1]+cf[0][2]+cf[0][3]\n",
    "arabiensis_CC = Accuary(arabiensis_CC_estimate, arabiensis_actual)\n",
    "print('arabiensis CC:', arabiensis_CC)\n",
    "\n",
    "culex_CC_estimate = cf[0][1]+cf[1][1]+cf[2][1]+cf[3][1]\n",
    "culex_actual = cf[1][0]+cf[1][1]+cf[1][2]+cf[1][3]\n",
    "culex_CC = Accuary(culex_CC_estimate, culex_actual)\n",
    "print('culex CC:', culex_CC)\n",
    "\n",
    "funestus_CC_estimate = cf[0][2]+cf[1][2]+cf[2][2]+cf[3][2]\n",
    "funestus_actual = cf[2][0]+cf[2][1]+cf[2][2]+cf[2][3]\n",
    "funestus_CC = Accuary(funestus_CC_estimate, funestus_actual)\n",
    "print('funestus CC:', funestus_CC)\n",
    "\n",
    "gambiae_CC_estimate = cf[0][3]+cf[1][3]+cf[2][3]+cf[3][3]\n",
    "gambiae_actual = cf[3][0]+cf[3][1]+cf[3][2]+cf[3][3]\n",
    "gambiae_CC = Accuary(gambiae_CC_estimate, gambiae_actual)\n",
    "print('gambiae CC:', gambiae_CC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusted Classify and Count(ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabiensis's TPR at semi_field: 0.24532710280373832\n",
      "culex's TPR at semi_field: 0.5804597701149425\n",
      "funestus's TPR at semi_field: 0.630859375\n",
      "gambiae's TPR at semi_field: 0.5683333333333334\n"
     ]
    }
   ],
   "source": [
    "# class's tpr\n",
    "arabiensis_estimate_number = cf[0][0]\n",
    "culex_estimate_number = cf[1][1]\n",
    "funestus_estimate_number = cf[2][2]\n",
    "gambiae_estimate_number = cf[3][3]\n",
    "\n",
    "arabiensis_semi_tpr = arabiensis_estimate_number / arabiensis_actual\n",
    "print(\"arabiensis's TPR at semi_field:\", arabiensis_semi_tpr)\n",
    "\n",
    "culex_semi_tpr = culex_estimate_number / culex_actual\n",
    "print(\"culex's TPR at semi_field:\", culex_semi_tpr)\n",
    "\n",
    "funestus_semi_tpr = funestus_estimate_number / funestus_actual\n",
    "print(\"funestus's TPR at semi_field:\", funestus_semi_tpr)\n",
    "\n",
    "gambiae_semi_tpr = gambiae_estimate_number / gambiae_actual\n",
    "print(\"gambiae's TPR at semi_field:\", gambiae_semi_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8978\n",
      "[LightGBM] [Info] Number of data points in the train set: 7516, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.379928\n",
      "[LightGBM] [Info] Start training from score -1.321889\n",
      "[LightGBM] [Info] Start training from score -1.413265\n",
      "[LightGBM] [Info] Start training from score -1.433702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8977\n",
      "[LightGBM] [Info] Number of data points in the train set: 7516, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.379928\n",
      "[LightGBM] [Info] Start training from score -1.321889\n",
      "[LightGBM] [Info] Start training from score -1.412718\n",
      "[LightGBM] [Info] Start training from score -1.434260\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000877 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8981\n",
      "[LightGBM] [Info] Number of data points in the train set: 7516, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.379928\n",
      "[LightGBM] [Info] Start training from score -1.321889\n",
      "[LightGBM] [Info] Start training from score -1.412718\n",
      "[LightGBM] [Info] Start training from score -1.434260\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8980\n",
      "[LightGBM] [Info] Number of data points in the train set: 7516, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.379928\n",
      "[LightGBM] [Info] Start training from score -1.321889\n",
      "[LightGBM] [Info] Start training from score -1.412718\n",
      "[LightGBM] [Info] Start training from score -1.434260\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8979\n",
      "[LightGBM] [Info] Number of data points in the train set: 7516, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.379400\n",
      "[LightGBM] [Info] Start training from score -1.321889\n",
      "[LightGBM] [Info] Start training from score -1.413265\n",
      "[LightGBM] [Info] Start training from score -1.434260\n",
      "number:  2605\n",
      "\tAcc: 0.5762\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.45      0.39      0.42       636\n",
      "     culex_female       0.51      0.81      0.62       495\n",
      "  funestus_female       0.71      0.79      0.75       713\n",
      "   gambiae_female       0.61      0.38      0.47       761\n",
      "\n",
      "         accuracy                           0.58      2605\n",
      "        macro avg       0.57      0.59      0.56      2605\n",
      "     weighted avg       0.58      0.58      0.56      2605\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 250 │            168 │               104 │              114 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  67 │            399 │                 6 │               23 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  39 │             62 │               563 │               49 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 195 │            153 │               124 │              289 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8964\n",
      "[LightGBM] [Info] Number of data points in the train set: 7710, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.430581\n",
      "[LightGBM] [Info] Start training from score -1.405941\n",
      "[LightGBM] [Info] Start training from score -1.334975\n",
      "[LightGBM] [Info] Start training from score -1.376228\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000936 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8966\n",
      "[LightGBM] [Info] Number of data points in the train set: 7710, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.430039\n",
      "[LightGBM] [Info] Start training from score -1.406471\n",
      "[LightGBM] [Info] Start training from score -1.334975\n",
      "[LightGBM] [Info] Start training from score -1.376228\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8970\n",
      "[LightGBM] [Info] Number of data points in the train set: 7710, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.430039\n",
      "[LightGBM] [Info] Start training from score -1.406471\n",
      "[LightGBM] [Info] Start training from score -1.334975\n",
      "[LightGBM] [Info] Start training from score -1.376228\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000908 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8974\n",
      "[LightGBM] [Info] Number of data points in the train set: 7711, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.430169\n",
      "[LightGBM] [Info] Start training from score -1.406071\n",
      "[LightGBM] [Info] Start training from score -1.335598\n",
      "[LightGBM] [Info] Start training from score -1.375845\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000864 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8964\n",
      "[LightGBM] [Info] Number of data points in the train set: 7711, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.430169\n",
      "[LightGBM] [Info] Start training from score -1.406071\n",
      "[LightGBM] [Info] Start training from score -1.335105\n",
      "[LightGBM] [Info] Start training from score -1.376358\n",
      "number:  2362\n",
      "\tAcc: 0.5601\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.07      0.01      0.02       694\n",
      "     culex_female       0.79      0.78      0.79       638\n",
      "  funestus_female       0.84      0.85      0.84       464\n",
      "   gambiae_female       0.36      0.75      0.49       566\n",
      "\n",
      "         accuracy                           0.56      2362\n",
      "        macro avg       0.52      0.60      0.53      2362\n",
      "     weighted avg       0.49      0.56      0.50      2362\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                   7 │             61 │                28 │              598 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  18 │            500 │                11 │              109 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  28 │             13 │               393 │               30 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  51 │             55 │                37 │              423 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8977\n",
      "[LightGBM] [Info] Number of data points in the train set: 7968, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.429378\n",
      "[LightGBM] [Info] Start training from score -1.371346\n",
      "[LightGBM] [Info] Start training from score -1.410172\n",
      "[LightGBM] [Info] Start training from score -1.336835\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8980\n",
      "[LightGBM] [Info] Number of data points in the train set: 7969, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.428979\n",
      "[LightGBM] [Info] Start training from score -1.371967\n",
      "[LightGBM] [Info] Start training from score -1.410297\n",
      "[LightGBM] [Info] Start training from score -1.336483\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8984\n",
      "[LightGBM] [Info] Number of data points in the train set: 7969, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.428979\n",
      "[LightGBM] [Info] Start training from score -1.371967\n",
      "[LightGBM] [Info] Start training from score -1.410297\n",
      "[LightGBM] [Info] Start training from score -1.336483\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000888 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 7969, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.428979\n",
      "[LightGBM] [Info] Start training from score -1.371472\n",
      "[LightGBM] [Info] Start training from score -1.410811\n",
      "[LightGBM] [Info] Start training from score -1.336483\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000808 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8980\n",
      "[LightGBM] [Info] Number of data points in the train set: 7969, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.428979\n",
      "[LightGBM] [Info] Start training from score -1.371472\n",
      "[LightGBM] [Info] Start training from score -1.410297\n",
      "[LightGBM] [Info] Start training from score -1.336961\n",
      "number:  2039\n",
      "\tAcc: 0.5900\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.55      0.39      0.46       614\n",
      "     culex_female       0.64      0.58      0.61       473\n",
      "  funestus_female       0.74      0.83      0.79       569\n",
      "   gambiae_female       0.40      0.55      0.46       383\n",
      "\n",
      "         accuracy                           0.59      2039\n",
      "        macro avg       0.58      0.59      0.58      2039\n",
      "     weighted avg       0.60      0.59      0.59      2039\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 242 │             99 │                79 │              194 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 107 │            275 │                 9 │               82 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  25 │             26 │               475 │               43 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  67 │             29 │                76 │              211 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8980\n",
      "[LightGBM] [Info] Number of data points in the train set: 8049, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.429065\n",
      "[LightGBM] [Info] Start training from score -1.370639\n",
      "[LightGBM] [Info] Start training from score -1.445274\n",
      "[LightGBM] [Info] Start training from score -1.306223\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8982\n",
      "[LightGBM] [Info] Number of data points in the train set: 8049, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.428546\n",
      "[LightGBM] [Info] Start training from score -1.370639\n",
      "[LightGBM] [Info] Start training from score -1.445801\n",
      "[LightGBM] [Info] Start training from score -1.306223\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000960 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8987\n",
      "[LightGBM] [Info] Number of data points in the train set: 8050, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.428670\n",
      "[LightGBM] [Info] Start training from score -1.370763\n",
      "[LightGBM] [Info] Start training from score -1.445398\n",
      "[LightGBM] [Info] Start training from score -1.306347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000786 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8984\n",
      "[LightGBM] [Info] Number of data points in the train set: 8050, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.428670\n",
      "[LightGBM] [Info] Start training from score -1.370763\n",
      "[LightGBM] [Info] Start training from score -1.445398\n",
      "[LightGBM] [Info] Start training from score -1.306347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8980\n",
      "[LightGBM] [Info] Number of data points in the train set: 8050, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.428670\n",
      "[LightGBM] [Info] Start training from score -1.370763\n",
      "[LightGBM] [Info] Start training from score -1.445398\n",
      "[LightGBM] [Info] Start training from score -1.306347\n",
      "number:  1938\n",
      "\tAcc: 0.5913\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.65      0.20      0.31       589\n",
      "     culex_female       0.52      0.84      0.64       445\n",
      "  funestus_female       0.82      0.80      0.81       629\n",
      "   gambiae_female       0.36      0.53      0.43       275\n",
      "\n",
      "         accuracy                           0.59      1938\n",
      "        macro avg       0.58      0.60      0.55      1938\n",
      "     weighted avg       0.63      0.59      0.56      1938\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 120 │            262 │                60 │              147 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                   8 │            375 │                 1 │               61 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  21 │             50 │               505 │               53 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  36 │             41 │                52 │              146 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8979\n",
      "[LightGBM] [Info] Number of data points in the train set: 8203, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.344162\n",
      "[LightGBM] [Info] Start training from score -1.494191\n",
      "[LightGBM] [Info] Start training from score -1.319229\n",
      "[LightGBM] [Info] Start training from score -1.396464\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8978\n",
      "[LightGBM] [Info] Number of data points in the train set: 8203, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.344162\n",
      "[LightGBM] [Info] Start training from score -1.494191\n",
      "[LightGBM] [Info] Start training from score -1.319229\n",
      "[LightGBM] [Info] Start training from score -1.396464\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8983\n",
      "[LightGBM] [Info] Number of data points in the train set: 8203, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.344162\n",
      "[LightGBM] [Info] Start training from score -1.494191\n",
      "[LightGBM] [Info] Start training from score -1.318774\n",
      "[LightGBM] [Info] Start training from score -1.396957\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8984\n",
      "[LightGBM] [Info] Number of data points in the train set: 8203, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.344162\n",
      "[LightGBM] [Info] Start training from score -1.494191\n",
      "[LightGBM] [Info] Start training from score -1.318774\n",
      "[LightGBM] [Info] Start training from score -1.396957\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8977\n",
      "[LightGBM] [Info] Number of data points in the train set: 8204, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.343816\n",
      "[LightGBM] [Info] Start training from score -1.494856\n",
      "[LightGBM] [Info] Start training from score -1.318895\n",
      "[LightGBM] [Info] Start training from score -1.396586\n",
      "number:  1746\n",
      "\tAcc: 0.4742\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.24      0.56      0.34       326\n",
      "     culex_female       0.74      0.42      0.53       699\n",
      "  funestus_female       0.57      0.70      0.63       258\n",
      "   gambiae_female       0.62      0.37      0.47       463\n",
      "\n",
      "         accuracy                           0.47      1746\n",
      "        macro avg       0.54      0.51      0.49      1746\n",
      "     weighted avg       0.59      0.47      0.49      1746\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 182 │             37 │                54 │               53 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 335 │            292 │                32 │               40 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  55 │              7 │               181 │               15 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 182 │             59 │                49 │              173 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8975\n",
      "[LightGBM] [Info] Number of data points in the train set: 8552, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.318924\n",
      "[LightGBM] [Info] Start training from score -1.357708\n",
      "[LightGBM] [Info] Start training from score -1.401375\n",
      "[LightGBM] [Info] Start training from score -1.473731\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8977\n",
      "[LightGBM] [Info] Number of data points in the train set: 8552, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.318924\n",
      "[LightGBM] [Info] Start training from score -1.357708\n",
      "[LightGBM] [Info] Start training from score -1.401375\n",
      "[LightGBM] [Info] Start training from score -1.473731\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8980\n",
      "[LightGBM] [Info] Number of data points in the train set: 8552, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.318924\n",
      "[LightGBM] [Info] Start training from score -1.357708\n",
      "[LightGBM] [Info] Start training from score -1.400900\n",
      "[LightGBM] [Info] Start training from score -1.474242\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8983\n",
      "[LightGBM] [Info] Number of data points in the train set: 8552, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.318924\n",
      "[LightGBM] [Info] Start training from score -1.357708\n",
      "[LightGBM] [Info] Start training from score -1.400900\n",
      "[LightGBM] [Info] Start training from score -1.474242\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8977\n",
      "[LightGBM] [Info] Number of data points in the train set: 8552, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.318487\n",
      "[LightGBM] [Info] Start training from score -1.357708\n",
      "[LightGBM] [Info] Start training from score -1.401375\n",
      "[LightGBM] [Info] Start training from score -1.474242\n",
      "number:  1310\n",
      "\tAcc: 0.5740\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.22      0.59      0.32       141\n",
      "     culex_female       0.50      0.70      0.59       250\n",
      "  funestus_female       0.84      0.67      0.75       367\n",
      "   gambiae_female       0.85      0.45      0.59       552\n",
      "\n",
      "         accuracy                           0.57      1310\n",
      "        macro avg       0.60      0.60      0.56      1310\n",
      "     weighted avg       0.71      0.57      0.60      1310\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  83 │             37 │                 7 │               14 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  54 │            175 │                 5 │               16 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  60 │             47 │               247 │               13 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 182 │             89 │                34 │              247 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "arabiensis_lab_tpr: 0.3579958098876909\n",
      "culex_lab_tpr: 0.6885985452854141\n",
      "funestus_lab_tpr: 0.774806427333826\n",
      "gambiae_lab_tpr: 0.505008349596272\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y = train_incubator['class'].values \n",
    "\n",
    "clf1 = lightgbm.LGBMClassifier()\n",
    "clf2 = DecisionTreeClassifier()\n",
    "clf3 = SVC(probability=True)\n",
    "# clf4 = Pipeline([('scaler', MinMaxScaler()), ('model', MLPClassifier(max_iter=5000))])\n",
    "\n",
    " # 使用 Platt Scaling 校准概率分数\n",
    "clf1_calibrated = CalibratedClassifierCV(clf1, method='sigmoid', cv=5)\n",
    "clf2_calibrated = CalibratedClassifierCV(clf2, method='sigmoid', cv=5)\n",
    "clf3_calibrated = CalibratedClassifierCV(clf3, method='sigmoid', cv=5)\n",
    "# clf4_calibrated = CalibratedClassifierCV(clf4, method='sigmoid', cv=5)\n",
    "\n",
    "# model = VotingClassifier(estimators=[\n",
    "#     ('lda', clf1), ('rf', clf2), ('svc', clf3), ('NB', clf4)], voting='soft')\n",
    "\n",
    "model = VotingClassifier(estimators=[\n",
    "    ('lda', clf1_calibrated), ('rf', clf2_calibrated), ('svc', clf3_calibrated)], voting='soft')\n",
    "\n",
    "groups = train_incubator['sensor'].values\n",
    "group_kfold = GroupKFold(n_splits=6)\n",
    "\n",
    "arabiensis_lab_tpr = 0\n",
    "culex_lab_tpr = 0\n",
    "funestus_lab_tpr = 0\n",
    "gambiae_lab_tpr = 0\n",
    "\n",
    "for train_index_lab, test_index_lab in group_kfold.split(X, y, groups):\n",
    "  X_train_lab, y_train_lab, X_test_lab, y_test_lab = X[train_index_lab], y[train_index_lab], X[test_index_lab], y[test_index_lab]\n",
    "  model.fit(X[train_index_lab], y[train_index_lab])\n",
    "\n",
    "  p_labels_lab = model.predict(X_test_lab)\n",
    "  a_labels_lab = y_test_lab\n",
    "  acc = accuracy_score(a_labels_lab, p_labels_lab)\n",
    "  print('number: ', len(a_labels_lab))\n",
    "\n",
    "  print(\"\\tAcc: %.4f\" % acc)\n",
    "  print (classification_report(a_labels_lab, p_labels_lab, labels=np.unique(y_test_lab)))\n",
    "      \n",
    "  cf_lab = confusion_matrix(a_labels_lab, p_labels_lab, labels=np.unique(y_train_lab))\n",
    "  arabiensis_actual_lab = cf_lab[0][0]+cf_lab[0][1]+cf_lab[0][2]+cf_lab[0][3]\n",
    "  culex_actual_lab = cf_lab[1][0]+cf_lab[1][1]+cf_lab[1][2]+cf_lab[1][3]\n",
    "  funestus_actual_lab = cf_lab[2][0]+cf_lab[2][1]+cf_lab[2][2]+cf_lab[2][3]\n",
    "  gambiae_actual_lab = cf_lab[3][0]+cf_lab[3][1]+cf_lab[3][2]+cf_lab[3][3]\n",
    "  arabiensis_lab_tpr += cf_lab[0][0] / arabiensis_actual_lab\n",
    "  culex_lab_tpr += cf_lab[1][1] / culex_actual_lab\n",
    "  funestus_lab_tpr += cf_lab[2][2] / funestus_actual_lab\n",
    "  gambiae_lab_tpr += cf_lab[3][3] / gambiae_actual_lab\n",
    "\n",
    "  print(tabulate(cf_lab, headers=np.unique(y_train_lab), tablefmt='fancy_grid'))\n",
    "\n",
    "arabiensis_lab_tpr = arabiensis_lab_tpr / 6\n",
    "culex_lab_tpr = culex_lab_tpr / 6\n",
    "funestus_lab_tpr = funestus_lab_tpr / 6\n",
    "gambiae_lab_tpr = gambiae_lab_tpr / 6\n",
    "\n",
    "print('arabiensis_lab_tpr:', arabiensis_lab_tpr)\n",
    "print('culex_lab_tpr:', culex_lab_tpr)\n",
    "print('funestus_lab_tpr:', funestus_lab_tpr)\n",
    "print('gambiae_lab_tpr:', gambiae_lab_tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabiensis ACC:  0.6852792575441076\n",
      "culex ACC:  0.8429581707500564\n",
      "funestus ACC:  0.814215464333253\n",
      "gambiae ACC:  0.8746060658448789\n"
     ]
    }
   ],
   "source": [
    "arabiensis_ACC_estimate = arabiensis_estimate_number / arabiensis_lab_tpr\n",
    "culex_ACC_estimate = culex_estimate_number / culex_lab_tpr\n",
    "funestus_ACC_estimate = funestus_estimate_number / funestus_lab_tpr\n",
    "gambiae_ACC_estimate = gambiae_estimate_number / gambiae_lab_tpr\n",
    "\n",
    "arabiensis_ACC = Accuary(arabiensis_ACC_estimate, arabiensis_actual)\n",
    "culex_ACC = Accuary(culex_ACC_estimate, culex_actual)\n",
    "funestus_ACC = Accuary(funestus_ACC_estimate, funestus_actual)\n",
    "gambiae_ACC = Accuary(gambiae_ACC_estimate, gambiae_actual)\n",
    "\n",
    "print('arabiensis ACC: ', arabiensis_ACC)\n",
    "print('culex ACC: ', culex_ACC)\n",
    "print('funestus ACC: ', funestus_ACC)\n",
    "print('gambiae ACC: ', gambiae_ACC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic Classify and Count(PCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabiensis PCC:  0.6895083425217652\n",
      "culex PCC:  0.8309063610574046\n",
      "funestus PCC:  0.9990011848369769\n",
      "gambiae PCC:  0.9247750946064693\n"
     ]
    }
   ],
   "source": [
    "train_scores, test_scores = getScores(X_train, X_test, y_train, nclasses)\n",
    "estimated_counts = np.mean(test_scores, axis=0) * len(test_scores)\n",
    "\n",
    "arabiensis_PCC_estimate = estimated_counts[0]\n",
    "culex_PCC_estimate = estimated_counts[1]\n",
    "funestus_PCC_estimate = estimated_counts[2]\n",
    "gambiae_PCC_estimate = estimated_counts[3]\n",
    "\n",
    "arabiensis_PCC = Accuary(arabiensis_PCC_estimate, arabiensis_actual)\n",
    "culex_PCC = Accuary(culex_PCC_estimate, culex_actual)\n",
    "funestus_PCC = Accuary(funestus_PCC_estimate, funestus_actual)\n",
    "gambiae_PCC = Accuary(gambiae_PCC_estimate, gambiae_actual)\n",
    "\n",
    "print('arabiensis PCC: ', arabiensis_PCC)\n",
    "print('culex PCC: ', culex_PCC)\n",
    "print('funestus PCC: ', funestus_PCC)\n",
    "print('gambiae PCC: ', gambiae_PCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation Maximisation for Quantification(EMQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabiensis EMQ:  0.6895083425217645\n",
      "culex EMQ:  0.830906361057405\n",
      "funestus EMQ:  0.9990011848369766\n",
      "gambiae EMQ:  0.9247750946064698\n"
     ]
    }
   ],
   "source": [
    "res = EMQ(test_scores, nclasses)\n",
    "\n",
    "arabiensis_EMQ_estimate = res[0] * len(test_scores)\n",
    "culex_EMQ_estimate = res[1] * len(test_scores)\n",
    "funestus_EMQ_estimate = res[2] * len(test_scores)\n",
    "gambiae_EMQ_estimate = res[3] * len(test_scores)\n",
    "\n",
    "arabiensis_EMQ = Accuary(arabiensis_EMQ_estimate, arabiensis_actual)\n",
    "culex_EMQ = Accuary(culex_EMQ_estimate, culex_actual)\n",
    "funestus_EMQ = Accuary(funestus_EMQ_estimate, funestus_actual)\n",
    "gambiae_EMQ = Accuary(gambiae_EMQ_estimate, gambiae_actual)\n",
    "\n",
    "print('arabiensis EMQ: ', arabiensis_EMQ)\n",
    "print('culex EMQ: ', culex_EMQ)\n",
    "print('funestus EMQ: ', funestus_EMQ)\n",
    "print('gambiae EMQ: ', gambiae_EMQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabiensis GAC:  0.973777604619007\n",
      "culex GAC:  0.4729513481822032\n",
      "funestus GAC:  0.4729513481822032\n",
      "gambiae GAC:  0.4220197721199228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\comp9991\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:336: FutureWarning: \n",
      "    Your problem is being solved with the ECOS solver by default. Starting in \n",
      "    CVXPY 1.5.0, Clarabel will be used as the default solver instead. To continue \n",
      "    using ECOS, specify the ECOS solver explicitly using the ``solver=cp.ECOS`` \n",
      "    argument to the ``problem.solve`` method.\n",
      "    \n",
      "  warnings.warn(ECOS_DEPRECATION_MSG, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 编码训练标签\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(y_train)\n",
    "\n",
    "res = GAC(train_scores, test_scores, train_labels, nclasses)\n",
    "\n",
    "arabiensis_GAC_estimate = res[0] * len(test_scores)\n",
    "culex_GAC_estimate = res[1] * len(test_scores)\n",
    "funestus_GAC_estimate = res[2] * len(test_scores)\n",
    "gambiae_GAC_estimate = res[3] * len(test_scores)\n",
    "\n",
    "arabiensis_GAC = Accuary(arabiensis_GAC_estimate, arabiensis_actual)\n",
    "culex_GAC = Accuary(culex_GAC_estimate, culex_actual)\n",
    "funestus_GAC = Accuary(funestus_GAC_estimate, funestus_actual)\n",
    "gambiae_GAC = Accuary(gambiae_GAC_estimate, gambiae_actual)\n",
    "\n",
    "print('arabiensis GAC: ', arabiensis_GAC)\n",
    "print('culex GAC: ', culex_GAC)\n",
    "print('funestus GAC: ', culex_GAC)\n",
    "print('gambiae GAC: ', gambiae_GAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabiensis GPAC:  0.3767976947611238\n",
      "culex GPAC:  0.11523932370097545\n",
      "funestus GPAC:  0.11523932370097545\n",
      "gambiae GPAC:  0.5774488654376889\n"
     ]
    }
   ],
   "source": [
    "res = GPAC(train_scores, test_scores, train_labels, nclasses)\n",
    "\n",
    "arabiensis_GPAC_estimate = res[0] * len(test_scores)\n",
    "culex_GPAC_estimate = res[1] * len(test_scores)\n",
    "funestus_GPAC_estimate = res[2] * len(test_scores)\n",
    "gambiae_GPAC_estimate = res[3] * len(test_scores)\n",
    "\n",
    "arabiensis_GPAC = Accuary(arabiensis_GPAC_estimate, arabiensis_actual)\n",
    "culex_GPAC = Accuary(culex_GPAC_estimate, culex_actual)\n",
    "funestus_GPAC = Accuary(funestus_GPAC_estimate, funestus_actual)\n",
    "gambiae_GPAC = Accuary(gambiae_GPAC_estimate, gambiae_actual)\n",
    "\n",
    "print('arabiensis GPAC: ', arabiensis_GPAC)\n",
    "print('culex GPAC: ', culex_GPAC)\n",
    "print('funestus GPAC: ', culex_GPAC)\n",
    "print('gambiae GPAC: ', gambiae_GPAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabiensis FM:  0.0050439052891326774\n",
      "culex FM:  0.10147810322149009\n",
      "funestus FM:  0.10147810322149009\n",
      "gambiae FM:  0.8938216627613574\n"
     ]
    }
   ],
   "source": [
    "res = FM(train_scores, test_scores, train_labels, nclasses)\n",
    "\n",
    "arabiensis_FM_estimate = res[0] * len(test_scores)\n",
    "culex_FM_estimate = res[1] * len(test_scores)\n",
    "funestus_FM_estimate = res[2] * len(test_scores)\n",
    "gambiae_FM_estimate = res[3] * len(test_scores)\n",
    "\n",
    "arabiensis_FM = Accuary(arabiensis_FM_estimate, arabiensis_actual)\n",
    "culex_FM = Accuary(culex_FM_estimate, culex_actual)\n",
    "funestus_FM = Accuary(funestus_FM_estimate, funestus_actual)\n",
    "gambiae_FM = Accuary(gambiae_FM_estimate, gambiae_actual)\n",
    "\n",
    "print('arabiensis FM: ', arabiensis_FM)\n",
    "print('culex FM: ', culex_FM)\n",
    "print('funestus FM: ', culex_FM)\n",
    "print('gambiae FM: ', gambiae_FM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabiensis threshold:  0.7934968735791429\n",
      "culex threshold:  0.9892401113182506\n",
      "funestus threshold:  0.9892401113182506\n",
      "gambiae threshold:  0.8606388968468781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "test_labels = label_encoder.fit_transform(y_test)\n",
    "\n",
    "def find_threshold_x(y_true, y_scores, nclasses):\n",
    "    thresholds = np.zeros(nclasses)\n",
    "    for i in range(nclasses):\n",
    "        fpr, tpr, thr = roc_curve(y_true == i, y_scores[:, i])\n",
    "        target_fpr = 1 - tpr\n",
    "        idx = np.argmin(np.abs(fpr - target_fpr))\n",
    "        thresholds[i] = thr[idx]\n",
    "    return thresholds\n",
    "\n",
    "def find_threshold_max(y_true, y_scores, nclasses):\n",
    "    thresholds = np.zeros(nclasses)\n",
    "    for i in range(nclasses):\n",
    "        fpr, tpr, thr = roc_curve(y_true == i, y_scores[:, i])\n",
    "        diff = tpr - fpr\n",
    "        idx = np.argmax(diff)\n",
    "        thresholds[i] = thr[idx] if idx < len(thr) else 0.5\n",
    "    return thresholds\n",
    "\n",
    "def find_threshold_t20(y_true, y_scores, nclasses):\n",
    "    thresholds = np.zeros(nclasses)\n",
    "    for i in range(nclasses):\n",
    "        fpr, tpr, thr = roc_curve(y_true == i, y_scores[:, i])\n",
    "        idx = np.argmin(np.abs(tpr - 0.2))\n",
    "        thresholds[i] = thr[idx] if idx < len(thr) else 0.2\n",
    "    return thresholds\n",
    "\n",
    "def calculate_tpr_fpr(y_true, y_scores, threshold):\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + tn)\n",
    "    return tpr, fpr\n",
    "\n",
    "thresholds = find_threshold_t20(test_labels, test_scores, nclasses)\n",
    "\n",
    "tpr = np.zeros(nclasses)\n",
    "fpr = np.zeros(nclasses)\n",
    "for i in range(nclasses):\n",
    "    tpr[i], fpr[i] = calculate_tpr_fpr(test_labels == i, test_scores[:, i], thresholds[i])\n",
    "prevalence = np.array([np.mean(test_labels == i) for i in range(nclasses)])\n",
    "p_hat = (prevalence - fpr) / (tpr - fpr)\n",
    "p_hat = np.clip(p_hat, 0, 1)\n",
    "p_hat /= p_hat.sum()\n",
    "\n",
    "arabiensis_threshold_estimate = p_hat[0] * len(test_scores)\n",
    "culex_threshold_estimate = p_hat[1] * len(test_scores)\n",
    "funestus_threshold_estimate = p_hat[2] * len(test_scores)\n",
    "gambiae_threshold_estimate = p_hat[3] * len(test_scores)\n",
    "\n",
    "arabiensis_threshold = Accuary(arabiensis_threshold_estimate, arabiensis_actual)\n",
    "culex_threshold = Accuary(culex_threshold_estimate, culex_actual)\n",
    "funestus_threshold = Accuary(funestus_threshold_estimate, funestus_actual)\n",
    "gambiae_threshold = Accuary(gambiae_threshold_estimate, gambiae_actual)\n",
    "\n",
    "print('arabiensis threshold: ', arabiensis_threshold)\n",
    "print('culex threshold: ', culex_threshold)\n",
    "print('funestus threshold: ', culex_threshold)\n",
    "print('gambiae threshold: ', gambiae_threshold)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp9991",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
