{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b847f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tabulate import tabulate\n",
    "from hyperopt import hp, fmin, tpe, Trials, space_eval\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cvx\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "import quadprog\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d24b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "arabiensis_female    3000\n",
      "culex_female         3000\n",
      "funestus_female      3000\n",
      "gambiae_female       3000\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "gambiae_female       600\n",
      "culex_female         522\n",
      "funestus_female      512\n",
      "arabiensis_female    428\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_incubator = pd.read_csv('train_incubator.csv')\n",
    "test_sf2 = pd.read_csv('test_sf2.csv')\n",
    "\n",
    "# Check number of examples per class\n",
    "print (train_incubator['class'].value_counts())\n",
    "print (test_sf2['class'].value_counts())\n",
    "\n",
    "nclasses = len(train_incubator['class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95be81b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets\n",
    "special_features = ['temperature', 'duration', 'humidity']\n",
    "wbf_features = ['L_harmcherry_wbf_mean','L_harmcherry_wbf_stddev']\n",
    "freq_features = [f'L_harmcherry_h{i}_freq' for i in range(1,9)]\n",
    "basefreq_features = [f'L_harmcherry_h{i}_basefreq' for i in range(1,9)]\n",
    "relbasefreq_features = [f'L_harmcherry_h{i}_relbasefreq' for i in range(1,9)]\n",
    "power_features = [f'L_harmcherry_h{i}_power' for i in range(1,9)]\n",
    "relpower_features = [f'L_harmcherry_h{i}_relpower' for i in range(1,9)]\n",
    "invented_features = [f'L_harmcherry_h{i}_invented' for i in range(1,9)]\n",
    "\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4df6d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  LGBM\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001315 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "number:  2062\n",
      "\tAcc: 0.4840\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.23      0.22      0.23       428\n",
      "     culex_female       0.52      0.54      0.53       522\n",
      "  funestus_female       0.93      0.46      0.62       512\n",
      "   gambiae_female       0.45      0.64      0.53       600\n",
      "\n",
      "         accuracy                           0.48      2062\n",
      "        macro avg       0.53      0.47      0.48      2062\n",
      "     weighted avg       0.54      0.48      0.49      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  96 │            128 │                 3 │              201 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 155 │            281 │                 5 │               81 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  66 │             19 │               238 │              189 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  92 │            116 │                 9 │              383 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train and test a LGBM model\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "models = [('LGBM', lgb.LGBMClassifier()),\n",
    "        #   ('XGB', XGBClassifier()),\n",
    "        #   ('RF', RandomForestClassifier())\n",
    "          ]\n",
    "# ensemble_model = VotingClassifier(estimators=[\n",
    "#     ('LGBM', lgb.LGBMClassifier(**best_params)),\n",
    "#     ('XGB', XGBClassifier()),\n",
    "#     ('RF', RandomForestClassifier())\n",
    "# ], voting='soft')\n",
    "\n",
    "for name, model in models:\n",
    "    print(\"Model: \", name)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    p_labels = model.predict(X_test)\n",
    "    a_labels = y_test\n",
    "    acc = accuracy_score(a_labels, p_labels)\n",
    "    print('number: ', len(a_labels))\n",
    "    \n",
    "    print(\"\\tAcc: %.4f\" % acc)\n",
    "    print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "        \n",
    "    cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "    print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e0bc3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: class_weights\n",
      "[LightGBM] [Warning] Unknown parameter: class_weights\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "Feature 1: Importance = 722, temperature\n",
      "Feature 2: Importance = 364, duration\n",
      "Feature 3: Importance = 1866, humidity\n",
      "Feature 4: Importance = 173, L_harmcherry_wbf_mean\n",
      "Feature 5: Importance = 412, L_harmcherry_wbf_stddev\n",
      "Feature 6: Importance = 295, L_harmcherry_h1_freq\n",
      "Feature 7: Importance = 393, L_harmcherry_h2_freq\n",
      "Feature 8: Importance = 230, L_harmcherry_h3_freq\n",
      "Feature 9: Importance = 242, L_harmcherry_h4_freq\n",
      "Feature 10: Importance = 185, L_harmcherry_h5_freq\n",
      "Feature 11: Importance = 209, L_harmcherry_h6_freq\n",
      "Feature 12: Importance = 168, L_harmcherry_h7_freq\n",
      "Feature 13: Importance = 235, L_harmcherry_h8_freq\n",
      "Feature 22: Importance = 358, L_harmcherry_h1_relbasefreq\n",
      "Feature 23: Importance = 448, L_harmcherry_h2_relbasefreq\n",
      "Feature 24: Importance = 403, L_harmcherry_h3_relbasefreq\n",
      "Feature 25: Importance = 404, L_harmcherry_h4_relbasefreq\n",
      "Feature 26: Importance = 416, L_harmcherry_h5_relbasefreq\n",
      "Feature 27: Importance = 320, L_harmcherry_h6_relbasefreq\n",
      "Feature 28: Importance = 347, L_harmcherry_h7_relbasefreq\n",
      "Feature 29: Importance = 423, L_harmcherry_h8_relbasefreq\n",
      "Feature 30: Importance = 486, L_harmcherry_h1_power\n",
      "Feature 31: Importance = 438, L_harmcherry_h2_power\n",
      "Feature 32: Importance = 425, L_harmcherry_h3_power\n",
      "Feature 33: Importance = 408, L_harmcherry_h4_power\n",
      "Feature 34: Importance = 389, L_harmcherry_h5_power\n",
      "Feature 35: Importance = 394, L_harmcherry_h6_power\n",
      "Feature 36: Importance = 396, L_harmcherry_h7_power\n",
      "Feature 37: Importance = 451, L_harmcherry_h8_power\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "# class_weights = {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0}\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(class_weights='balanced')\n",
    "\n",
    "# 训练模型\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# 获取特征重要性\n",
    "feature_importance = lgb_model.feature_importances_\n",
    "\n",
    "new_features = []\n",
    "\n",
    "# 打印特征重要性\n",
    "for i, importance in enumerate(feature_importance):\n",
    "    if ((importance - 1) > 0):\n",
    "        print(f\"Feature {i+1}: Importance = {importance}, {feature_set[i]}\")\n",
    "        new_features.append(feature_set[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22cd6c",
   "metadata": {},
   "source": [
    "Test's confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7e39f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "number:  2062\n",
      "\tAcc: 0.4840\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.23      0.22      0.23       428\n",
      "     culex_female       0.52      0.54      0.53       522\n",
      "  funestus_female       0.93      0.46      0.62       512\n",
      "   gambiae_female       0.45      0.64      0.53       600\n",
      "\n",
      "         accuracy                           0.48      2062\n",
      "        macro avg       0.53      0.47      0.48      2062\n",
      "     weighted avg       0.54      0.48      0.49      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  96 │            128 │                 3 │              201 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 155 │            281 │                 5 │               81 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  66 │             19 │               238 │              189 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  92 │            116 │                 9 │              383 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Train and test a LGBM model\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "    \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "p_labels = model.predict(X_test)\n",
    "a_labels = y_test\n",
    "acc = accuracy_score(a_labels, p_labels)\n",
    "print('number: ', len(a_labels))\n",
    "\n",
    "print(\"\\tAcc: %.4f\" % acc)\n",
    "print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "    \n",
    "cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5fc6e",
   "metadata": {},
   "source": [
    "Lab's confustion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b8f2da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8981\n",
      "[LightGBM] [Info] Number of data points in the train set: 9395, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.379823\n",
      "[LightGBM] [Info] Start training from score -1.321889\n",
      "[LightGBM] [Info] Start training from score -1.412937\n",
      "[LightGBM] [Info] Start training from score -1.434148\n",
      "number:  2605\n",
      "\tAcc: 0.5893\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.44      0.42      0.43       636\n",
      "     culex_female       0.56      0.77      0.65       495\n",
      "  funestus_female       0.72      0.80      0.76       713\n",
      "   gambiae_female       0.58      0.42      0.49       761\n",
      "\n",
      "         accuracy                           0.59      2605\n",
      "        macro avg       0.58      0.60      0.58      2605\n",
      "     weighted avg       0.58      0.59      0.58      2605\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 264 │            131 │               111 │              130 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  67 │            379 │                10 │               39 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  50 │             31 │               573 │               59 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 214 │            130 │                98 │              319 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001504 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8972\n",
      "[LightGBM] [Info] Number of data points in the train set: 9638, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.430199\n",
      "[LightGBM] [Info] Start training from score -1.406205\n",
      "[LightGBM] [Info] Start training from score -1.335126\n",
      "[LightGBM] [Info] Start training from score -1.376178\n",
      "number:  2362\n",
      "\tAcc: 0.5453\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.04      0.01      0.01       694\n",
      "     culex_female       0.80      0.75      0.77       638\n",
      "  funestus_female       0.85      0.82      0.83       464\n",
      "   gambiae_female       0.36      0.76      0.48       566\n",
      "\n",
      "         accuracy                           0.55      2362\n",
      "        macro avg       0.51      0.58      0.53      2362\n",
      "     weighted avg       0.48      0.55      0.49      2362\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                   5 │             57 │                27 │              605 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  19 │            476 │                14 │              129 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  34 │             11 │               379 │               40 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  59 │             52 │                27 │              428 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001662 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8985\n",
      "[LightGBM] [Info] Number of data points in the train set: 9961, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.429059\n",
      "[LightGBM] [Info] Start training from score -1.371645\n",
      "[LightGBM] [Info] Start training from score -1.410375\n",
      "[LightGBM] [Info] Start training from score -1.336649\n",
      "number:  2039\n",
      "\tAcc: 0.6057\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.56      0.45      0.50       614\n",
      "     culex_female       0.67      0.55      0.60       473\n",
      "  funestus_female       0.76      0.83      0.79       569\n",
      "   gambiae_female       0.43      0.59      0.49       383\n",
      "\n",
      "         accuracy                           0.61      2039\n",
      "        macro avg       0.60      0.61      0.60      2039\n",
      "     weighted avg       0.61      0.61      0.60      2039\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 278 │             79 │                66 │              191 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 123 │            260 │                18 │               72 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  31 │             25 │               471 │               42 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  68 │             23 │                66 │              226 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002121 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8989\n",
      "[LightGBM] [Info] Number of data points in the train set: 10062, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.428724\n",
      "[LightGBM] [Info] Start training from score -1.370714\n",
      "[LightGBM] [Info] Start training from score -1.445454\n",
      "[LightGBM] [Info] Start training from score -1.306298\n",
      "number:  1938\n",
      "\tAcc: 0.6259\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.67      0.29      0.41       589\n",
      "     culex_female       0.55      0.83      0.66       445\n",
      "  funestus_female       0.85      0.81      0.83       629\n",
      "   gambiae_female       0.40      0.60      0.48       275\n",
      "\n",
      "         accuracy                           0.63      1938\n",
      "        macro avg       0.62      0.63      0.59      1938\n",
      "     weighted avg       0.66      0.63      0.61      1938\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 172 │            229 │                50 │              138 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                   8 │            369 │                 1 │               67 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  35 │             40 │               507 │               47 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  41 │             34 │                35 │              165 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001568 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8977\n",
      "[LightGBM] [Info] Number of data points in the train set: 8944, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.261579\n",
      "[LightGBM] [Info] Start training from score -1.472655\n",
      "[LightGBM] [Info] Start training from score -1.325985\n",
      "[LightGBM] [Info] Start training from score -1.505364\n",
      "number:  3056\n",
      "\tAcc: 0.5370\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.24      0.54      0.33       467\n",
      "     culex_female       0.69      0.50      0.58       949\n",
      "  funestus_female       0.70      0.68      0.69       625\n",
      "   gambiae_female       0.70      0.48      0.57      1015\n",
      "\n",
      "         accuracy                           0.54      3056\n",
      "        macro avg       0.58      0.55      0.54      3056\n",
      "     weighted avg       0.63      0.54      0.56      3056\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 253 │             69 │                65 │               80 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 356 │            477 │                38 │               78 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 123 │             27 │               427 │               48 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 334 │            114 │                83 │              484 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y = train_incubator['class'].values \n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "groups = train_incubator['sensor'].values\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "for train_index, test_index in group_kfold.split(X, y, groups):\n",
    "  X_train, y_train, X_test, y_test = X[train_index], y[train_index], X[test_index], y[test_index]\n",
    "  model.fit(X[train_index], y[train_index])\n",
    "\n",
    "  p_labels = model.predict(X_test)\n",
    "  a_labels = y_test\n",
    "  acc = accuracy_score(a_labels, p_labels)\n",
    "  print('number: ', len(a_labels))\n",
    "\n",
    "  print(\"\\tAcc: %.4f\" % acc)\n",
    "  print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "      \n",
    "  cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "  print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fcbabbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8981\n",
      "[LightGBM] [Info] Number of data points in the train set: 9395, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.379823\n",
      "[LightGBM] [Info] Start training from score -1.321889\n",
      "[LightGBM] [Info] Start training from score -1.412937\n",
      "[LightGBM] [Info] Start training from score -1.434148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8972\n",
      "[LightGBM] [Info] Number of data points in the train set: 9638, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.430199\n",
      "[LightGBM] [Info] Start training from score -1.406205\n",
      "[LightGBM] [Info] Start training from score -1.335126\n",
      "[LightGBM] [Info] Start training from score -1.376178\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8985\n",
      "[LightGBM] [Info] Number of data points in the train set: 9961, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.429059\n",
      "[LightGBM] [Info] Start training from score -1.371645\n",
      "[LightGBM] [Info] Start training from score -1.410375\n",
      "[LightGBM] [Info] Start training from score -1.336649\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8989\n",
      "[LightGBM] [Info] Number of data points in the train set: 10062, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.428724\n",
      "[LightGBM] [Info] Start training from score -1.370714\n",
      "[LightGBM] [Info] Start training from score -1.445454\n",
      "[LightGBM] [Info] Start training from score -1.306298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8984\n",
      "[LightGBM] [Info] Number of data points in the train set: 10254, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.344092\n",
      "[LightGBM] [Info] Start training from score -1.494324\n",
      "[LightGBM] [Info] Start training from score -1.318980\n",
      "[LightGBM] [Info] Start training from score -1.396686\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8982\n",
      "[LightGBM] [Info] Number of data points in the train set: 10690, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.318837\n",
      "[LightGBM] [Info] Start training from score -1.357708\n",
      "[LightGBM] [Info] Start training from score -1.401185\n",
      "[LightGBM] [Info] Start training from score -1.474037\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[0.22315699 0.22405571 0.05762986 0.49515744]\n"
     ]
    }
   ],
   "source": [
    "def getScores(X_train, X_test, Y_train, nclasses):\n",
    "\n",
    "    # model = lgb.LGBMClassifier(**best_params)\n",
    "    model = lgb.LGBMClassifier()\n",
    "   \n",
    "    train_scores = np.zeros((len(X_train), nclasses))\n",
    "    test_scores = np.zeros((len(X_test), nclasses))\n",
    "\n",
    "    groups = train_incubator['sensor'].values\n",
    "    group_kfold = GroupKFold(n_splits=6)\n",
    "\n",
    "    for train_index, test_index in group_kfold.split(X_train, Y_train, groups):\n",
    "        model.fit(X_train[train_index], Y_train[train_index])\n",
    "        train_scores[test_index] = model.predict_proba(X_train)[test_index]\n",
    "    \n",
    "    model.fit(X_train, Y_train)\n",
    "    test_scores = model.predict_proba(X_test)\n",
    "           \n",
    "    return train_scores, test_scores\n",
    "\n",
    "def EMQ(test_scores, nclasses):\n",
    "    max_it = 1000        # Max num of iterations\n",
    "    eps = 1e-1           # Small constant for stopping criterium\n",
    "\n",
    "    p_tr = [0.25, 0.25, 0.25, 0.25]\n",
    "    p_s = np.copy(p_tr)\n",
    "    p_cond_tr = np.array(test_scores)\n",
    "    p_cond_s = np.zeros(p_cond_tr.shape)\n",
    "    prob_arrays = []\n",
    "\n",
    "    for _ in range(max_it):\n",
    "        # Add Laplacian smoothing\n",
    "        # r = (p_s + alpha) / (p_tr + (alpha * nclasses))\n",
    "        r = p_s / p_tr\n",
    "        \n",
    "        p_cond_s = p_cond_tr * r\n",
    "        s = np.sum(p_cond_s, axis = 1)\n",
    "        for c in range(nclasses):\n",
    "            p_cond_s[:,c] = p_cond_s[:,c] / s\n",
    "\n",
    "        prob_arrays.append(p_cond_s)\n",
    "        p_s_old = np.copy(p_s)\n",
    "        p_s = np.sum(p_cond_s, axis = 0) / p_cond_s.shape[0]\n",
    "        if (np.sum(np.abs(p_s - p_s_old)) < eps):\n",
    "            break\n",
    "\n",
    "    return(p_s/np.sum(p_s))\n",
    "    # return p_cond_s\n",
    "\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).to_numpy()\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "train_scores, test_scores = getScores(X_train, X_test, y_train, 4)\n",
    "res = EMQ(test_scores, 4)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738711a",
   "metadata": {},
   "source": [
    "Probabilistic Classify and Count (PCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fca72584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "Estimated counts using PCC:\n",
      "arabiensis_female: 528.99\n",
      "culex_female: 522.31\n",
      "funestus_female: 224.69\n",
      "gambiae_female: 786.01\n"
     ]
    }
   ],
   "source": [
    "# 将数据集拆分为训练集和测试集\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).to_numpy()\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "# 训练一个概率分类器（这里使用随机森林分类器）\n",
    "model = lgb.LGBMClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 预测测试集的后验概率\n",
    "proba_predictions = model.predict_proba(X_test)\n",
    "class_labels = model.classes_\n",
    "\n",
    "# 使用PCC方法估计每个类别的数量\n",
    "# 通过计算每个类别后验概率的平均值并乘以总样本数，得到每个类别的估计数量\n",
    "def pcc(proba_predictions, class_labels):\n",
    "    estimated_counts = np.mean(proba_predictions, axis=0) * len(proba_predictions)\n",
    "    return dict(zip(class_labels, estimated_counts))\n",
    "\n",
    "estimated_counts = pcc(proba_predictions, class_labels)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Estimated counts using PCC:\")\n",
    "for label, count in estimated_counts.items():\n",
    "    print(f\"{label}: {count:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d7215",
   "metadata": {},
   "source": [
    "Expectation Maximisation for Quantification(EMQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8931433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001373 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8981\n",
      "[LightGBM] [Info] Number of data points in the train set: 9395, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.379823\n",
      "[LightGBM] [Info] Start training from score -1.321889\n",
      "[LightGBM] [Info] Start training from score -1.412937\n",
      "[LightGBM] [Info] Start training from score -1.434148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8972\n",
      "[LightGBM] [Info] Number of data points in the train set: 9638, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.430199\n",
      "[LightGBM] [Info] Start training from score -1.406205\n",
      "[LightGBM] [Info] Start training from score -1.335126\n",
      "[LightGBM] [Info] Start training from score -1.376178\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8985\n",
      "[LightGBM] [Info] Number of data points in the train set: 9961, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.429059\n",
      "[LightGBM] [Info] Start training from score -1.371645\n",
      "[LightGBM] [Info] Start training from score -1.410375\n",
      "[LightGBM] [Info] Start training from score -1.336649\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000956 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8989\n",
      "[LightGBM] [Info] Number of data points in the train set: 10062, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.428724\n",
      "[LightGBM] [Info] Start training from score -1.370714\n",
      "[LightGBM] [Info] Start training from score -1.445454\n",
      "[LightGBM] [Info] Start training from score -1.306298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001799 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8984\n",
      "[LightGBM] [Info] Number of data points in the train set: 10254, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.344092\n",
      "[LightGBM] [Info] Start training from score -1.494324\n",
      "[LightGBM] [Info] Start training from score -1.318980\n",
      "[LightGBM] [Info] Start training from score -1.396686\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8982\n",
      "[LightGBM] [Info] Number of data points in the train set: 10690, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.318837\n",
      "[LightGBM] [Info] Start training from score -1.357708\n",
      "[LightGBM] [Info] Start training from score -1.401185\n",
      "[LightGBM] [Info] Start training from score -1.474037\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "Estimated counts using PCC:\n",
      "arabiensis_female: 528.99\n",
      "culex_female: 522.31\n",
      "funestus_female: 224.69\n",
      "gambiae_female: 786.01\n",
      "Estimated counts using EMQ:\n",
      "[ 460.1497194   462.00287171  118.83277603 1021.01463286]\n"
     ]
    }
   ],
   "source": [
    "def getScores(X_train, X_test, Y_train, nclasses):\n",
    "\n",
    "    model = lgb.LGBMClassifier(**best_params)\n",
    "    # model = lgb.LGBMClassifier()\n",
    "   \n",
    "    train_scores = np.zeros((len(X_train), nclasses))\n",
    "    test_scores = np.zeros((len(X_test), nclasses))\n",
    "\n",
    "    groups = train_incubator['sensor'].values\n",
    "    group_kfold = GroupKFold(n_splits=6)\n",
    "\n",
    "    for train_index, test_index in group_kfold.split(X_train, Y_train, groups):\n",
    "        model.fit(X_train[train_index], Y_train[train_index])\n",
    "        train_scores[test_index] = model.predict_proba(X_train)[test_index]\n",
    "    \n",
    "    model.fit(X_train, Y_train)\n",
    "    test_scores = model.predict_proba(X_test)\n",
    "           \n",
    "    return train_scores, test_scores\n",
    "\n",
    "def pcc(test_scores, class_labels):\n",
    "    estimated_counts = np.mean(test_scores, axis=0) * len(test_scores)\n",
    "    return dict(zip(class_labels, estimated_counts))\n",
    "\n",
    "def EMQ(test_scores, nclasses):\n",
    "    max_it = 1000        # Max num of iterations\n",
    "    eps = 1e-1           # Small constant for stopping criterium\n",
    "\n",
    "    p_tr = [0.25, 0.25, 0.25, 0.25]\n",
    "    p_s = np.copy(p_tr)\n",
    "    p_cond_tr = np.array(test_scores)\n",
    "    p_cond_s = np.zeros(p_cond_tr.shape)\n",
    "    prob_arrays = []\n",
    "\n",
    "    for _ in range(max_it):\n",
    "        # Add Laplacian smoothing\n",
    "        # r = (p_s + alpha) / (p_tr + (alpha * nclasses))\n",
    "        r = p_s / p_tr\n",
    "        \n",
    "        p_cond_s = p_cond_tr * r\n",
    "        s = np.sum(p_cond_s, axis = 1)\n",
    "        for c in range(nclasses):\n",
    "            p_cond_s[:,c] = p_cond_s[:,c] / s\n",
    "\n",
    "        prob_arrays.append(p_cond_s)\n",
    "        p_s_old = np.copy(p_s)\n",
    "        p_s = np.sum(p_cond_s, axis = 0) / p_cond_s.shape[0]\n",
    "        if (np.sum(np.abs(p_s - p_s_old)) < eps):\n",
    "            break\n",
    "\n",
    "    return (p_s/np.sum(p_s))\n",
    "    # return p_cond_s \n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).to_numpy()\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "nclasses = 4\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "# y_proba = model.predict_proba(X_test)\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "train_scores, test_scores = getScores(X_train, X_test, y_train, nclasses)\n",
    "\n",
    "pcc_estimated_counts = pcc(test_scores, class_labels)\n",
    "\n",
    "emq_estimated_counts = EMQ(test_scores, nclasses)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Estimated counts using PCC:\")\n",
    "for label, count in pcc_estimated_counts.items():\n",
    "    print(f\"{label}: {count:.2f}\")\n",
    "\n",
    "print(\"Estimated counts using EMQ:\")\n",
    "print(emq_estimated_counts * len(test_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48385a",
   "metadata": {},
   "source": [
    "Probabilistic Adjusted Classify and Count(PACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a963a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27817765 0.27040989 0.0178869  0.43352555]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'numpy.float64'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 42\u001b[0m\n\u001b[0;32m     38\u001b[0m nclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# train_scores, test_scores = getScores(X_train, X_test, y_train, nclasses)\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m estimated_counts \u001b[38;5;241m=\u001b[39m \u001b[43mPACC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimated Counts:\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimated_counts)\n",
      "Cell \u001b[1;32mIn[19], line 18\u001b[0m, in \u001b[0;36mPACC\u001b[1;34m(proba_predictions, y_true)\u001b[0m\n\u001b[0;32m     16\u001b[0m     expected_conditional \u001b[38;5;241m=\u001b[39m expected_conditionals[class_label]\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(expected_conditional)\n\u001b[1;32m---> 18\u001b[0m     estimated_count \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpected_conditional\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclass_count\u001b[49m\n\u001b[0;32m     19\u001b[0m     estimated_counts\u001b[38;5;241m.\u001b[39mappend(estimated_count)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 将类别标签和估计数量对应起来，形成字典\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'numpy.float64'"
     ]
    }
   ],
   "source": [
    "def PACC(proba_predictions, y_true):\n",
    "    # 计算真实类别的数量\n",
    "    class_counts = {'A': 428, 'B': 522, 'C': 512, 'D': 600}\n",
    "    \n",
    "    # 计算各个类别的条件期望\n",
    "    expected_conditionals = []\n",
    "    for class_label in np.unique(y_true):\n",
    "        class_indices = np.where(y_true == class_label)[0]\n",
    "        class_proba_predictions = proba_predictions[class_indices]\n",
    "        class_expected_conditional = np.mean(class_proba_predictions, axis=0)\n",
    "        expected_conditionals.append(class_expected_conditional)\n",
    "    \n",
    "    # 计算每个类别的估计数量\n",
    "    estimated_counts = []\n",
    "    for class_label, class_count in enumerate(class_counts):\n",
    "        expected_conditional = expected_conditionals[class_label]\n",
    "        print(expected_conditional)\n",
    "        estimated_count = np.sum(expected_conditional) * class_count\n",
    "        estimated_counts.append(estimated_count)\n",
    "    \n",
    "    # 将类别标签和估计数量对应起来，形成字典\n",
    "    class_labels = np.unique(y_true)\n",
    "    estimated_counts_dict = dict(zip(class_labels, estimated_counts))\n",
    "    \n",
    "    return estimated_counts_dict\n",
    "\n",
    "# 使用示例\n",
    "# proba_predictions 是测试集的后验概率预测结果，shape为(n_samples, n_classes)\n",
    "# y_true 是测试集的真实类别标签\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).to_numpy()\n",
    "y_test = test_sf2['class']\n",
    "\n",
    "nclasses = 4\n",
    "\n",
    "# train_scores, test_scores = getScores(X_train, X_test, y_train, nclasses)\n",
    "\n",
    "estimated_counts = PACC(test_scores, y_test)\n",
    "print(\"Estimated Counts:\", estimated_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680ea50",
   "metadata": {},
   "source": [
    "### Estimate the probability distribution for each class using different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "004a7cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabiensis_female' 'culex_female' 'funestus_female' 'gambiae_female']\n",
      "12000\n",
      "[0.25 0.25 0.25 0.25]\n"
     ]
    }
   ],
   "source": [
    "def class_dist(Y, nclasses):\n",
    "    return np.array([np.count_nonzero(Y == i) for i in range(nclasses)]) / Y.shape[0]\n",
    "\n",
    "Y = train_incubator['class'].values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y)\n",
    "\n",
    "classes = class_dist(Y_encoded, nclasses)\n",
    "\n",
    "print(np.unique(Y))\n",
    "print(Y.shape[0])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5c3aa4",
   "metadata": {},
   "source": [
    "参数选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c145fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best parameters found: OrderedDict([('colsample_bytree', 0.6), ('learning_rate', 0.05052669042178889), ('max_depth', 13), ('min_child_samples', 72), ('n_estimators', 117), ('num_leaves', 35), ('reg_alpha', 9.104333828074182e-06), ('reg_lambda', 0.0003436244662748896), ('subsample', 0.8892404650088725)])\n",
      "number:  2062\n",
      "\tAcc: 0.4840\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.25      0.20      0.22       428\n",
      "     culex_female       0.51      0.54      0.52       522\n",
      "  funestus_female       0.90      0.49      0.64       512\n",
      "   gambiae_female       0.43      0.63      0.51       600\n",
      "\n",
      "         accuracy                           0.48      2062\n",
      "        macro avg       0.52      0.47      0.47      2062\n",
      "     weighted avg       0.53      0.48      0.49      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  86 │            121 │                 6 │              215 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 150 │            280 │                 5 │               87 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  34 │             21 │               252 │              205 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  81 │            123 │                16 │              380 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from skopt import BayesSearchCV\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# 构建特征集和目标变量\n",
    "feature_set = special_features + wbf_features + freq_features + basefreq_features + relbasefreq_features + power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "# 假设 sensor_id 在 train_incubator 中\n",
    "groups = train_incubator['sensor'].values\n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "# 定义模型\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "# 定义参数空间\n",
    "param_space = {\n",
    "    'num_leaves': (10, 100),  # 整数范围\n",
    "    'learning_rate': (1e-3, 1e-1, 'log-uniform'),  # 对数均匀分布\n",
    "    'n_estimators': (50, 500),  # 整数范围\n",
    "    'max_depth': (3, 15),  # 树的最大深度\n",
    "    'min_child_samples': (5, 100),  # 子叶节点的最小数据量\n",
    "    'subsample': (0.6, 1.0),  # 每棵树的样本子集比例\n",
    "    'colsample_bytree': (0.6, 1.0),  # 每棵树的特征子集比例\n",
    "    'reg_alpha': (1e-6, 1e-1, 'log-uniform'),  # L1 正则化\n",
    "    'reg_lambda': (1e-6, 1e-1, 'log-uniform')  # L2 正则化\n",
    "}\n",
    "\n",
    "# 交叉验证的折数\n",
    "cv_folds = 6\n",
    "\n",
    "# 创建 GroupKFold 实例\n",
    "group_kfold = GroupKFold(n_splits=cv_folds)\n",
    "\n",
    "# 创建 BayesSearchCV 实例\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=model,\n",
    "    search_spaces=param_space,\n",
    "    cv=group_kfold,\n",
    "    scoring='accuracy',\n",
    "    n_iter=50,  # 搜索次数\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 进行参数搜索和模型训练\n",
    "bayes_search.fit(X_train, y_train, groups=groups)\n",
    "\n",
    "# 输出最佳参数\n",
    "print(f\"Best parameters found: {bayes_search.best_params_}\")\n",
    "\n",
    "# 使用最佳参数的模型\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "# 评估在测试集上的性能\n",
    "p_labels = best_model.predict(X_test)\n",
    "a_labels = y_test\n",
    "acc = accuracy_score(a_labels, p_labels)\n",
    "print('number: ', len(a_labels))\n",
    "\n",
    "print(\"\\tAcc: %.4f\" % acc)\n",
    "print(classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "\n",
    "cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab9a5eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "number:  2062\n",
      "\tAcc: 0.4840\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.25      0.20      0.22       428\n",
      "     culex_female       0.51      0.54      0.52       522\n",
      "  funestus_female       0.90      0.49      0.64       512\n",
      "   gambiae_female       0.43      0.63      0.51       600\n",
      "\n",
      "         accuracy                           0.48      2062\n",
      "        macro avg       0.52      0.47      0.47      2062\n",
      "     weighted avg       0.53      0.48      0.49      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  86 │            121 │                 6 │              215 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 150 │            280 │                 5 │               87 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  34 │             21 │               252 │              205 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  81 │            123 │                16 │              380 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Train and test a LGBM model\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "# 最佳参数\n",
    "best_params = {\n",
    "    'colsample_bytree': 0.6,\n",
    "    'learning_rate': 0.05052669042178889,\n",
    "    'max_depth': 13,\n",
    "    'min_child_samples': 72,\n",
    "    'n_estimators': 117,\n",
    "    'num_leaves': 35,\n",
    "    'reg_alpha': 9.104333828074182e-06,\n",
    "    'reg_lambda': 0.0003436244662748896,\n",
    "    'subsample': 0.8892404650088725\n",
    "}\n",
    "\n",
    "\n",
    "model = lgb.LGBMClassifier(**best_params)\n",
    "    \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "p_labels = model.predict(X_test)\n",
    "a_labels = y_test\n",
    "acc = accuracy_score(a_labels, p_labels)\n",
    "print('number: ', len(a_labels))\n",
    "\n",
    "print(\"\\tAcc: %.4f\" % acc)\n",
    "print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "    \n",
    "cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp9991",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
