{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b847f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tabulate import tabulate\n",
    "from hyperopt import hp, fmin, tpe, Trials, space_eval\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cvx\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "import quadprog\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d24b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "arabiensis_female    3000\n",
      "culex_female         3000\n",
      "funestus_female      3000\n",
      "gambiae_female       3000\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "gambiae_female       600\n",
      "culex_female         522\n",
      "funestus_female      512\n",
      "arabiensis_female    428\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_incubator = pd.read_csv('train_incubator.csv')\n",
    "test_sf2 = pd.read_csv('test_sf2.csv')\n",
    "\n",
    "# Check number of examples per class\n",
    "print (train_incubator['class'].value_counts())\n",
    "print (test_sf2['class'].value_counts())\n",
    "\n",
    "nclasses = len(train_incubator['class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "95be81b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets\n",
    "special_features = ['temperature', 'duration', 'humidity']\n",
    "wbf_features = ['L_harmcherry_wbf_mean','L_harmcherry_wbf_stddev']\n",
    "freq_features = [f'L_harmcherry_h{i}_freq' for i in range(1,9)]\n",
    "basefreq_features = [f'L_harmcherry_h{i}_basefreq' for i in range(1,9)]\n",
    "relbasefreq_features = [f'L_harmcherry_h{i}_relbasefreq' for i in range(1,9)]\n",
    "power_features = [f'L_harmcherry_h{i}_power' for i in range(1,9)]\n",
    "relpower_features = [f'L_harmcherry_h{i}_relpower' for i in range(1,9)]\n",
    "invented_features = [f'L_harmcherry_h{i}_invented' for i in range(1,9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "4df6d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  LGBM\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001676 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "number:  2062\n",
      "\tAcc: 0.4840\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.23      0.22      0.23       428\n",
      "     culex_female       0.52      0.54      0.53       522\n",
      "  funestus_female       0.93      0.46      0.62       512\n",
      "   gambiae_female       0.45      0.64      0.53       600\n",
      "\n",
      "         accuracy                           0.48      2062\n",
      "        macro avg       0.53      0.47      0.48      2062\n",
      "     weighted avg       0.54      0.48      0.49      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  96 │            128 │                 3 │              201 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 155 │            281 │                 5 │               81 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  66 │             19 │               238 │              189 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  92 │            116 │                 9 │              383 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Train and test a LGBM model\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "models = [('LGBM', lgb.LGBMClassifier())]\n",
    "\n",
    "for name, model in models:\n",
    "    print(\"Model: \", name)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    p_labels = model.predict(X_test)\n",
    "    a_labels = y_test\n",
    "    acc = accuracy_score(a_labels, p_labels)\n",
    "    print('number: ', len(a_labels))\n",
    "    \n",
    "    print(\"\\tAcc: %.4f\" % acc)\n",
    "    print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "        \n",
    "    cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "    print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "f8931433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.14      0.12      0.13       428\n",
      "     culex_female       0.54      0.48      0.51       522\n",
      "  funestus_female       0.92      0.20      0.32       512\n",
      "   gambiae_female       0.40      0.74      0.52       600\n",
      "\n",
      "         accuracy                           0.41      2062\n",
      "        macro avg       0.50      0.39      0.37      2062\n",
      "     weighted avg       0.51      0.41      0.39      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  52 │            104 │                 1 │              271 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 122 │            251 │                 4 │              145 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 136 │             17 │               101 │              258 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  52 │             97 │                 4 │              447 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "def getScores(X_train, X_test, Y_train, nclasses):\n",
    "\n",
    "    # model = lgb.LGBMClassifier(**best_params)\n",
    "    model = lgb.LGBMClassifier()\n",
    "   \n",
    "    train_scores = np.zeros((len(X_train), nclasses))\n",
    "    test_scores = np.zeros((len(X_test), nclasses))\n",
    "\n",
    "    groups = train_incubator['sensor'].values\n",
    "    group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "    for train_index, test_index in group_kfold.split(X_train, Y_train, groups):\n",
    "        model.fit(X_train[train_index], Y_train[train_index])\n",
    "        train_scores[test_index] = model.predict_proba(X_train)[test_index]\n",
    "    \n",
    "    model.fit(X_train, Y_train)\n",
    "    test_scores = model.predict_proba(X_test)\n",
    "           \n",
    "    return train_scores, test_scores\n",
    "\n",
    "def EMQ(test_scores, nclasses):\n",
    "    max_it = 1000        # Max num of iterations\n",
    "    eps = 1e-1           # Small constant for stopping criterium\n",
    "\n",
    "    p_tr = [0.25, 0.25, 0.25, 0.25]\n",
    "    p_s = np.copy(p_tr)\n",
    "    p_cond_tr = np.array(test_scores)\n",
    "    p_cond_s = np.zeros(p_cond_tr.shape)\n",
    "    prob_arrays = []\n",
    "\n",
    "    for _ in range(max_it):\n",
    "        # Add Laplacian smoothing\n",
    "        # r = (p_s + alpha) / (p_tr + (alpha * nclasses))\n",
    "        r = p_s / p_tr\n",
    "        \n",
    "        p_cond_s = p_cond_tr * r\n",
    "        s = np.sum(p_cond_s, axis = 1)\n",
    "        for c in range(nclasses):\n",
    "            p_cond_s[:,c] = p_cond_s[:,c] / s\n",
    "\n",
    "        prob_arrays.append(p_cond_s)\n",
    "        p_s_old = np.copy(p_s)\n",
    "        p_s = np.sum(p_cond_s, axis = 0) / p_cond_s.shape[0]\n",
    "        if (np.sum(np.abs(p_s - p_s_old)) < eps):\n",
    "            break\n",
    "\n",
    "    # return(p_s/np.sum(p_s))\n",
    "    return p_cond_s\n",
    "\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "# Select evaluation indicators\n",
    "scoring = 'accuracy'\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_proba = model.predict_proba(X_test)\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "emq_result = EMQ(y_proba, 4)\n",
    "# print(emq_result)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "p_pred = []\n",
    "cont = 0\n",
    "for item in emq_result:\n",
    "    cont += 1\n",
    "    p_pred.append(np.argmax(item))\n",
    "\n",
    "train_labels = label_encoder.fit_transform(y_test)\n",
    "p_pred = label_encoder.inverse_transform(p_pred)\n",
    "\n",
    "print (classification_report(y_test, p_pred, labels=np.unique(y_test)))\n",
    "cf = confusion_matrix(y_test, p_pred, labels=np.unique(y_test))\n",
    "print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "caca9813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: class_weights\n",
      "[LightGBM] [Warning] Unknown parameter: class_weights\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011871 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 61617\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 284\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "Feature 1: Importance = 469, temperature\n",
      "Feature 2: Importance = 68, duration\n",
      "Feature 3: Importance = 1147, humidity\n",
      "Feature 4: Importance = 505, luminosity\n",
      "Feature 5: Importance = 1142, altitude\n",
      "Feature 6: Importance = 298, air_pressure\n",
      "Feature 7: Importance = 1091, hour\n",
      "Feature 8: Importance = 19, L_harmcherry_wbf_mean\n",
      "Feature 9: Importance = 30, L_harmcherry_wbf_stddev\n",
      "Feature 10: Importance = 7, L_wbf_autocorrelation\n",
      "Feature 11: Importance = 30, L_wbf_cepstrum\n",
      "Feature 12: Importance = 26, L_harmcherry_wbf_coeffofvariation\n",
      "Feature 14: Importance = 7, R_harmcherry_wbf_mean\n",
      "Feature 15: Importance = 13, R_harmcherry_wbf_stddev\n",
      "Feature 16: Importance = 27, R_wbf_autocorrelation\n",
      "Feature 17: Importance = 22, R_wbf_cepstrum\n",
      "Feature 18: Importance = 7, R_harmcherry_wbf_coeffofvariation\n",
      "Feature 20: Importance = 7, M_harmcherry_wbf_mean\n",
      "Feature 21: Importance = 22, M_harmcherry_wbf_stddev\n",
      "Feature 22: Importance = 40, M_wbf_autocorrelation\n",
      "Feature 23: Importance = 16, M_wbf_cepstrum\n",
      "Feature 24: Importance = 16, M_harmcherry_wbf_coeffofvariation\n",
      "Feature 25: Importance = 3, M_harmcherry_wbf_trustworthyharmonics\n",
      "Feature 26: Importance = 17, L_harmcherry_h1_freq\n",
      "Feature 27: Importance = 31, L_harmcherry_h2_freq\n",
      "Feature 28: Importance = 17, L_harmcherry_h3_freq\n",
      "Feature 29: Importance = 24, L_harmcherry_h4_freq\n",
      "Feature 30: Importance = 12, L_harmcherry_h5_freq\n",
      "Feature 31: Importance = 29, L_harmcherry_h6_freq\n",
      "Feature 32: Importance = 22, L_harmcherry_h7_freq\n",
      "Feature 33: Importance = 41, L_harmcherry_h8_freq\n",
      "Feature 34: Importance = 17, R_harmcherry_h1_freq\n",
      "Feature 35: Importance = 14, R_harmcherry_h2_freq\n",
      "Feature 36: Importance = 13, R_harmcherry_h3_freq\n",
      "Feature 37: Importance = 14, R_harmcherry_h4_freq\n",
      "Feature 38: Importance = 16, R_harmcherry_h5_freq\n",
      "Feature 39: Importance = 20, R_harmcherry_h6_freq\n",
      "Feature 40: Importance = 9, R_harmcherry_h7_freq\n",
      "Feature 41: Importance = 8, R_harmcherry_h8_freq\n",
      "Feature 42: Importance = 27, M_harmcherry_h1_freq\n",
      "Feature 43: Importance = 21, M_harmcherry_h2_freq\n",
      "Feature 44: Importance = 10, M_harmcherry_h3_freq\n",
      "Feature 45: Importance = 17, M_harmcherry_h4_freq\n",
      "Feature 46: Importance = 17, M_harmcherry_h5_freq\n",
      "Feature 47: Importance = 27, M_harmcherry_h6_freq\n",
      "Feature 48: Importance = 13, M_harmcherry_h7_freq\n",
      "Feature 49: Importance = 14, M_harmcherry_h8_freq\n",
      "Feature 74: Importance = 26, L_harmcherry_h1_relbasefreq\n",
      "Feature 75: Importance = 29, L_harmcherry_h2_relbasefreq\n",
      "Feature 76: Importance = 27, L_harmcherry_h3_relbasefreq\n",
      "Feature 77: Importance = 21, L_harmcherry_h4_relbasefreq\n",
      "Feature 78: Importance = 42, L_harmcherry_h5_relbasefreq\n",
      "Feature 79: Importance = 32, L_harmcherry_h6_relbasefreq\n",
      "Feature 80: Importance = 33, L_harmcherry_h7_relbasefreq\n",
      "Feature 81: Importance = 48, L_harmcherry_h8_relbasefreq\n",
      "Feature 82: Importance = 23, R_harmcherry_h1_relbasefreq\n",
      "Feature 83: Importance = 36, R_harmcherry_h2_relbasefreq\n",
      "Feature 84: Importance = 26, R_harmcherry_h3_relbasefreq\n",
      "Feature 85: Importance = 37, R_harmcherry_h4_relbasefreq\n",
      "Feature 86: Importance = 30, R_harmcherry_h5_relbasefreq\n",
      "Feature 87: Importance = 26, R_harmcherry_h6_relbasefreq\n",
      "Feature 88: Importance = 35, R_harmcherry_h7_relbasefreq\n",
      "Feature 89: Importance = 36, R_harmcherry_h8_relbasefreq\n",
      "Feature 90: Importance = 20, M_harmcherry_h1_relbasefreq\n",
      "Feature 91: Importance = 39, M_harmcherry_h2_relbasefreq\n",
      "Feature 92: Importance = 35, M_harmcherry_h3_relbasefreq\n",
      "Feature 93: Importance = 25, M_harmcherry_h4_relbasefreq\n",
      "Feature 94: Importance = 27, M_harmcherry_h5_relbasefreq\n",
      "Feature 95: Importance = 37, M_harmcherry_h6_relbasefreq\n",
      "Feature 96: Importance = 34, M_harmcherry_h7_relbasefreq\n",
      "Feature 97: Importance = 20, M_harmcherry_h8_relbasefreq\n",
      "Feature 98: Importance = 31, L_harmcherry_h1_power\n",
      "Feature 99: Importance = 20, L_harmcherry_h2_power\n",
      "Feature 100: Importance = 38, L_harmcherry_h3_power\n",
      "Feature 101: Importance = 20, L_harmcherry_h4_power\n",
      "Feature 102: Importance = 29, L_harmcherry_h5_power\n",
      "Feature 103: Importance = 51, L_harmcherry_h6_power\n",
      "Feature 104: Importance = 35, L_harmcherry_h7_power\n",
      "Feature 105: Importance = 37, L_harmcherry_h8_power\n",
      "Feature 106: Importance = 43, R_harmcherry_h1_power\n",
      "Feature 107: Importance = 36, R_harmcherry_h2_power\n",
      "Feature 108: Importance = 55, R_harmcherry_h3_power\n",
      "Feature 109: Importance = 19, R_harmcherry_h4_power\n",
      "Feature 110: Importance = 21, R_harmcherry_h5_power\n",
      "Feature 111: Importance = 48, R_harmcherry_h6_power\n",
      "Feature 112: Importance = 37, R_harmcherry_h7_power\n",
      "Feature 113: Importance = 72, R_harmcherry_h8_power\n",
      "Feature 114: Importance = 22, M_harmcherry_h1_power\n",
      "Feature 115: Importance = 13, M_harmcherry_h2_power\n",
      "Feature 116: Importance = 34, M_harmcherry_h3_power\n",
      "Feature 117: Importance = 26, M_harmcherry_h4_power\n",
      "Feature 118: Importance = 16, M_harmcherry_h5_power\n",
      "Feature 119: Importance = 62, M_harmcherry_h6_power\n",
      "Feature 120: Importance = 25, M_harmcherry_h7_power\n",
      "Feature 121: Importance = 47, M_harmcherry_h8_power\n",
      "Feature 122: Importance = 42, L_harmcherry_h1_relpower\n",
      "Feature 124: Importance = 35, L_harmcherry_h3_relpower\n",
      "Feature 125: Importance = 17, L_harmcherry_h4_relpower\n",
      "Feature 126: Importance = 24, L_harmcherry_h5_relpower\n",
      "Feature 127: Importance = 40, L_harmcherry_h6_relpower\n",
      "Feature 128: Importance = 21, L_harmcherry_h7_relpower\n",
      "Feature 129: Importance = 36, L_harmcherry_h8_relpower\n",
      "Feature 130: Importance = 33, R_harmcherry_h1_relpower\n",
      "Feature 132: Importance = 28, R_harmcherry_h3_relpower\n",
      "Feature 133: Importance = 37, R_harmcherry_h4_relpower\n",
      "Feature 134: Importance = 39, R_harmcherry_h5_relpower\n",
      "Feature 135: Importance = 31, R_harmcherry_h6_relpower\n",
      "Feature 136: Importance = 38, R_harmcherry_h7_relpower\n",
      "Feature 137: Importance = 24, R_harmcherry_h8_relpower\n",
      "Feature 138: Importance = 16, M_harmcherry_h1_relpower\n",
      "Feature 140: Importance = 42, M_harmcherry_h3_relpower\n",
      "Feature 141: Importance = 25, M_harmcherry_h4_relpower\n",
      "Feature 142: Importance = 40, M_harmcherry_h5_relpower\n",
      "Feature 143: Importance = 39, M_harmcherry_h6_relpower\n",
      "Feature 144: Importance = 32, M_harmcherry_h7_relpower\n",
      "Feature 145: Importance = 38, M_harmcherry_h8_relpower\n",
      "Feature 152: Importance = 2, L_harmcherry_h7_invented\n",
      "Feature 153: Importance = 6, L_harmcherry_h8_invented\n",
      "Feature 156: Importance = 10, R_harmcherry_h3_invented\n",
      "Feature 172: Importance = 7, L_old_wbf_eh_approx\n",
      "Feature 173: Importance = 27, L_old_complexity\n",
      "Feature 174: Importance = 32, L_old_complexity_znorm\n",
      "Feature 175: Importance = 12, L_old_time_length\n",
      "Feature 176: Importance = 42, L_old_inharmonicity\n",
      "Feature 177: Importance = 1690, L_old_rho\n",
      "Feature 179: Importance = 8, M_old_wbf_eh_peak\n",
      "Feature 180: Importance = 20, M_old_wbf_eh_approx\n",
      "Feature 181: Importance = 73, M_old_complexity\n",
      "Feature 182: Importance = 90, M_old_complexity_znorm\n",
      "Feature 184: Importance = 34, M_old_inharmonicity\n",
      "Feature 187: Importance = 12, L_skips_score\n",
      "Feature 193: Importance = 21, R_skips_score\n",
      "Feature 197: Importance = 67, M_skips_score\n",
      "Feature 198: Importance = 31, L_old_peaks_freq_1\n",
      "Feature 199: Importance = 38, L_old_peaks_freq_2\n",
      "Feature 200: Importance = 26, L_old_peaks_freq_3\n",
      "Feature 201: Importance = 36, L_old_peaks_freq_4\n",
      "Feature 202: Importance = 40, L_old_peaks_freq_5\n",
      "Feature 203: Importance = 30, L_old_peaks_freq_6\n",
      "Feature 204: Importance = 27, R_old_peaks_freq_1\n",
      "Feature 205: Importance = 46, R_old_peaks_freq_2\n",
      "Feature 206: Importance = 18, R_old_peaks_freq_3\n",
      "Feature 207: Importance = 33, R_old_peaks_freq_4\n",
      "Feature 208: Importance = 27, R_old_peaks_freq_5\n",
      "Feature 209: Importance = 66, R_old_peaks_freq_6\n",
      "Feature 210: Importance = 23, M_old_peaks_freq_1\n",
      "Feature 211: Importance = 20, M_old_peaks_freq_2\n",
      "Feature 212: Importance = 34, M_old_peaks_freq_3\n",
      "Feature 213: Importance = 22, M_old_peaks_freq_4\n",
      "Feature 214: Importance = 46, M_old_peaks_freq_5\n",
      "Feature 215: Importance = 35, M_old_peaks_freq_6\n",
      "Feature 216: Importance = 36, L_old_eh_1\n",
      "Feature 217: Importance = 23, L_old_eh_2\n",
      "Feature 218: Importance = 11, L_old_eh_3\n",
      "Feature 219: Importance = 11, L_old_eh_4\n",
      "Feature 220: Importance = 12, L_old_eh_5\n",
      "Feature 221: Importance = 16, L_old_eh_6\n",
      "Feature 222: Importance = 6, L_old_eh_7\n",
      "Feature 223: Importance = 31, L_old_eh_8\n",
      "Feature 224: Importance = 12, L_old_eh_9\n",
      "Feature 225: Importance = 34, L_old_eh_10\n",
      "Feature 226: Importance = 7, L_old_eh_11\n",
      "Feature 227: Importance = 18, L_old_eh_12\n",
      "Feature 228: Importance = 28, L_old_eh_13\n",
      "Feature 229: Importance = 33, L_old_eh_14\n",
      "Feature 230: Importance = 13, L_old_eh_15\n",
      "Feature 231: Importance = 29, L_old_eh_16\n",
      "Feature 232: Importance = 26, L_old_eh_17\n",
      "Feature 233: Importance = 17, L_old_eh_18\n",
      "Feature 234: Importance = 16, L_old_eh_19\n",
      "Feature 235: Importance = 14, L_old_eh_20\n",
      "Feature 236: Importance = 27, L_old_eh_21\n",
      "Feature 237: Importance = 18, L_old_eh_22\n",
      "Feature 238: Importance = 13, L_old_eh_23\n",
      "Feature 239: Importance = 15, L_old_eh_24\n",
      "Feature 240: Importance = 23, L_old_eh_25\n",
      "Feature 241: Importance = 14, L_old_eh_26\n",
      "Feature 242: Importance = 30, R_old_eh_1\n",
      "Feature 243: Importance = 15, R_old_eh_2\n",
      "Feature 244: Importance = 13, R_old_eh_3\n",
      "Feature 245: Importance = 13, R_old_eh_4\n",
      "Feature 246: Importance = 10, R_old_eh_5\n",
      "Feature 247: Importance = 25, R_old_eh_6\n",
      "Feature 248: Importance = 9, R_old_eh_7\n",
      "Feature 249: Importance = 19, R_old_eh_8\n",
      "Feature 250: Importance = 9, R_old_eh_9\n",
      "Feature 251: Importance = 11, R_old_eh_10\n",
      "Feature 252: Importance = 8, R_old_eh_11\n",
      "Feature 253: Importance = 14, R_old_eh_12\n",
      "Feature 254: Importance = 23, R_old_eh_13\n",
      "Feature 255: Importance = 24, R_old_eh_14\n",
      "Feature 256: Importance = 14, R_old_eh_15\n",
      "Feature 257: Importance = 25, R_old_eh_16\n",
      "Feature 258: Importance = 16, R_old_eh_17\n",
      "Feature 259: Importance = 17, R_old_eh_18\n",
      "Feature 260: Importance = 13, R_old_eh_19\n",
      "Feature 261: Importance = 20, R_old_eh_20\n",
      "Feature 262: Importance = 33, R_old_eh_21\n",
      "Feature 263: Importance = 11, R_old_eh_22\n",
      "Feature 264: Importance = 7, R_old_eh_23\n",
      "Feature 265: Importance = 12, R_old_eh_24\n",
      "Feature 266: Importance = 14, R_old_eh_25\n",
      "Feature 267: Importance = 14, R_old_eh_26\n",
      "Feature 268: Importance = 17, M_old_eh_1\n",
      "Feature 269: Importance = 25, M_old_eh_2\n",
      "Feature 270: Importance = 12, M_old_eh_3\n",
      "Feature 271: Importance = 20, M_old_eh_4\n",
      "Feature 272: Importance = 21, M_old_eh_5\n",
      "Feature 273: Importance = 28, M_old_eh_6\n",
      "Feature 274: Importance = 9, M_old_eh_7\n",
      "Feature 275: Importance = 27, M_old_eh_8\n",
      "Feature 276: Importance = 18, M_old_eh_9\n",
      "Feature 277: Importance = 15, M_old_eh_10\n",
      "Feature 278: Importance = 19, M_old_eh_11\n",
      "Feature 279: Importance = 40, M_old_eh_12\n",
      "Feature 280: Importance = 23, M_old_eh_13\n",
      "Feature 281: Importance = 16, M_old_eh_14\n",
      "Feature 282: Importance = 24, M_old_eh_15\n",
      "Feature 283: Importance = 22, M_old_eh_16\n",
      "Feature 284: Importance = 26, M_old_eh_17\n",
      "Feature 285: Importance = 37, M_old_eh_18\n",
      "Feature 286: Importance = 31, M_old_eh_19\n",
      "Feature 287: Importance = 22, M_old_eh_20\n",
      "Feature 288: Importance = 33, M_old_eh_21\n",
      "Feature 289: Importance = 24, M_old_eh_22\n",
      "Feature 290: Importance = 11, M_old_eh_23\n",
      "Feature 291: Importance = 28, M_old_eh_24\n",
      "Feature 292: Importance = 33, M_old_eh_25\n",
      "Feature 293: Importance = 19, M_old_eh_26\n"
     ]
    }
   ],
   "source": [
    "special_features = ['temperature', 'duration', 'humidity', 'luminosity', 'altitude', 'air_pressure', 'hour']\n",
    "wbfL_features = ['L_harmcherry_wbf_mean', 'L_harmcherry_wbf_stddev', 'L_wbf_autocorrelation', 'L_wbf_cepstrum', 'L_harmcherry_wbf_coeffofvariation', 'L_harmcherry_wbf_trustworthyharmonics']\n",
    "wbfR_features = ['R_harmcherry_wbf_mean', 'R_harmcherry_wbf_stddev', 'R_wbf_autocorrelation', 'R_wbf_cepstrum', 'R_harmcherry_wbf_coeffofvariation', 'R_harmcherry_wbf_trustworthyharmonics']\n",
    "wbfM_features = ['M_harmcherry_wbf_mean', 'M_harmcherry_wbf_stddev', 'M_wbf_autocorrelation', 'M_wbf_cepstrum', 'M_harmcherry_wbf_coeffofvariation', 'M_harmcherry_wbf_trustworthyharmonics']\n",
    "freqL_features = [f'L_harmcherry_h{i}_freq' for i in range(1,9)]\n",
    "freqR_features = [f'R_harmcherry_h{i}_freq' for i in range(1,9)]\n",
    "freqM_features = [f'M_harmcherry_h{i}_freq' for i in range(1,9)]\n",
    "basefreqL_features = [f'L_harmcherry_h{i}_basefreq' for i in range(1,9)]\n",
    "basefreqR_features = [f'R_harmcherry_h{i}_basefreq' for i in range(1,9)]\n",
    "basefreqM_features = [f'M_harmcherry_h{i}_basefreq' for i in range(1,9)]\n",
    "relbasefreqL_features = [f'L_harmcherry_h{i}_relbasefreq' for i in range(1,9)]\n",
    "relbasefreqR_features = [f'R_harmcherry_h{i}_relbasefreq' for i in range(1,9)]\n",
    "relbasefreqM_features = [f'M_harmcherry_h{i}_relbasefreq' for i in range(1,9)]\n",
    "powerL_features = [f'L_harmcherry_h{i}_power' for i in range(1,9)]\n",
    "powerR_features = [f'R_harmcherry_h{i}_power' for i in range(1,9)]\n",
    "powerM_features = [f'M_harmcherry_h{i}_power' for i in range(1,9)]\n",
    "relpowerL_features = [f'L_harmcherry_h{i}_relpower' for i in range(1,9)]\n",
    "relpowerR_features = [f'R_harmcherry_h{i}_relpower' for i in range(1,9)]\n",
    "relpowerM_features = [f'M_harmcherry_h{i}_relpower' for i in range(1,9)]\n",
    "inventedL_features = [f'L_harmcherry_h{i}_invented' for i in range(1,9)]\n",
    "inventedR_features = [f'R_harmcherry_h{i}_invented' for i in range(1,9)]\n",
    "inventedM_features = [f'M_harmcherry_h{i}_invented' for i in range(1,9)]\n",
    "oldL_features = ['L_old_wbf_eh', 'L_old_wbf_eh_peak', 'L_old_wbf_eh_approx', 'L_old_complexity', 'L_old_complexity_znorm', 'L_old_time_length', 'L_old_inharmonicity', 'L_old_rho']\n",
    "oldR_features = ['R_old_wbf_eh', 'R_old_wbf_eh_peak', 'R_old_wbf_eh_approx', 'R_old_complexity', 'R_old_complexity_znorm', 'R_old_time_length', 'R_old_inharmonicity', 'R_old_rho']\n",
    "oldM_features = ['M_old_wbf_eh', 'M_old_wbf_eh_peak', 'M_old_wbf_eh_approx', 'M_old_complexity', 'M_old_complexity_znorm', 'M_old_time_length', 'M_old_inharmonicity', 'M_old_rho']\n",
    "skipsL_features = ['L_skips_density', 'L_skips_score', 'L_skips_thresh', 'L_skips_count']\n",
    "skipsR_features = ['R_skips_thresh', 'R_skips_count', 'R_skips_density', 'R_skips_score']\n",
    "skipsM_features = ['M_skips_thresh', 'M_skips_count', 'M_skips_density', 'M_skips_score']\n",
    "peakL_features = [f'L_old_peaks_freq_{i}' for i in range(1,7)]\n",
    "peakR_features = [f'R_old_peaks_freq_{i}' for i in range(1,7)]\n",
    "peakM_features = [f'M_old_peaks_freq_{i}' for i in range(1,7)]\n",
    "ehL_features = [f'L_old_eh_{i}' for i in range(1,27)]\n",
    "ehR_features = [f'R_old_eh_{i}' for i in range(1,27)]\n",
    "ehM_features = [f'M_old_eh_{i}' for i in range(1,27)]\n",
    "\n",
    "feature_set = special_features + wbfL_features + wbfR_features + wbfM_features + freqL_features + freqR_features + freqM_features + basefreqL_features + basefreqR_features + basefreqM_features + \\\n",
    "relbasefreqL_features + relbasefreqR_features + relbasefreqM_features + powerL_features + powerR_features + powerM_features + relpowerL_features + relpowerR_features + relpowerM_features + \\\n",
    "inventedL_features + inventedR_features + inventedM_features + oldL_features + oldM_features + skipsL_features + skipsR_features + skipsM_features + peakL_features + peakR_features + peakM_features + \\\n",
    "ehL_features + ehR_features + ehM_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "# class_weights = {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0}\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(class_weights='balanced')\n",
    "\n",
    "# 训练模型\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# 获取特征重要性\n",
    "feature_importance = lgb_model.feature_importances_\n",
    "\n",
    "new_features = []\n",
    "\n",
    "# 打印特征重要性\n",
    "for i, importance in enumerate(feature_importance):\n",
    "    if ((importance - 1) > 0):\n",
    "        print(f\"Feature {i+1}: Importance = {importance}, {feature_set[i]}\")\n",
    "        new_features.append(feature_set[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "5920877c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "12af691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 55022\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 229\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "number:  2062\n",
      "\tAcc: 0.5010\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.64      0.10      0.18       428\n",
      "     culex_female       0.42      0.95      0.58       522\n",
      "  funestus_female       0.61      0.85      0.71       512\n",
      "   gambiae_female       0.58      0.10      0.17       600\n",
      "\n",
      "         accuracy                           0.50      2062\n",
      "        macro avg       0.56      0.50      0.41      2062\n",
      "     weighted avg       0.56      0.50      0.41      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  44 │            301 │                69 │               14 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                   0 │            495 │                26 │                1 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                   7 │             42 │               434 │               29 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  18 │            339 │               183 │               60 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Train and test a LGBM model\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=new_features)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=new_features)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "    \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "p_labels = model.predict(X_test)\n",
    "a_labels = y_test\n",
    "acc = accuracy_score(a_labels, p_labels)\n",
    "print('number: ', len(a_labels))\n",
    "\n",
    "print(\"\\tAcc: %.4f\" % acc)\n",
    "print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "    \n",
    "cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "04fb87a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 912\n",
      "[LightGBM] [Info] Number of data points in the train set: 9600, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score -1.392145\n",
      "[LightGBM] [Info] Start training from score -1.385878\n",
      "[LightGBM] [Info] Start training from score -1.387962\n",
      "[LightGBM] [Info] Start training from score -1.379236\n",
      "number:  2400\n",
      "\tAcc: 0.9350\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.92      0.94      0.93       614\n",
      "     culex_female       0.91      0.92      0.91       599\n",
      "  funestus_female       0.94      0.92      0.93       604\n",
      "   gambiae_female       0.97      0.96      0.97       583\n",
      "\n",
      "         accuracy                           0.94      2400\n",
      "        macro avg       0.94      0.94      0.94      2400\n",
      "     weighted avg       0.94      0.94      0.94      2400\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 576 │             27 │                10 │                1 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  32 │            551 │                15 │                1 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  14 │             20 │               557 │               13 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                   2 │             10 │                11 │              560 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = pd.DataFrame(train_incubator, columns=['temperature', 'duration', 'humidity', 'luminosity', 'altitude', 'air_pressure', 'hour'])\n",
    "y = train_incubator['class'].values \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "    \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "p_labels = model.predict(X_test)\n",
    "a_labels = y_test\n",
    "acc = accuracy_score(a_labels, p_labels)\n",
    "print('number: ', len(a_labels))\n",
    "\n",
    "print(\"\\tAcc: %.4f\" % acc)\n",
    "print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "    \n",
    "cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680ea50",
   "metadata": {},
   "source": [
    "### Estimate the probability distribution for each class using different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "004a7cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabiensis_female' 'culex_female' 'funestus_female' 'gambiae_female']\n",
      "12000\n",
      "[0.25 0.25 0.25 0.25]\n"
     ]
    }
   ],
   "source": [
    "def class_dist(Y, nclasses):\n",
    "    return np.array([np.count_nonzero(Y == i) for i in range(nclasses)]) / Y.shape[0]\n",
    "\n",
    "Y = train_incubator['class'].values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y)\n",
    "\n",
    "classes = class_dist(Y_encoded, nclasses)\n",
    "\n",
    "print(np.unique(Y))\n",
    "print(Y.shape[0])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79fc7a",
   "metadata": {},
   "source": [
    "Use getScores to get the predicted score of train and test\n",
    "\n",
    "Perform group cross-validation on the data through the group_kfold.split() method, where group_kfold is a defined group cross-validation object. In each iteration of cross-validation, use the fit() method to fit the model, and use the predict_proba() method to obtain the probability score of each sample belonging to each category, and then fill these scores into the corresponding positions of the train_scores array.\n",
    "\n",
    "Finally, the model is refitted on the entire training set and the predict_proba() method is used to obtain the predicted probability score for the test set and stored in the test_scores array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30e3bb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9572\n",
      "[LightGBM] [Info] Number of data points in the train set: 9395, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.379823\n",
      "[LightGBM] [Info] Start training from score -1.321889\n",
      "[LightGBM] [Info] Start training from score -1.412937\n",
      "[LightGBM] [Info] Start training from score -1.434148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9570\n",
      "[LightGBM] [Info] Number of data points in the train set: 9638, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.430199\n",
      "[LightGBM] [Info] Start training from score -1.406205\n",
      "[LightGBM] [Info] Start training from score -1.335126\n",
      "[LightGBM] [Info] Start training from score -1.376178\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9577\n",
      "[LightGBM] [Info] Number of data points in the train set: 9961, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.429059\n",
      "[LightGBM] [Info] Start training from score -1.371645\n",
      "[LightGBM] [Info] Start training from score -1.410375\n",
      "[LightGBM] [Info] Start training from score -1.336649\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9592\n",
      "[LightGBM] [Info] Number of data points in the train set: 10062, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.428724\n",
      "[LightGBM] [Info] Start training from score -1.370714\n",
      "[LightGBM] [Info] Start training from score -1.445454\n",
      "[LightGBM] [Info] Start training from score -1.306298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9551\n",
      "[LightGBM] [Info] Number of data points in the train set: 8944, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.261579\n",
      "[LightGBM] [Info] Start training from score -1.472655\n",
      "[LightGBM] [Info] Start training from score -1.325985\n",
      "[LightGBM] [Info] Start training from score -1.505364\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9591\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n"
     ]
    }
   ],
   "source": [
    "best_params = {'learning_rate': 0.13, \n",
    "                'max_depth': 7, \n",
    "                'n_estimators': 200,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'num_leaves': 5,\n",
    "                'reg_alpha': 0.01,\n",
    "                'reg_lambda': 0.01,\n",
    "                'subsample': 0.88}\n",
    "\n",
    "def getScores(X_train, X_test, Y_train, nclasses):\n",
    "\n",
    "    # model = lgb.LGBMClassifier(**best_params)\n",
    "    model = lgb.LGBMClassifier()\n",
    "   \n",
    "    train_scores = np.zeros((len(X_train), nclasses))\n",
    "    test_scores = np.zeros((len(X_test), nclasses))\n",
    "\n",
    "    groups = train_incubator['sensor'].values\n",
    "    group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "    for train_index, test_index in group_kfold.split(X_train, Y_train, groups):\n",
    "        model.fit(X_train[train_index], Y_train[train_index])\n",
    "        train_scores[test_index] = model.predict_proba(X_train)[test_index]\n",
    "    \n",
    "    model.fit(X_train, Y_train)\n",
    "    test_scores = model.predict_proba(X_test)\n",
    "           \n",
    "    return train_scores, test_scores\n",
    "\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).values\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).values\n",
    "y_train = train_incubator['class'].values\n",
    "groups = train_incubator['sensor'].values\n",
    "\n",
    "train_scores, test_scores = getScores(X_train, X_test, y_train, nclasses)\n",
    "# print('train_scores')\n",
    "# print(train_scores)\n",
    "# print('test_scores')\n",
    "# print(test_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908b541e",
   "metadata": {},
   "source": [
    "EMQ function\n",
    "\n",
    "EM algorithm to estimate the class distribution of the test set. The EMQ function assumes that the class distribution of the test set is unknown, but can be estimated by the class conditional probabilities on the test set. It iteratively adjusts the class distribution so that, under given model parameters, the class conditional probability of the test set best matches the actual observed test set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c8138ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "\n",
    "\n",
    "def EMQ(test_scores, train_labels, nclasses):\n",
    "    max_it = 1000        # Max num of iterations\n",
    "    eps = 1e-6           # Small constant for stopping criterium\n",
    "\n",
    "    p_tr = class_dist(train_labels, nclasses)\n",
    "    p_s = np.copy(p_tr)\n",
    "    p_cond_tr = np.array(test_scores)\n",
    "    p_cond_s = np.zeros(p_cond_tr.shape)\n",
    "    prob_arrays = []\n",
    "\n",
    "    for _ in range(max_it):\n",
    "        # Add Laplacian smoothing\n",
    "        # r = (p_s + alpha) / (p_tr + (alpha * nclasses))\n",
    "        r = p_s / p_tr\n",
    "        \n",
    "        p_cond_s = p_cond_tr * r\n",
    "        s = np.sum(p_cond_s, axis = 1)\n",
    "        for c in range(nclasses):\n",
    "            p_cond_s[:,c] = p_cond_s[:,c] / s\n",
    "\n",
    "        prob_arrays.append(p_cond_s)\n",
    "        p_s_old = np.copy(p_s)\n",
    "        p_s = np.sum(p_cond_s, axis = 0) / p_cond_s.shape[0]\n",
    "        if (np.sum(np.abs(p_s - p_s_old)) < eps):\n",
    "            break\n",
    "\n",
    "    return(p_s/np.sum(p_s))\n",
    "    # return p_cond_s\n",
    "\n",
    "Y = train_incubator['class'].values\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(Y)\n",
    "EMQ_result = EMQ(test_scores, train_labels, nclasses)\n",
    "# print(EMQ_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6beaa",
   "metadata": {},
   "source": [
    "### Test model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0e17288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  LGBM\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9591\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\tAcc: 0.5131\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.40      0.21      0.27       428\n",
      "     culex_female       0.45      0.79      0.57       522\n",
      "  funestus_female       0.62      0.87      0.72       512\n",
      "   gambiae_female       0.56      0.19      0.29       600\n",
      "\n",
      "         accuracy                           0.51      2062\n",
      "        macro avg       0.51      0.51      0.46      2062\n",
      "     weighted avg       0.51      0.51      0.46      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  90 │            206 │                63 │               69 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  87 │            410 │                21 │                4 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                   2 │             49 │               443 │               18 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  48 │            251 │               186 │              115 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Train and test a LGBM model\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "best_params = {'learning_rate': 0.14, \n",
    "                'max_depth': 9, \n",
    "                'n_estimators': 212,\n",
    "                'colsample_bytree': 0.82,\n",
    "                'num_leaves': 49,\n",
    "                'reg_alpha': 0.077,\n",
    "                'reg_lambda': 0.7,\n",
    "                'subsample': 0.93}\n",
    "\n",
    "models = [('LGBM', lgb.LGBMClassifier(**best_params))]\n",
    "\n",
    "for name, model in models:\n",
    "    print(\"Model: \", name)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    p_labels = model.predict(X_test)\n",
    "    a_labels = y_test\n",
    "    acc = accuracy_score(a_labels, p_labels)\n",
    "    \n",
    "    print(\"\\tAcc: %.4f\" % acc)\n",
    "    print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "        \n",
    "    cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "    print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2649b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  LGBM\n",
      "test_ 2062\n",
      "EMQ [[1.99353250e-02 1.46674435e-01 8.33390241e-01 1.47534652e-17]\n",
      " [1.95943795e-01 5.96583048e-01 2.07473157e-01 1.13075602e-16]\n",
      " [5.53557446e-03 1.83026675e-01 8.11437750e-01 2.62948143e-15]\n",
      " ...\n",
      " [2.98539471e-04 5.76050478e-03 9.93940956e-01 6.38476092e-17]\n",
      " [4.29737974e-03 7.04579196e-02 9.25244701e-01 1.91612037e-15]\n",
      " [4.89837457e-04 6.64269685e-03 9.92867466e-01 2.41864730e-17]]\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001771 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9591\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: [1024 1441 6394]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m p_labels:\n\u001b[0;32m     36\u001b[0m     p_pred\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39margmax(item \u001b[38;5;241m*\u001b[39m EMQ_result))\n\u001b[1;32m---> 37\u001b[0m p_pred \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m a_labels \u001b[38;5;241m=\u001b[39m y_test\n\u001b[0;32m     40\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(a_labels, p_pred)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\comp9991\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:160\u001b[0m, in \u001b[0;36mLabelEncoder.inverse_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    158\u001b[0m diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msetdiff1d(y, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)))\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(diff):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(diff))\n\u001b[0;32m    161\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(y)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[y]\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: [1024 1441 6394]"
     ]
    }
   ],
   "source": [
    "# Train and test a LGBM model\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "best_params = {'learning_rate': 0.14, \n",
    "                'max_depth': 9, \n",
    "                'n_estimators': 212,\n",
    "                'colsample_bytree': 0.82,\n",
    "                'num_leaves': 49,\n",
    "                'reg_alpha': 0.077,\n",
    "                'reg_lambda': 0.7,\n",
    "                'subsample': 0.93}\n",
    "\n",
    "model = lgb.LGBMClassifier(**best_params)\n",
    "# model = lgb.LGBMClassifier()\n",
    "\n",
    "# for name, model in models:\n",
    "print(\"Model: \", name)\n",
    "\n",
    "Y = train_incubator['class'].values\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(Y)\n",
    "EMQ_result = EMQ(test_scores, nclasses)\n",
    "print('EMQ', EMQ_result)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "p_labels = model.predict_proba(X_test)\n",
    "p_pred = []\n",
    "for item in p_labels:\n",
    "    p_pred.append(np.argmax(item * EMQ_result))\n",
    "p_pred = label_encoder.inverse_transform(p_pred)\n",
    "\n",
    "a_labels = y_test\n",
    "acc = accuracy_score(a_labels, p_pred)\n",
    "\n",
    "print(\"\\tAcc: %.4f\" % acc)\n",
    "# print (classification_report(a_labels, p_pred, labels=np.unique(y_test)))\n",
    "      \n",
    "# cf = confusion_matrix(a_labels, p_pred, labels=np.unique(y_train))\n",
    "# print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cf250209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_ 2062\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3056, 2062]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     p_pred\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39margmax(item))\n\u001b[0;32m     43\u001b[0m p_pred \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39minverse_transform(p_pred)\n\u001b[1;32m---> 45\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\comp9991\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\comp9991\\lib\\site-packages\\sklearn\\metrics\\_classification.py:213\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\comp9991\\lib\\site-packages\\sklearn\\metrics\\_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\comp9991\\lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3056, 2062]"
     ]
    }
   ],
   "source": [
    "alpha = 1\n",
    "\n",
    "def EMQ(test_scores, nclasses):\n",
    "    max_it = 1000        # Max num of iterations\n",
    "    eps = 1e-6           # Small constant for stopping criterium\n",
    "\n",
    "    # p_tr = class_dist(train_labels, nclasses)\n",
    "    p_tr = [0.25, 0.25, 0.25, 0.25]\n",
    "    p_s = np.copy(p_tr)\n",
    "    p_cond_tr = np.array(test_scores)\n",
    "    p_cond_s = np.zeros(p_cond_tr.shape)\n",
    "    prob_arrays = []\n",
    "    print('test_', len(p_cond_s))\n",
    "\n",
    "    for _ in range(max_it):\n",
    "        # Add Laplacian smoothing\n",
    "        # r = (p_s + alpha) / (p_tr + (alpha * nclasses))\n",
    "        r = p_s / p_tr\n",
    "        \n",
    "        p_cond_s = p_cond_tr * r\n",
    "        s = np.sum(p_cond_s, axis = 1)\n",
    "        for c in range(nclasses):\n",
    "            p_cond_s[:,c] = p_cond_s[:,c] / s\n",
    "\n",
    "        prob_arrays.append(p_cond_s)\n",
    "        p_s_old = np.copy(p_s)\n",
    "        p_s = np.sum(p_cond_s, axis = 0) / p_cond_s.shape[0]\n",
    "        if (np.sum(np.abs(p_s - p_s_old)) < eps):\n",
    "            break\n",
    "    # print(len(p_cond_s))\n",
    "\n",
    "    # return(p_s/np.sum(p_s))\n",
    "    return p_cond_s\n",
    "\n",
    "Y = train_incubator['class'].values\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(Y)\n",
    "EMQ_result = EMQ(test_scores, nclasses)\n",
    "# print(EMQ_result)\n",
    "p_pred = []\n",
    "for item in EMQ_result:\n",
    "    p_pred.append(np.argmax(item))\n",
    "p_pred = label_encoder.inverse_transform(p_pred)\n",
    "\n",
    "acc = accuracy_score(y_test, p_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5c744f",
   "metadata": {},
   "source": [
    "尝试交叉验证的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1b584453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9572\n",
      "[LightGBM] [Info] Number of data points in the train set: 9395, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.379823\n",
      "[LightGBM] [Info] Start training from score -1.321889\n",
      "[LightGBM] [Info] Start training from score -1.412937\n",
      "[LightGBM] [Info] Start training from score -1.434148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9570\n",
      "[LightGBM] [Info] Number of data points in the train set: 9638, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.430199\n",
      "[LightGBM] [Info] Start training from score -1.406205\n",
      "[LightGBM] [Info] Start training from score -1.335126\n",
      "[LightGBM] [Info] Start training from score -1.376178\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9577\n",
      "[LightGBM] [Info] Number of data points in the train set: 9961, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.429059\n",
      "[LightGBM] [Info] Start training from score -1.371645\n",
      "[LightGBM] [Info] Start training from score -1.410375\n",
      "[LightGBM] [Info] Start training from score -1.336649\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9592\n",
      "[LightGBM] [Info] Number of data points in the train set: 10062, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.428724\n",
      "[LightGBM] [Info] Start training from score -1.370714\n",
      "[LightGBM] [Info] Start training from score -1.445454\n",
      "[LightGBM] [Info] Start training from score -1.306298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9551\n",
      "[LightGBM] [Info] Number of data points in the train set: 8944, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.261579\n",
      "[LightGBM] [Info] Start training from score -1.472655\n",
      "[LightGBM] [Info] Start training from score -1.325985\n",
      "[LightGBM] [Info] Start training from score -1.505364\n",
      "test_ 3056\n",
      "acc [0.8268979057591623]\n",
      "acc_emq [0.8285340314136126]\n",
      "acc_avg 0.16537958115183246\n",
      "acc_emq_avg 0.16570680628272252\n"
     ]
    }
   ],
   "source": [
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y = train_incubator['class'].values\n",
    "groups = train_incubator['sensor'].values\n",
    "# Select evaluation indicators\n",
    "scoring = 'accuracy'\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "emq_accuracies = []\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in group_kfold.split(X, y, groups=groups):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    emq_result = EMQ(y_proba, 4)\n",
    "\n",
    "    p_pred = []\n",
    "    cont = 0\n",
    "    for item in emq_result:\n",
    "        cont += 1\n",
    "        p_pred.append(np.argmax(item))\n",
    "\n",
    "    p_pred = label_encoder.inverse_transform(p_pred)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    emq_accuracy = accuracy_score(y_test, p_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    emq_accuracies.append(emq_accuracy)\n",
    "    accuracies.append(accuracy)\n",
    "print('acc', accuracies)\n",
    "print('acc_emq', emq_accuracies)\n",
    "print('acc_avg', np.sum(accuracies) / 5)\n",
    "print('acc_emq_avg', np.sum(accuracies) / 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp9991",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
