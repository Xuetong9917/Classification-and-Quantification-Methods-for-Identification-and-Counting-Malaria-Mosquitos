{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b847f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tabulate import tabulate\n",
    "from hyperopt import hp, fmin, tpe, Trials, space_eval\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cvx\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "import quadprog\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d24b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "arabiensis_female    3000\n",
      "culex_female         3000\n",
      "funestus_female      3000\n",
      "gambiae_female       3000\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "gambiae_female       600\n",
      "culex_female         522\n",
      "funestus_female      512\n",
      "arabiensis_female    428\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_incubator = pd.read_csv('train_incubator.csv')\n",
    "test_sf2 = pd.read_csv('test_sf2.csv')\n",
    "\n",
    "# Check number of examples per class\n",
    "print (train_incubator['class'].value_counts())\n",
    "print (test_sf2['class'].value_counts())\n",
    "\n",
    "nclasses = len(train_incubator['class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95be81b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets\n",
    "special_features = ['temperature', 'duration', 'humidity']\n",
    "wbf_features = ['L_harmcherry_wbf_mean','L_harmcherry_wbf_stddev']\n",
    "freq_features = [f'L_harmcherry_h{i}_freq' for i in range(1,9)]\n",
    "basefreq_features = [f'L_harmcherry_h{i}_basefreq' for i in range(1,9)]\n",
    "relbasefreq_features = [f'L_harmcherry_h{i}_relbasefreq' for i in range(1,9)]\n",
    "power_features = [f'L_harmcherry_h{i}_power' for i in range(1,9)]\n",
    "relpower_features = [f'L_harmcherry_h{i}_relpower' for i in range(1,9)]\n",
    "invented_features = [f'L_harmcherry_h{i}_invented' for i in range(1,9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df6d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  LGBM\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "number:  2062\n",
      "\tAcc: 0.4840\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.23      0.22      0.23       428\n",
      "     culex_female       0.52      0.54      0.53       522\n",
      "  funestus_female       0.93      0.46      0.62       512\n",
      "   gambiae_female       0.45      0.64      0.53       600\n",
      "\n",
      "         accuracy                           0.48      2062\n",
      "        macro avg       0.53      0.47      0.48      2062\n",
      "     weighted avg       0.54      0.48      0.49      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  96 │            128 │                 3 │              201 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 155 │            281 │                 5 │               81 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  66 │             19 │               238 │              189 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  92 │            116 │                 9 │              383 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Train and test a LGBM model\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "models = [('LGBM', lgb.LGBMClassifier())]\n",
    "\n",
    "for name, model in models:\n",
    "    print(\"Model: \", name)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    p_labels = model.predict(X_test)\n",
    "    a_labels = y_test\n",
    "    acc = accuracy_score(a_labels, p_labels)\n",
    "    print('number: ', len(a_labels))\n",
    "    \n",
    "    print(\"\\tAcc: %.4f\" % acc)\n",
    "    print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "        \n",
    "    cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "    print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e0bc3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: class_weights\n",
      "[LightGBM] [Warning] Unknown parameter: class_weights\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "Feature 1: Importance = 722, temperature\n",
      "Feature 2: Importance = 364, duration\n",
      "Feature 3: Importance = 1866, humidity\n",
      "Feature 4: Importance = 173, L_harmcherry_wbf_mean\n",
      "Feature 5: Importance = 412, L_harmcherry_wbf_stddev\n",
      "Feature 6: Importance = 295, L_harmcherry_h1_freq\n",
      "Feature 7: Importance = 393, L_harmcherry_h2_freq\n",
      "Feature 8: Importance = 230, L_harmcherry_h3_freq\n",
      "Feature 9: Importance = 242, L_harmcherry_h4_freq\n",
      "Feature 10: Importance = 185, L_harmcherry_h5_freq\n",
      "Feature 11: Importance = 209, L_harmcherry_h6_freq\n",
      "Feature 12: Importance = 168, L_harmcherry_h7_freq\n",
      "Feature 13: Importance = 235, L_harmcherry_h8_freq\n",
      "Feature 22: Importance = 358, L_harmcherry_h1_relbasefreq\n",
      "Feature 23: Importance = 448, L_harmcherry_h2_relbasefreq\n",
      "Feature 24: Importance = 403, L_harmcherry_h3_relbasefreq\n",
      "Feature 25: Importance = 404, L_harmcherry_h4_relbasefreq\n",
      "Feature 26: Importance = 416, L_harmcherry_h5_relbasefreq\n",
      "Feature 27: Importance = 320, L_harmcherry_h6_relbasefreq\n",
      "Feature 28: Importance = 347, L_harmcherry_h7_relbasefreq\n",
      "Feature 29: Importance = 423, L_harmcherry_h8_relbasefreq\n",
      "Feature 30: Importance = 486, L_harmcherry_h1_power\n",
      "Feature 31: Importance = 438, L_harmcherry_h2_power\n",
      "Feature 32: Importance = 425, L_harmcherry_h3_power\n",
      "Feature 33: Importance = 408, L_harmcherry_h4_power\n",
      "Feature 34: Importance = 389, L_harmcherry_h5_power\n",
      "Feature 35: Importance = 394, L_harmcherry_h6_power\n",
      "Feature 36: Importance = 396, L_harmcherry_h7_power\n",
      "Feature 37: Importance = 451, L_harmcherry_h8_power\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "# class_weights = {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0}\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(class_weights='balanced')\n",
    "\n",
    "# 训练模型\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# 获取特征重要性\n",
    "feature_importance = lgb_model.feature_importances_\n",
    "\n",
    "new_features = []\n",
    "\n",
    "# 打印特征重要性\n",
    "for i, importance in enumerate(feature_importance):\n",
    "    if ((importance - 1) > 0):\n",
    "        print(f\"Feature {i+1}: Importance = {importance}, {feature_set[i]}\")\n",
    "        new_features.append(feature_set[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cedea12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7e39f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6946\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "number:  2062\n",
      "\tAcc: 0.4840\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.23      0.22      0.23       428\n",
      "     culex_female       0.52      0.54      0.53       522\n",
      "  funestus_female       0.93      0.46      0.62       512\n",
      "   gambiae_female       0.45      0.64      0.53       600\n",
      "\n",
      "         accuracy                           0.48      2062\n",
      "        macro avg       0.53      0.47      0.48      2062\n",
      "     weighted avg       0.54      0.48      0.49      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  96 │            128 │                 3 │              201 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 155 │            281 │                 5 │               81 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  66 │             19 │               238 │              189 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  92 │            116 │                 9 │              383 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Train and test a LGBM model\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=new_features)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=new_features)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "    \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "p_labels = model.predict(X_test)\n",
    "a_labels = y_test\n",
    "acc = accuracy_score(a_labels, p_labels)\n",
    "print('number: ', len(a_labels))\n",
    "\n",
    "print(\"\\tAcc: %.4f\" % acc)\n",
    "print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "    \n",
    "cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b8f2da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000821 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6941\n",
      "[LightGBM] [Info] Number of data points in the train set: 9395, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score -1.379823\n",
      "[LightGBM] [Info] Start training from score -1.321889\n",
      "[LightGBM] [Info] Start training from score -1.412937\n",
      "[LightGBM] [Info] Start training from score -1.434148\n",
      "number:  2605\n",
      "\tAcc: 0.5893\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.44      0.42      0.43       636\n",
      "     culex_female       0.56      0.77      0.65       495\n",
      "  funestus_female       0.72      0.80      0.76       713\n",
      "   gambiae_female       0.58      0.42      0.49       761\n",
      "\n",
      "         accuracy                           0.59      2605\n",
      "        macro avg       0.58      0.60      0.58      2605\n",
      "     weighted avg       0.58      0.59      0.58      2605\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 264 │            131 │               111 │              130 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  67 │            379 │                10 │               39 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  50 │             31 │               573 │               59 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 214 │            130 │                98 │              319 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6932\n",
      "[LightGBM] [Info] Number of data points in the train set: 9638, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score -1.430199\n",
      "[LightGBM] [Info] Start training from score -1.406205\n",
      "[LightGBM] [Info] Start training from score -1.335126\n",
      "[LightGBM] [Info] Start training from score -1.376178\n",
      "number:  2362\n",
      "\tAcc: 0.5453\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.04      0.01      0.01       694\n",
      "     culex_female       0.80      0.75      0.77       638\n",
      "  funestus_female       0.85      0.82      0.83       464\n",
      "   gambiae_female       0.36      0.76      0.48       566\n",
      "\n",
      "         accuracy                           0.55      2362\n",
      "        macro avg       0.51      0.58      0.53      2362\n",
      "     weighted avg       0.48      0.55      0.49      2362\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                   5 │             57 │                27 │              605 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  19 │            476 │                14 │              129 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  34 │             11 │               379 │               40 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  59 │             52 │                27 │              428 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6945\n",
      "[LightGBM] [Info] Number of data points in the train set: 9961, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score -1.429059\n",
      "[LightGBM] [Info] Start training from score -1.371645\n",
      "[LightGBM] [Info] Start training from score -1.410375\n",
      "[LightGBM] [Info] Start training from score -1.336649\n",
      "number:  2039\n",
      "\tAcc: 0.6057\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.56      0.45      0.50       614\n",
      "     culex_female       0.67      0.55      0.60       473\n",
      "  funestus_female       0.76      0.83      0.79       569\n",
      "   gambiae_female       0.43      0.59      0.49       383\n",
      "\n",
      "         accuracy                           0.61      2039\n",
      "        macro avg       0.60      0.61      0.60      2039\n",
      "     weighted avg       0.61      0.61      0.60      2039\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 278 │             79 │                66 │              191 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 123 │            260 │                18 │               72 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  31 │             25 │               471 │               42 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  68 │             23 │                66 │              226 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000742 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6949\n",
      "[LightGBM] [Info] Number of data points in the train set: 10062, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score -1.428724\n",
      "[LightGBM] [Info] Start training from score -1.370714\n",
      "[LightGBM] [Info] Start training from score -1.445454\n",
      "[LightGBM] [Info] Start training from score -1.306298\n",
      "number:  1938\n",
      "\tAcc: 0.6259\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.67      0.29      0.41       589\n",
      "     culex_female       0.55      0.83      0.66       445\n",
      "  funestus_female       0.85      0.81      0.83       629\n",
      "   gambiae_female       0.40      0.60      0.48       275\n",
      "\n",
      "         accuracy                           0.63      1938\n",
      "        macro avg       0.62      0.63      0.59      1938\n",
      "     weighted avg       0.66      0.63      0.61      1938\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 172 │            229 │                50 │              138 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                   8 │            369 │                 1 │               67 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  35 │             40 │               507 │               47 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  41 │             34 │                35 │              165 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6937\n",
      "[LightGBM] [Info] Number of data points in the train set: 8944, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score -1.261579\n",
      "[LightGBM] [Info] Start training from score -1.472655\n",
      "[LightGBM] [Info] Start training from score -1.325985\n",
      "[LightGBM] [Info] Start training from score -1.505364\n",
      "number:  3056\n",
      "\tAcc: 0.5370\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.24      0.54      0.33       467\n",
      "     culex_female       0.69      0.50      0.58       949\n",
      "  funestus_female       0.70      0.68      0.69       625\n",
      "   gambiae_female       0.70      0.48      0.57      1015\n",
      "\n",
      "         accuracy                           0.54      3056\n",
      "        macro avg       0.58      0.55      0.54      3056\n",
      "     weighted avg       0.63      0.54      0.56      3056\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                 253 │             69 │                65 │               80 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 356 │            477 │                38 │               78 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 123 │             27 │               427 │               48 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 334 │            114 │                83 │              484 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(train_incubator, columns=new_features).to_numpy()\n",
    "y = train_incubator['class'].values \n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "groups = train_incubator['sensor'].values\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "for train_index, test_index in group_kfold.split(X, y, groups):\n",
    "  X_train, y_train, X_test, y_test = X[train_index], y[train_index], X[test_index], y[test_index]\n",
    "  model.fit(X[train_index], y[train_index])\n",
    "\n",
    "  p_labels = model.predict(X_test)\n",
    "  a_labels = y_test\n",
    "  acc = accuracy_score(a_labels, p_labels)\n",
    "  print('number: ', len(a_labels))\n",
    "\n",
    "  print(\"\\tAcc: %.4f\" % acc)\n",
    "  print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "      \n",
    "  cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "  print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fcbabbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8981\n",
      "[LightGBM] [Info] Number of data points in the train set: 9395, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.379823\n",
      "[LightGBM] [Info] Start training from score -1.321889\n",
      "[LightGBM] [Info] Start training from score -1.412937\n",
      "[LightGBM] [Info] Start training from score -1.434148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8972\n",
      "[LightGBM] [Info] Number of data points in the train set: 9638, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.430199\n",
      "[LightGBM] [Info] Start training from score -1.406205\n",
      "[LightGBM] [Info] Start training from score -1.335126\n",
      "[LightGBM] [Info] Start training from score -1.376178\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001082 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8985\n",
      "[LightGBM] [Info] Number of data points in the train set: 9961, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.429059\n",
      "[LightGBM] [Info] Start training from score -1.371645\n",
      "[LightGBM] [Info] Start training from score -1.410375\n",
      "[LightGBM] [Info] Start training from score -1.336649\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8989\n",
      "[LightGBM] [Info] Number of data points in the train set: 10062, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.428724\n",
      "[LightGBM] [Info] Start training from score -1.370714\n",
      "[LightGBM] [Info] Start training from score -1.445454\n",
      "[LightGBM] [Info] Start training from score -1.306298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8977\n",
      "[LightGBM] [Info] Number of data points in the train set: 8944, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.261579\n",
      "[LightGBM] [Info] Start training from score -1.472655\n",
      "[LightGBM] [Info] Start training from score -1.325985\n",
      "[LightGBM] [Info] Start training from score -1.505364\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001404 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[0.22315699 0.22405571 0.05762986 0.49515744]\n"
     ]
    }
   ],
   "source": [
    "def getScores(X_train, X_test, Y_train, nclasses):\n",
    "\n",
    "    # model = lgb.LGBMClassifier(**best_params)\n",
    "    model = lgb.LGBMClassifier()\n",
    "   \n",
    "    train_scores = np.zeros((len(X_train), nclasses))\n",
    "    test_scores = np.zeros((len(X_test), nclasses))\n",
    "\n",
    "    groups = train_incubator['sensor'].values\n",
    "    group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "    for train_index, test_index in group_kfold.split(X_train, Y_train, groups):\n",
    "        model.fit(X_train[train_index], Y_train[train_index])\n",
    "        train_scores[test_index] = model.predict_proba(X_train)[test_index]\n",
    "    \n",
    "    model.fit(X_train, Y_train)\n",
    "    test_scores = model.predict_proba(X_test)\n",
    "           \n",
    "    return train_scores, test_scores\n",
    "\n",
    "def EMQ(test_scores, nclasses):\n",
    "    max_it = 1000        # Max num of iterations\n",
    "    eps = 1e-1           # Small constant for stopping criterium\n",
    "\n",
    "    p_tr = [0.25, 0.25, 0.25, 0.25]\n",
    "    p_s = np.copy(p_tr)\n",
    "    p_cond_tr = np.array(test_scores)\n",
    "    p_cond_s = np.zeros(p_cond_tr.shape)\n",
    "    prob_arrays = []\n",
    "\n",
    "    for _ in range(max_it):\n",
    "        # Add Laplacian smoothing\n",
    "        # r = (p_s + alpha) / (p_tr + (alpha * nclasses))\n",
    "        r = p_s / p_tr\n",
    "        \n",
    "        p_cond_s = p_cond_tr * r\n",
    "        s = np.sum(p_cond_s, axis = 1)\n",
    "        for c in range(nclasses):\n",
    "            p_cond_s[:,c] = p_cond_s[:,c] / s\n",
    "\n",
    "        prob_arrays.append(p_cond_s)\n",
    "        p_s_old = np.copy(p_s)\n",
    "        p_s = np.sum(p_cond_s, axis = 0) / p_cond_s.shape[0]\n",
    "        if (np.sum(np.abs(p_s - p_s_old)) < eps):\n",
    "            break\n",
    "\n",
    "    return(p_s/np.sum(p_s))\n",
    "    # return p_cond_s\n",
    "\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).to_numpy()\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).to_numpy()\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "train_scores, test_scores = getScores(X_train, X_test, y_train, 4)\n",
    "res = EMQ(test_scores, 4)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8931433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8986\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.14      0.12      0.13       428\n",
      "     culex_female       0.54      0.48      0.51       522\n",
      "  funestus_female       0.92      0.20      0.32       512\n",
      "   gambiae_female       0.40      0.74      0.52       600\n",
      "\n",
      "         accuracy                           0.41      2062\n",
      "        macro avg       0.50      0.39      0.37      2062\n",
      "     weighted avg       0.51      0.41      0.39      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  52 │            104 │                 1 │              271 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 122 │            251 │                 4 │              145 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                 136 │             17 │               101 │              258 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  52 │             97 │                 4 │              447 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "def getScores(X_train, X_test, Y_train, nclasses):\n",
    "\n",
    "    # model = lgb.LGBMClassifier(**best_params)\n",
    "    model = lgb.LGBMClassifier()\n",
    "   \n",
    "    train_scores = np.zeros((len(X_train), nclasses))\n",
    "    test_scores = np.zeros((len(X_test), nclasses))\n",
    "\n",
    "    groups = train_incubator['sensor'].values\n",
    "    group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "    for train_index, test_index in group_kfold.split(X_train, Y_train, groups):\n",
    "        model.fit(X_train[train_index], Y_train[train_index])\n",
    "        train_scores[test_index] = model.predict_proba(X_train)[test_index]\n",
    "    \n",
    "    model.fit(X_train, Y_train)\n",
    "    test_scores = model.predict_proba(X_test)\n",
    "           \n",
    "    return train_scores, test_scores\n",
    "\n",
    "def EMQ(test_scores, nclasses):\n",
    "    max_it = 1000        # Max num of iterations\n",
    "    eps = 1e-1           # Small constant for stopping criterium\n",
    "\n",
    "    p_tr = [0.25, 0.25, 0.25, 0.25]\n",
    "    p_s = np.copy(p_tr)\n",
    "    p_cond_tr = np.array(test_scores)\n",
    "    p_cond_s = np.zeros(p_cond_tr.shape)\n",
    "    prob_arrays = []\n",
    "\n",
    "    for _ in range(max_it):\n",
    "        # Add Laplacian smoothing\n",
    "        # r = (p_s + alpha) / (p_tr + (alpha * nclasses))\n",
    "        r = p_s / p_tr\n",
    "        \n",
    "        p_cond_s = p_cond_tr * r\n",
    "        s = np.sum(p_cond_s, axis = 1)\n",
    "        for c in range(nclasses):\n",
    "            p_cond_s[:,c] = p_cond_s[:,c] / s\n",
    "\n",
    "        prob_arrays.append(p_cond_s)\n",
    "        p_s_old = np.copy(p_s)\n",
    "        p_s = np.sum(p_cond_s, axis = 0) / p_cond_s.shape[0]\n",
    "        if (np.sum(np.abs(p_s - p_s_old)) < eps):\n",
    "            break\n",
    "\n",
    "    # return(p_s/np.sum(p_s))\n",
    "    return p_cond_s\n",
    "\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "# Select evaluation indicators\n",
    "scoring = 'accuracy'\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_proba = model.predict_proba(X_test)\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "emq_result = EMQ(y_proba, 4)\n",
    "# print(emq_result)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "p_pred = []\n",
    "cont = 0\n",
    "for item in emq_result:\n",
    "    cont += 1\n",
    "    p_pred.append(np.argmax(item))\n",
    "\n",
    "train_labels = label_encoder.fit_transform(y_test)\n",
    "p_pred = label_encoder.inverse_transform(p_pred)\n",
    "\n",
    "print (classification_report(y_test, p_pred, labels=np.unique(y_test)))\n",
    "cf = confusion_matrix(y_test, p_pred, labels=np.unique(y_test))\n",
    "print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680ea50",
   "metadata": {},
   "source": [
    "### Estimate the probability distribution for each class using different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "004a7cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabiensis_female' 'culex_female' 'funestus_female' 'gambiae_female']\n",
      "12000\n",
      "[0.25 0.25 0.25 0.25]\n"
     ]
    }
   ],
   "source": [
    "def class_dist(Y, nclasses):\n",
    "    return np.array([np.count_nonzero(Y == i) for i in range(nclasses)]) / Y.shape[0]\n",
    "\n",
    "Y = train_incubator['class'].values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y)\n",
    "\n",
    "classes = class_dist(Y_encoded, nclasses)\n",
    "\n",
    "print(np.unique(Y))\n",
    "print(Y.shape[0])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79fc7a",
   "metadata": {},
   "source": [
    "Use getScores to get the predicted score of train and test\n",
    "\n",
    "Perform group cross-validation on the data through the group_kfold.split() method, where group_kfold is a defined group cross-validation object. In each iteration of cross-validation, use the fit() method to fit the model, and use the predict_proba() method to obtain the probability score of each sample belonging to each category, and then fill these scores into the corresponding positions of the train_scores array.\n",
    "\n",
    "Finally, the model is refitted on the entire training set and the predict_proba() method is used to obtain the predicted probability score for the test set and stored in the test_scores array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30e3bb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9572\n",
      "[LightGBM] [Info] Number of data points in the train set: 9395, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.379823\n",
      "[LightGBM] [Info] Start training from score -1.321889\n",
      "[LightGBM] [Info] Start training from score -1.412937\n",
      "[LightGBM] [Info] Start training from score -1.434148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9570\n",
      "[LightGBM] [Info] Number of data points in the train set: 9638, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.430199\n",
      "[LightGBM] [Info] Start training from score -1.406205\n",
      "[LightGBM] [Info] Start training from score -1.335126\n",
      "[LightGBM] [Info] Start training from score -1.376178\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9577\n",
      "[LightGBM] [Info] Number of data points in the train set: 9961, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.429059\n",
      "[LightGBM] [Info] Start training from score -1.371645\n",
      "[LightGBM] [Info] Start training from score -1.410375\n",
      "[LightGBM] [Info] Start training from score -1.336649\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9592\n",
      "[LightGBM] [Info] Number of data points in the train set: 10062, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.428724\n",
      "[LightGBM] [Info] Start training from score -1.370714\n",
      "[LightGBM] [Info] Start training from score -1.445454\n",
      "[LightGBM] [Info] Start training from score -1.306298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9551\n",
      "[LightGBM] [Info] Number of data points in the train set: 8944, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.261579\n",
      "[LightGBM] [Info] Start training from score -1.472655\n",
      "[LightGBM] [Info] Start training from score -1.325985\n",
      "[LightGBM] [Info] Start training from score -1.505364\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9591\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n"
     ]
    }
   ],
   "source": [
    "best_params = {'learning_rate': 0.13, \n",
    "                'max_depth': 7, \n",
    "                'n_estimators': 200,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'num_leaves': 5,\n",
    "                'reg_alpha': 0.01,\n",
    "                'reg_lambda': 0.01,\n",
    "                'subsample': 0.88}\n",
    "\n",
    "def getScores(X_train, X_test, Y_train, nclasses):\n",
    "\n",
    "    # model = lgb.LGBMClassifier(**best_params)\n",
    "    model = lgb.LGBMClassifier()\n",
    "   \n",
    "    train_scores = np.zeros((len(X_train), nclasses))\n",
    "    test_scores = np.zeros((len(X_test), nclasses))\n",
    "\n",
    "    groups = train_incubator['sensor'].values\n",
    "    group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "    for train_index, test_index in group_kfold.split(X_train, Y_train, groups):\n",
    "        model.fit(X_train[train_index], Y_train[train_index])\n",
    "        train_scores[test_index] = model.predict_proba(X_train)[test_index]\n",
    "    \n",
    "    model.fit(X_train, Y_train)\n",
    "    test_scores = model.predict_proba(X_test)\n",
    "           \n",
    "    return train_scores, test_scores\n",
    "\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set).values\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set).values\n",
    "y_train = train_incubator['class'].values\n",
    "groups = train_incubator['sensor'].values\n",
    "\n",
    "train_scores, test_scores = getScores(X_train, X_test, y_train, nclasses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908b541e",
   "metadata": {},
   "source": [
    "EMQ function\n",
    "\n",
    "EM algorithm to estimate the class distribution of the test set. The EMQ function assumes that the class distribution of the test set is unknown, but can be estimated by the class conditional probabilities on the test set. It iteratively adjusts the class distribution so that, under given model parameters, the class conditional probability of the test set best matches the actual observed test set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c8138ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "\n",
    "\n",
    "def EMQ(test_scores, train_labels, nclasses):\n",
    "    max_it = 1000        # Max num of iterations\n",
    "    eps = 1e-6           # Small constant for stopping criterium\n",
    "\n",
    "    p_tr = class_dist(train_labels, nclasses)\n",
    "    p_s = np.copy(p_tr)\n",
    "    p_cond_tr = np.array(test_scores)\n",
    "    p_cond_s = np.zeros(p_cond_tr.shape)\n",
    "    prob_arrays = []\n",
    "\n",
    "    for _ in range(max_it):\n",
    "        # Add Laplacian smoothing\n",
    "        # r = (p_s + alpha) / (p_tr + (alpha * nclasses))\n",
    "        r = p_s / p_tr\n",
    "        \n",
    "        p_cond_s = p_cond_tr * r\n",
    "        s = np.sum(p_cond_s, axis = 1)\n",
    "        for c in range(nclasses):\n",
    "            p_cond_s[:,c] = p_cond_s[:,c] / s\n",
    "\n",
    "        prob_arrays.append(p_cond_s)\n",
    "        p_s_old = np.copy(p_s)\n",
    "        p_s = np.sum(p_cond_s, axis = 0) / p_cond_s.shape[0]\n",
    "        if (np.sum(np.abs(p_s - p_s_old)) < eps):\n",
    "            break\n",
    "\n",
    "    return(p_s/np.sum(p_s))\n",
    "    # return p_cond_s\n",
    "\n",
    "Y = train_incubator['class'].values\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(Y)\n",
    "EMQ_result = EMQ(test_scores, train_labels, nclasses)\n",
    "# print(EMQ_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6beaa",
   "metadata": {},
   "source": [
    "### Test model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0e17288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  LGBM\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9591\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\tAcc: 0.5131\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "arabiensis_female       0.40      0.21      0.27       428\n",
      "     culex_female       0.45      0.79      0.57       522\n",
      "  funestus_female       0.62      0.87      0.72       512\n",
      "   gambiae_female       0.56      0.19      0.29       600\n",
      "\n",
      "         accuracy                           0.51      2062\n",
      "        macro avg       0.51      0.51      0.46      2062\n",
      "     weighted avg       0.51      0.51      0.46      2062\n",
      "\n",
      "╒═════════════════════╤════════════════╤═══════════════════╤══════════════════╕\n",
      "│   arabiensis_female │   culex_female │   funestus_female │   gambiae_female │\n",
      "╞═════════════════════╪════════════════╪═══════════════════╪══════════════════╡\n",
      "│                  90 │            206 │                63 │               69 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  87 │            410 │                21 │                4 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                   2 │             49 │               443 │               18 │\n",
      "├─────────────────────┼────────────────┼───────────────────┼──────────────────┤\n",
      "│                  48 │            251 │               186 │              115 │\n",
      "╘═════════════════════╧════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Train and test a LGBM model\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "best_params = {'learning_rate': 0.14, \n",
    "                'max_depth': 9, \n",
    "                'n_estimators': 212,\n",
    "                'colsample_bytree': 0.82,\n",
    "                'num_leaves': 49,\n",
    "                'reg_alpha': 0.077,\n",
    "                'reg_lambda': 0.7,\n",
    "                'subsample': 0.93}\n",
    "\n",
    "models = [('LGBM', lgb.LGBMClassifier(**best_params))]\n",
    "\n",
    "for name, model in models:\n",
    "    print(\"Model: \", name)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    p_labels = model.predict(X_test)\n",
    "    a_labels = y_test\n",
    "    acc = accuracy_score(a_labels, p_labels)\n",
    "    \n",
    "    print(\"\\tAcc: %.4f\" % acc)\n",
    "    print (classification_report(a_labels, p_labels, labels=np.unique(y_test)))\n",
    "        \n",
    "    cf = confusion_matrix(a_labels, p_labels, labels=np.unique(y_train))\n",
    "    print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2649b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  LGBM\n",
      "test_ 2062\n",
      "EMQ [[1.99353250e-02 1.46674435e-01 8.33390241e-01 1.47534652e-17]\n",
      " [1.95943795e-01 5.96583048e-01 2.07473157e-01 1.13075602e-16]\n",
      " [5.53557446e-03 1.83026675e-01 8.11437750e-01 2.62948143e-15]\n",
      " ...\n",
      " [2.98539471e-04 5.76050478e-03 9.93940956e-01 6.38476092e-17]\n",
      " [4.29737974e-03 7.04579196e-02 9.25244701e-01 1.91612037e-15]\n",
      " [4.89837457e-04 6.64269685e-03 9.92867466e-01 2.41864730e-17]]\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001771 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9591\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: [1024 1441 6394]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m p_labels:\n\u001b[0;32m     36\u001b[0m     p_pred\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39margmax(item \u001b[38;5;241m*\u001b[39m EMQ_result))\n\u001b[1;32m---> 37\u001b[0m p_pred \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m a_labels \u001b[38;5;241m=\u001b[39m y_test\n\u001b[0;32m     40\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(a_labels, p_pred)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\comp9991\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:160\u001b[0m, in \u001b[0;36mLabelEncoder.inverse_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    158\u001b[0m diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msetdiff1d(y, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)))\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(diff):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(diff))\n\u001b[0;32m    161\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(y)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[y]\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: [1024 1441 6394]"
     ]
    }
   ],
   "source": [
    "# Train and test a LGBM model\n",
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X_train = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y_train = train_incubator['class'].values \n",
    "\n",
    "X_test = pd.DataFrame(test_sf2, columns=feature_set)\n",
    "y_test = test_sf2['class'].values\n",
    "\n",
    "best_params = {'learning_rate': 0.14, \n",
    "                'max_depth': 9, \n",
    "                'n_estimators': 212,\n",
    "                'colsample_bytree': 0.82,\n",
    "                'num_leaves': 49,\n",
    "                'reg_alpha': 0.077,\n",
    "                'reg_lambda': 0.7,\n",
    "                'subsample': 0.93}\n",
    "\n",
    "model = lgb.LGBMClassifier(**best_params)\n",
    "# model = lgb.LGBMClassifier()\n",
    "\n",
    "# for name, model in models:\n",
    "print(\"Model: \", name)\n",
    "\n",
    "Y = train_incubator['class'].values\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(Y)\n",
    "EMQ_result = EMQ(test_scores, nclasses)\n",
    "print('EMQ', EMQ_result)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "p_labels = model.predict_proba(X_test)\n",
    "p_pred = []\n",
    "for item in p_labels:\n",
    "    p_pred.append(np.argmax(item * EMQ_result))\n",
    "p_pred = label_encoder.inverse_transform(p_pred)\n",
    "\n",
    "a_labels = y_test\n",
    "acc = accuracy_score(a_labels, p_pred)\n",
    "\n",
    "print(\"\\tAcc: %.4f\" % acc)\n",
    "# print (classification_report(a_labels, p_pred, labels=np.unique(y_test)))\n",
    "      \n",
    "# cf = confusion_matrix(a_labels, p_pred, labels=np.unique(y_train))\n",
    "# print(tabulate(cf, headers=np.unique(y_train), tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cf250209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_ 2062\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3056, 2062]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     p_pred\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39margmax(item))\n\u001b[0;32m     43\u001b[0m p_pred \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39minverse_transform(p_pred)\n\u001b[1;32m---> 45\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\comp9991\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\comp9991\\lib\\site-packages\\sklearn\\metrics\\_classification.py:213\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\comp9991\\lib\\site-packages\\sklearn\\metrics\\_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\comp9991\\lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3056, 2062]"
     ]
    }
   ],
   "source": [
    "alpha = 1\n",
    "\n",
    "def EMQ(test_scores, nclasses):\n",
    "    max_it = 1000        # Max num of iterations\n",
    "    eps = 1e-6           # Small constant for stopping criterium\n",
    "\n",
    "    # p_tr = class_dist(train_labels, nclasses)\n",
    "    p_tr = [0.25, 0.25, 0.25, 0.25]\n",
    "    p_s = np.copy(p_tr)\n",
    "    p_cond_tr = np.array(test_scores)\n",
    "    p_cond_s = np.zeros(p_cond_tr.shape)\n",
    "    prob_arrays = []\n",
    "    print('test_', len(p_cond_s))\n",
    "\n",
    "    for _ in range(max_it):\n",
    "        # Add Laplacian smoothing\n",
    "        # r = (p_s + alpha) / (p_tr + (alpha * nclasses))\n",
    "        r = p_s / p_tr\n",
    "        \n",
    "        p_cond_s = p_cond_tr * r\n",
    "        s = np.sum(p_cond_s, axis = 1)\n",
    "        for c in range(nclasses):\n",
    "            p_cond_s[:,c] = p_cond_s[:,c] / s\n",
    "\n",
    "        prob_arrays.append(p_cond_s)\n",
    "        p_s_old = np.copy(p_s)\n",
    "        p_s = np.sum(p_cond_s, axis = 0) / p_cond_s.shape[0]\n",
    "        if (np.sum(np.abs(p_s - p_s_old)) < eps):\n",
    "            break\n",
    "    # print(len(p_cond_s))\n",
    "\n",
    "    # return(p_s/np.sum(p_s))\n",
    "    return p_cond_s\n",
    "\n",
    "Y = train_incubator['class'].values\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(Y)\n",
    "EMQ_result = EMQ(test_scores, nclasses)\n",
    "# print(EMQ_result)\n",
    "p_pred = []\n",
    "for item in EMQ_result:\n",
    "    p_pred.append(np.argmax(item))\n",
    "p_pred = label_encoder.inverse_transform(p_pred)\n",
    "\n",
    "acc = accuracy_score(y_test, p_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5c744f",
   "metadata": {},
   "source": [
    "尝试交叉验证的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1b584453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9572\n",
      "[LightGBM] [Info] Number of data points in the train set: 9395, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.379823\n",
      "[LightGBM] [Info] Start training from score -1.321889\n",
      "[LightGBM] [Info] Start training from score -1.412937\n",
      "[LightGBM] [Info] Start training from score -1.434148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9570\n",
      "[LightGBM] [Info] Number of data points in the train set: 9638, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.430199\n",
      "[LightGBM] [Info] Start training from score -1.406205\n",
      "[LightGBM] [Info] Start training from score -1.335126\n",
      "[LightGBM] [Info] Start training from score -1.376178\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9577\n",
      "[LightGBM] [Info] Number of data points in the train set: 9961, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.429059\n",
      "[LightGBM] [Info] Start training from score -1.371645\n",
      "[LightGBM] [Info] Start training from score -1.410375\n",
      "[LightGBM] [Info] Start training from score -1.336649\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9592\n",
      "[LightGBM] [Info] Number of data points in the train set: 10062, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.428724\n",
      "[LightGBM] [Info] Start training from score -1.370714\n",
      "[LightGBM] [Info] Start training from score -1.445454\n",
      "[LightGBM] [Info] Start training from score -1.306298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9551\n",
      "[LightGBM] [Info] Number of data points in the train set: 8944, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -1.261579\n",
      "[LightGBM] [Info] Start training from score -1.472655\n",
      "[LightGBM] [Info] Start training from score -1.325985\n",
      "[LightGBM] [Info] Start training from score -1.505364\n",
      "test_ 3056\n",
      "acc [0.8268979057591623]\n",
      "acc_emq [0.8285340314136126]\n",
      "acc_avg 0.16537958115183246\n",
      "acc_emq_avg 0.16570680628272252\n"
     ]
    }
   ],
   "source": [
    "feature_set = special_features+wbf_features+freq_features+basefreq_features+relbasefreq_features+power_features\n",
    "\n",
    "X = pd.DataFrame(train_incubator, columns=feature_set)\n",
    "y = train_incubator['class'].values\n",
    "groups = train_incubator['sensor'].values\n",
    "# Select evaluation indicators\n",
    "scoring = 'accuracy'\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "emq_accuracies = []\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in group_kfold.split(X, y, groups=groups):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    emq_result = EMQ(y_proba, 4)\n",
    "\n",
    "    p_pred = []\n",
    "    cont = 0\n",
    "    for item in emq_result:\n",
    "        cont += 1\n",
    "        p_pred.append(np.argmax(item))\n",
    "\n",
    "    p_pred = label_encoder.inverse_transform(p_pred)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    emq_accuracy = accuracy_score(y_test, p_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    emq_accuracies.append(emq_accuracy)\n",
    "    accuracies.append(accuracy)\n",
    "print('acc', accuracies)\n",
    "print('acc_emq', emq_accuracies)\n",
    "print('acc_avg', np.sum(accuracies) / 5)\n",
    "print('acc_emq_avg', np.sum(accuracies) / 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp9991",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
